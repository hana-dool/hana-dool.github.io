---
title:  "Example &#58; Counting lies"
excerpt: "거짓말을 세는 가짓수"
categories:
  - Py_Pymc
last_modified_at: 2021-10-24
toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

 베이즈 통계의 꽃 MCMC 에 대해 어떻게 모델링 하는지 알아봅시다.
{: .notice--warning}

# [MCMC modeling](#link){: .btn .btn--primary}{: .align-center}

> ## Introduction

- 이전에서는 pymc 의 내부 매커니즘이나, MCMC 의 작동법들을 밝히지 않았습니다. 
  - 여기에서는 이러한 방법에 대해서 어떻게 MCMC 가 작동되는지에 대해서 알아보겠습니다. 
- 미지수가 N 개인 Bayesian inference 문제를 다룰때에응, 우리는 N 차 사전확률 공간을 만듭니다. 
  - 이떄에 추가적으로 '표면(곡선)' 을 정의하는데, 이는 사전확률공간을 나타내는 표면이라고 생각하시면 됩니다. 
  - 예를들어 미지수 2개 (p1,p2) 가 있고 둘의 사전분포가 Uniform 이면 아래와 같이 2차원 공간에 변수를 나타내고(x,y) 둘의 분포는 z 좌표를 이용하여 나타냅니다.

![png](/assets/images/Python/42_1.png)

![png](/assets/images/Python/42_2.png)

> ## MCMC

- 우리는 사전 prior 평면과 데이터를 이용하여 사후확률분포를 만들어내야 합니다. 
  - 하지만 이러한 작업은 어마어마한 적분을 해야하기떄문에 차원이 커질수록 정확한 값을 찾는것은 계산적으로 불가능합니다. 
- MCMC 는 분포 자체가 아니라 사후확률 분포의 표본을 돌려준다는것을 명심합시다! 
  - 엄청나게 많은 Inference 를 실시하면서 MCMC 는 내가 발견한 조약돌이 , 내가 찾으려는 산에서 나올 확률이 얼마나 될까..? 같은 반복작업을 수행한다. 
  - 즉 원래의 산을 다시 만들 수 있다는 희망을 가지고 수천개의 조약돌을 돌려줌으로서 그 작업을 완료하게 됩니다. 
  - Pymc 에서 이러한 조약돌을 표본(sample) 이라 하고, 이러한 값들의 모음을 트레이스 (trace) 라고 합니다.
- MCMC 가 똑똑하게 검색한다는것은 우리가 원하는대로 MCMC 는 사후확률이 높은 영역으로 수렴한다는 의미입니다.
  - MCMC 는 이웃한 위치를 탐색하고, 확률이 높은 영역으로 이동함으로서 이러한 일을 수행하게 됩니다. 
  - 즉 다시 말해서 수렴 (Convergence) 는 공간의 한 점을 향해 움직인다기 보다는 공간의 넓은 영역을 이동하며, 랜덤하게 그 영역으로 걸어 들어가 그 영역에서 표본을 채취한다고 보시면 됩니다. 

> ## 왜 수천개의 표본이 필요? 

- 수천개의 표본을 돌려주는게 사실 비효율적으로 들릴 수 있습니다.
  - 하지만 이것은 매우 효율적입니다. 다음과 같은 대안들이 무쓸모하다는 점을 잘 기억합시다.

1. 산맥 (True 분포) 는 N 차원 표면입니다. 이를 수학적으로 표현하는것은 '매~우' 어렵습니다.
2. 지형의 정상(산의 가장 높은 지점) 을 돌려주는것은 수학적으로 가능하고 현명한듯 보이지만 (가장 높은 지점이 미지수의 가장 가능성 있는 추정값일것입니다.) 이는 지형의 자세한 형태를 무시하는 방법입니다. 

- 이러한 계산상의 이유 외에도 표본을 돌려주는 가장 큰 이유는 , 어려운 문제를 풀기 위한 큰 수의 법칙 (Law of large number) 를 사용하는것이 편하기 떄문입니다.

> ## MCMC 

1. 현재 위치에서 시작한다.
2. 새로 이동할 위치를 조사한다.
3. 데이터와 사전 확률분포에 따른 위치를 바탕으로 새로운 위치를 수용하거나 기각한다. 
4. 수용한다면 새로운 위치로 이동한 뒤 1단계로 돌아간다. / 그렇지 않으면 이동하지 않고 1단계로 이동한다.
5. 수많은 반복 후에 모든 가능성 위치를 돌려준다. 

- 이러한 방법으로 우리는 사후 확률 분포가 존재하는 지역을 향해 이동하고, 이동중에 표본을 수집하게 됩니다. 
  - 일단 사후확률 분포에 도달하면 모두 사후확률분포에 속할 가능성이 있으므로 표본을 쉽게 모을 수 있습니다.

> ## Example 

- 우리는 다음과 같은 데이터셋을 가지고 있다고 가정합시다. 

```python
figsize(12.5, 4)
data = np.loadtxt("data/mixture_data.csv", delimiter=",")

plt.hist(data, bins=20, color="k", histtype="stepfilled", alpha=0.8)
plt.title("Histogram of the dataset")
plt.ylim([0, None]);
print(data[:10], "...")
```

![png](/assets/images/Python/43_1.png)

- 위의 그래프에서 데이터 형태는 이중형(bimodal form), $120$과 $200$ 근처에서 꼭대기가 두 개인 모습처럼 보인다. 이 데이터셋에는 **클러스터가 두 개**있을 수 있다.
- 이 데이터셋을 아래의 알고리즘으로 모델링 해보자

1. 각 데이터 포인트에 대해 $p$의 확률을 가진 클러스터 1을 선택하거나 클러스터 2를 선택한다.
2. 1단계에서 선택한 클러스터의 모수가 $u_i$와 $\sigma_i$인 정규확률분포에서 확률변수를 뽑는다.
3. 반복한다.

- 여기서 $p$나 정규확률분포의 모수를 모르므로, 추론하거나 알아내야 한다.
- 정규확률분포를 $\text{Nor}_0$와 $\text{Nor}_1$로 표기하자. 이 둘은 각각 미지의 평균과 표준편차, $\mu_i$와 $\sigma_i$ ($i=0, 1$)을 가진다. 
- 특정 데이터 포인트는 $\text{Nor}_0$이나 $\text{Nor}_1$에서 나올 수 있다. 클러스터1($\text{Nor}_0$을 따르는)의 **사전확률**은 우리가 모르기 때문에 모델링하기 위해 0과 1사이의 균등 확률변수를 만들고 이를 $p$라고 하자.

```python
import pymc3 as pm
import theano.tensor as T

with pm.Model() as model:
    p1 = pm.Uniform('p', 0, 1)
    p2 = 1 - p1
    p = T.stack([p1, p2])
    assignment = pm.Categorical("assignment", p, 
                                shape=data.shape[0],
                                testval=np.random.randint(0, 2, data.shape[0]))
    
print("prior assignment, with p = %.2f:" % p1.tag.test_value)
print(assignment.tag.test_value[:10])
```

```
prior assignment, with p = 0.50:
[0 0 1 0 0 1 1 1 0 0]
```



{: .notice--success}

