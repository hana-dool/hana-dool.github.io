---
title:  "Pymc Basic Grammer"
excerpt: "Pymc 를 이용하기 위한 기본 Grammer"
categories:
  - Py_Pymc
last_modified_at: 2021-10-24
toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

 Pymc 의 기초를 정리해 보았습니다. 어디에도 잘 정리된 문서가 없어서... 사방팔방에서 모아본 문서입니다!
{: .notice--warning}

# [Pymc3](#link){: .btn .btn--primary}{: .align-center}

- PyMC3 모델을 정의할떄 우리는 Theano function 을 이용하게 됩니다. 어떤 Flow 를 통해서 이루어질

# [Pymc3 Basic](#link){: .btn .btn--primary}{: .align-center}

-  우선 Simple linear regression 을 이용하여 모델링 하는것을 예시로 사용하겠습니다. 

> ## Linear regression modeling

$$Y\sim N(\mu , \sigma^2)$$

$$\mu = \alpha + \beta_1X_1 + \beta_2 X_2$$

- 우리는 위와 같은 linear regression model 을 고려할 것입니다. 
  - 이를 단순히 Freq 모델링을 하게 된다면 Least square method 를 사용하여 모델링을 하게 될 것입니다. 
- 하지만 여기에서 우리는 Bayesian 방법론을 사용해 보겠습니다. 
  - 즉 우리는 각 parameter 에 대해서 prior 를 주게 됩니다.

$$\alpha \sim N(0,10)$$

$$\beta_i \sim N(0,10)$$

$$\sigma \sim \mid N(0,1)\mid$$

- 이를 모델링한다고 가정하고, 한번 모델링을 진행해 보겠습니다.

```python
import numpy as np
import matplotlib.pyplot as plt
# Intialize random number generator
np.random.seed(123)
# True parameter values
alpha, sigma = 1, 1
beta = [1, 2.5]
# Size of dataset
size = 100
# Predictor variable
X1 = np.linspace(0, 1, size)
X2 = np.linspace(0,.2, size)
# Simulate outcome variable
Y = alpha + beta[0]*X1 + beta[1]*X2 + np.random.randn(size)*sigma
```

- 우선 위처럼 Random Sample 을 만들었습니다. 이 모델에 대해서 모델링을 진행해 볼까요?

```python
from pymc3 import Model, Normal, HalfNormal

# The following code implements the model in PyMC:
basic_model = Model()

with basic_model:
    # 우리가 모르는 파라미터에 대해서 prior 를 주는 단계
    alpha = Normal('alpha', mu=0, sd=10)
    beta = Normal('beta', mu=0, sd=10, shape=2)
    sigma = HalfNormal('sigma', sd=1)
    
    # 평균에 대한 해석
    mu = alpha + beta[0]*X1 + beta[1]*X2
    
    # 데이터의 확률론적 해석 (N(mu , sima^2)) 과 실제 데이터 (observed) 을 합친다.
    # 그러면 우리는 prior + data = posterior 를 얻을 수 있을것입니다.
    Y_obs = Normal('Y_obs', mu=mu, sd=sigma, observed=Y)
```

- 위처럼 우리는 Basic 모델을 정의할 수 있습니다. 이러한 모델은 아래와 같이 생겼습니다.

```python
pm.model_to_graphviz(basic_model)
```

![png](/assets/images/Python/44_3.png)

> ## Model()

> With 문과 사용하는 Model

- 위 처럼 모델을 선언한 뒤에 with 문으로 연결시키면, model 이라는 객체에 우리가 주었던 값들을 모두 저장하게 됩니다. 

```python
with basic_model:
    # 우리가 모르는 파라미터에 대해서 prior 를 주는 단계
    alpha = Normal('alpha', mu=0, sd=10)
    beta = Normal('beta', mu=0, sd=10, shape=2)
```

- 위처럼 with 을 선언하고 계속 변수를 선언하면, basic_model (model 객체) 에 계속 쌓이게 됩니다. 

> Model.basic_RVs : random parameter 출력

```python
# 모델에 포함되어있는 Random variable 을 출력합니다.
basic_model.basic_RVs
```

```
[alpha ~ Normal,
 beta ~ Normal,
 sigma_log__ ~ TransformedDistribution,
 Y_obs ~ Normal]
```

> basic_model.변수 : 변수가 어떤 분포를 따르는지 출력

```python
# 모델에 포함되어있는 beta 변수가 어떤 분포를 따르는지 출력
basic_model.beta
```

```
beta∼Normal(mu=0.0, sigma=10.0)
```

> pm.model_to_graphviz(model 객체) : 모델 객체를 시각화 하여 표현

![png](/assets/images/Python/44_3.png)

> ## Stochastic random variable

```python
alpha = Normal(’alpha’, mu=0, sd=10)
beta = Normal(’beta’, mu=0, sd=10, shape=2)
sigma = HalfNormal(’sigma’, sd=1)
```

- 이러한 변수들을 확률론적 변수라고 합니다. 
  - 왜냐하면 고정되어있는 변수가 아니고, 고정된 (또는 부보 parameter 로 연결된) parameter 로 인한 확률 분포 이기 때문입니다. 
- 위처럼 변수의 이름과 첫번쨰 이름은 일치해야 합니다.
- 나머지 변수는 매개 변수(하이퍼 파라미터) 입니다. 
- 일반적으로 베타 / 감마 / 이항분포 등이 이용될 수 있습니다.

> ## pm.Deterministic (Deterministic Variable)

> Deterministic R.V. 란?

```python
mu = alpha + beta[0]*X1 + beta[1]*X2
```

- 위처럼 우리는 mu 에 대해서 정의를 하게 되는데요, 이는 parent value (여기서는 beta) 가 정해지면 mu 가 완전히 상수처럼 결정(Deterministic) 된다는것에 집중합시다.
- 이전에 Stochastic value 를 생각해볼까요? 

```python
alpha = Normal(’alpha’, mu=0, sd=10)
```

- 위처럼 hyperparameter 가 완전히 정의된 상태이지만 alpha 는 여전히 R.V 입니다. 그러므로 확률론적 변수라고 하는것입니다.

> pm.Deterministic

```python
with pm.Model() as model:
    mu = pm.Normal('mu', mu=0., sd=1.)
    mu_d= mu+5 #this just adds 5 to all samples of mu, but samples are not stored
    Y_obs = pm.Normal('Y_obs', mu=mu_d, sd=1,observed=data)
```

- 위와 같이 mu_d 는 mu+5 의 샘플인것은 맞지만, 이러한 샘플은 저장되지 않습니다.
  - 즉 우리는 pm.Deterministic('mu_d',',mu+5') 을 이용한다면 이러한 deterministic variables 는 저장될 수 있으며 나중에 trace object 로부터 계산이 가능해집니다.

> DIfference with Anonymous deterministic variable

```python
with model:
    rate = pm.switch(switchpoint >= np.arange(112), early_mean, late_mean)
```

```python
with model:
    rate = pm.Deterministic('rate', pm.switch(switchpoint >= np.arange(112), early_mean, late_mean))
```

- 위 두 경우의 차이점은 뭘까요? 이전에 말했듯 '값을 저장하여, trace plot' 에서 볼지 안볼지의 차이입니다.

> ## Observed Stochastic

```python
Y_obs = Normal(’Y_obs’, mu=mu, sd=sigma, observed=Y)
```

- Y_obs 의 경우 우리는 Observed Stochastic 변수라고 부르게 됩니다.
  - 그리고 이는 model 의 Data likelihood 입니다. 
- 이전의 Stochastic Variable 과 동일하지만 'observed' 변수가 들어있다는것이 차이점입니다.
  - 이러한 '데이터' 의 정보를 확률 변수에 전달하게 됩니다. 
- 이전의 mu , sigma 변수들과는 다르게, Y_obs 의 파라미터들은 정해져 있지 않습니다. 
  - mu 는 Deterministic Variable
  - sigma 는 stochastic Variable 
- 이는 likelihood 에 대해서 parent - child 관계를 만들어냅니다.

> ## testval argument

```python
import pymc3 as pm
import numpy as np

ndims = 2
nobs = 20

zdata = np.random.normal(loc=0, scale=0.75, size=(ndims, nobs))
BoundedGamma = pm.Bound(pm.Gamma, 0.5, 2)

with pm.Model() as model:
    xbound = BoundedGamma('xbound', alpha=1, beta=2,testval = 1)
    z = pm.Normal('z', mu=0, tau=xbound, shape=(ndims, 1), observed=zdata)
with model:
    start = pm.find_MAP()
with model:
    step = pm.NUTS()
with model: 
    trace = pm.sample(3000, step, start)

pm.traceplot(trace);
```

- 위와 같은 모델을 볼떄에 testval = 1 을 지정한다면, 시작값은1 이라고 지정한것과 같게 됩니다. 
  - 이러한 세팅은, 변수가 가지면 안되는 값이 있을때, 이 값을 가지지 않게 하기 위하여 사용되고는 합니다. 

> ## Model Visualization

```python
pm.model_to_graphviz(basic_model)
```

![png](/assets/images/Python/44_3.png)

- 위와 같이 , model_to_graphvis 라는 메서드를 사용하면, 우리가 정의한 모델을 시각화할 수 있습니다.

> ## shape

```python
## Build model
gmm = pm.Model()
# Specify number of groups
K = 3

with gmm:
    # Prior over z
    p = pm.Dirichlet('p', a=np.array([1.]*K))
    # z is the component that the data point is being sampled from.
    # Since we have N data points, z should be a vector with N elements.
    z = pm.Categorical('z', p=p, shape=N)
    # Prior over the component means and standard deviations
    mu = pm.Normal('mu', mu=0., sd=10., shape=K)
    sigma = pm.HalfCauchy('sigma', beta=1., shape=K)
    # Specify the likelihood
    Y_obs = pm.Normal('Y_obs', mu=mu[z], sd=sigma[z], observed=y)
```

- 위처럼 shape 를 지정할 경우, 길이가 K 인 array 를 형성하게 됩니다.

# [Pymc3 Fitting](#link){: .btn .btn--primary}{: .align-center}

> ## Model Fitting

- 우리의 모델을 완전하게 specified 하고 나면, 다음은 우리 모델에 대해 unkown variable 을 posterior 을 이용하여 추정하는 것이 남았습니다.
  - 이러한 posterior 응 analytic 하게 구하는것이 너무 벅차므로 우리는 근사하여 추정하게 됩니다. 
- 이러한 방법은 두가지가 고려될 수 있습니다. 하나는 최적화 방법을 이용하여 Maximum a posteriori (MAP) 지점을 찾아내는것과 / MCMC 샘플링 방법을 이용하여 posterior 를 요약하는 방식입니다.

> ## find_MAP (Maximum a Posteriori methods)

- 이 방법은 , posterior 의 값이 제일 높은 지점을 콕 찝어서 그 parameter 로 psoterior 를 요약하는 방식입니다.
  - 매우 빠르고 적용하기 쉽다는 장점이 있습니다. 
- 하지만 오로지 point estimator 만을 주기 떄문에 모든 분포의 모양을 알 수 는 없다는 단점이 있습니다. 

> find_MAP

- pymc 에도 이러한 방법을 위한 모듈이 있습니다. 

```python
from pymc3 import find_MAP
map_estimate = find_MAP(model=basic_model)
print(map_estimate)
```



```
{‘alpha’: array(1.0136638069892534),
‘beta’: array([ 1.46791629, 0.29358326]),
‘sigma_log’: array(0.11928770010017063)}
```

- 위처럼 MAP 를 적용하였을떄에, estimator 가 나오는것을 볼 수 있습니다.
- Broyden–Fletcher–Goldfarb–Shanno (BFGS) optimization algorithm 의 알고리즘을 이용하여 log -posterior 의 maximum 값을 찾아줍니다.

> scipy.optimize module

- 이떄에 위에서의 Optimization 베서드가 아니라 다른 방법을 이용해서 MAP 를 찾을 수 있습니다. 

```python
map_estimate = pm.find_MAP(model=basic_model, method="powell")
map_estimate
```



```
{'alpha': array(1.02069137),
 'beta': array([0.87013416, 1.6375205 ]),
 'sigma_log__': array(-0.09271233),
 'sigma': array(0.91145566)}
```

> Usage

- 이 방식의 문제점은, 매우 좁은 영역에서 큰 값을 가지는 분포의 경우 (즉 뾰족한 산) 극도로 밀도가 낮지만(값이 높긴 해도, 너비가 너무 좁아서), MAP 를 적용하면 이 값을 대표값으로 선정할 수 있습니다. 
- 또한 대부분의 MAP 를 찾는 방법론들은 Local optima 에 빠지기 쉽습니다.
- 이러한 오류들로 인하여 사실 MAP 방법론은 잘 쓰이지 않는 방법론입니다. 
- 그러므로 대게 아래에서의 Sampling 기반 방법론을 더 선호합니다.

> ## Sampling Method

- MAP 를 찾는것은 매우 빠르고 쉬운 방법이기는 하지만 MAP 는 불확실성에 대해서 잘 말해주지 않아서 안좋은 방법이다.
  - CI 는 구간의 너비에 따라 불확실성에 대해서 말해준다는것을 기억하자 ! 
- 그러므로 대신에 MCMC 와 같인 SImulation 기반 접근 방식을 사용하면 , posterior 와 같은 Sample 을 만들어 낼 수 있습니다. 
  - 이러한 MCMC 샘플링을 수행하기 위해 우리는 metropolis , No-U-Turn Sampler Slice , HamiltonianMC 등의 샘플러가 포함되어 있습니다.

```python
NUTS
Metropolis
Slice
HamiltonianMC
BinaryMetropolis
```

> Recommendation

- 만약 모수가 이산형 변수이거나, Binary 또는 Continuous 변수일떄에는어떠한 방법론을 쓰는게 좋을까요? 
  - Binary variables will be assigned to `BinaryMetropolis`
  - Discrete variables will be assigned to `Metropolis`
  - Continuous variables will be assigned to `NUTS`
- 위와 같이 일반적으로 사용되는 방법론 (https://docs.pymc.io/en/stable/pymc-examples/examples/getting_started.html) 을 알 수 있습니다. 

> NUTS

- NUTS 에 대해서 좀 더 깊이알아봅시다. 
- NUTS는 메트로폴리스 헤스팅 방법에서의 제안분포의 분산 처럼, 스케일링 매개변수가 필요합니다.
  - 너무 큰 점프를 하거나, 작은 점프를 하지 않도록 조절하는것이 좋습니다. 
- 효율적인 샘플링을 용이하게 하려면 이 스케일링 변수를 잘 설정하는것이 중요합니다.
  - 다행히도 Pymc 의 튜닝 당계에서 얻은 샘플의 분산을 기반으로 NUTS 의 파라미터를 합리적이세 초기화 합니다.

> ## pm.sample

```python
with basic_model:
    # draw 500 posterior samples
    trace = pm.sample(500, return_inferencedata=False)
```

- sample 함수는 주어진 횟수(500) 개 동안 샘플을 생성하고 수집합니다.
  - 위의 경우, sampling 하는 방법이 정해져 있지 않은데, Default 는 NUTS 방법론을 사용합니다.
- 다음과 같이 마지막 5개 샘플틀 볼 수 있습니다.

```python
trace["alpha"][-5:]
```

```
array([0.92353953, 0.85614491, 1.03088924, 1.02631406, 0.92231234])
```

> Other method

- 만일 다른방법으로 샘플링을 하고 싶다면 어떡할까요? 
  - 이떄에는 step 이라는 position 변수에 우리가 쓰고싶은 Sampler 를 넣어주면 됩니다.

```python
with basic_model:
    # instantiate sampler
    step = pm.Slice()

    # draw 5000 posterior samples
    trace = pm.sample(5000, step=step, return_inferencedata=False)
```

> ## Posterior Analysis

> Posterior plot

- 이제 우리는 posterior 가 잘 생성 되었는지에 대해서 Diagnotic 을 수행할 수 있습니다.

```python
with basic_model:
    az.plot_trace(trace);
```

![png](/assets/images/Python/44_1.png)

- 왼쪽 분포 그림은 각 Radom variable 의 marginal distribution 을 보여주는 것입니다. 
- 오른쪽 그림은 샘플링된 포인트들을 선으로 이어서, sequential order 로 만든것입니다. 

> Posterior Summary

- 위의 posterior sampling 의 결과를 다음과 같이 요약할 수도 있습니다.

```python
with basic_model:
    display(az.summary(trace, round_to=2))
```

![png](/assets/images/Python/44_2.png)

- 이떄 r_hat 이 1에 가까운 값을 가질떄에 MCMC 가 잘 수렴되었다고 생각하게 됩니다. 
  - 위 예제에서는 1 의 값을 가지므로 이상적인 상황이라 할 수 있습니다.

# [Visualization](#link){: .btn .btn--primary}{: .align-center}

> ## model_to_graphviz : 모델 시각화

```python
pm.model_to_graphviz(basic_model)
```

![png](/assets/images/Python/44_3.png)

> ## posterior distribution

```python
niter = 2000
with pm.Model() as coin_context:
    p = pm.Beta('p', alpha=2, beta=2)
    y = pm.Binomial('y', n=n, p=p, observed=heads)
    trace = pm.sample(niter)
```

- 위와 같이 probability 모델을 

# [Pymc3 Diagnotic](#link){: .btn .btn--primary}{: .align-center}



# [Pymc3 Model Check](#link){: .btn .btn--primary}{: .align-center}





# [Pymc3 Tips](#link){: .btn .btn--primary}{: .align-center}

> ## help 

```python
help(Normal)
```

```
help(Normal)
Help on class Normal in module pymc3.distributions.continuous:
class Normal(pymc3.distributions.distribution.Continuous)
| Normal log-likelihood.
|
| .. math::
ight\}
|
| Parameters
| ----------
| mu : float
| Mean of the distribution.
| tau : float
| Precision of the distribution, which corresponds to
| :math:‘1/\sigma^2‘ (tau > 0).
| sd : float
| Standard deviation of the distribution. Alternative parameterization
....
```

- 위처럼 pymc3 에서 어떻게 정의되었는지를 살펴보기 위해서라면 help 함수를 사용할 수 있습니다.

---

**reference**

- <https://peerj.com/articles/cs-55.pdf>
- <https://arxiv.org/pdf/1507.08050.pdf>
- <https://docs.pymc.io/en/stable/pymc-examples/examples/getting_started.html>
- <https://notebook.community/fonnesbeck/PyMC3_Oslo/notebooks/5.%20Model%20Building%20with%20PyMC3>

{: .notice--success}

