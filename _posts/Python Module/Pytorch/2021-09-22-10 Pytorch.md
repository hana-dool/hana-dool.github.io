---
title: "Pytorch Basic and Tensor Manipulation" 
excerpt: "파이토치의 기본에 대해서 알아봅시다."
categories:
  - Py_Pytorch
date : 2021-09-22 16:00:00 +0900
last_modified_at: 2021-09-22

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

 제생각이지만 파이토치는 점차 텐서를 앞지르고 딥/머신러닝 툴로서 독보적인 위치를 점할것이라고 생각합니다. 그 이유는 훨씬 유연하고, Low level 에서 다룰수 있는게 많기 떄문입니다. 텐서가 케라스덕분에 쉽다던데, 그건 솔직히 나중가서 더 깊게 연구하는 사람 입장이 되면 오히려 거슬릴 따름인거같더라구요. 그러므로 파이토치 합시다!
{: .notice--warning}

# [파이토치 패키지의 기본 구성](#link){: .btn .btn--primary}{: .align-center}

> ## torch

- 메인 Namespace 입니다. 다양한 수학 함수가 포함되어 있으며 Numpy 와 유사한 행동을 합니다. 

> ## torch.autograd

- 자동 미분을 위한 함수들이 들어있습니다. 
- 최적화 및 Backpropagation 을 위해서라면 미분이 필수적일 것입니다. 

> ## torch.nn

- 신경망을 구축하기 위한 다양한 데이터구조, 레이어 등이 정의되어 있습니다.
  - RNN , RSTM 과 같은 레이어, 
  - Relu 와 같은 활성화 함수 
  - loglodd 와 같은 손실 함수 

> ## torch.optim

- optimization 을 구현하기 위한 다양한 최적화 알고리즘이 구현되어 있습니다.
  - Adam 
  - SGd .. 

> ## torch.utils.data

- sgd 의 반복 연산을 실행시 사용하는 미니배치용 Utility 함수가 포함되어있습니다.

> ## torch.onnx

- ONNX(Open Neural Network Exchange)의 포맷으로 모델을 익스포트(export)할 때 사용합니다. 
- ONNX는 서로 다른 딥 러닝 프레임워크 간에 모델을 공유할 때 사용하는 포맷입니다.

# [Tensor](#link){: .btn .btn--primary}{: .align-center}

> ## Tensor

- 딥러닝시에 다루게 되면 가장 기본적인 단위는 벡터 , 행렬, 텐서입니다.
  - 넘파이와 행동이 거의 유사함

- 차원이 없는 값을 Scalar
  - 4, 3, 1.5 등의 value 라고 이해하시면 편합니다.
- 1차원으로 구성된 Vector 
  - (4,) (3,) 등으로 이해하시면 됩니다.
- 2차원으로 구성된 Matrix
- 3차원으로 구성된 Tensor
  - 데이터 사이언스에서는 3차 이상의 텐서는 다차원 행렬 / 배열로 간주합니다.
  - 또한 1,2,3 차원의 값들을 그냥 나눠 부르지 않고 Tensor 라고 합쳐 부르기도 합니다.
- 기본단위가 텐서여서, list 나 array 로 데이터가 되어있는 경우 , Tensor 로 고쳐주어야 함

> ## Tensor Shape Convention

> 2D Tensor

![png](/assets/images/Python/34_1.png)

- 2차원 텐서는 기본적으로 가로가 Dim , 세로가 Batch size 가 됩니다.
  - t = (batch size, dim) 이 되는것입니다. 

- 훈련 데이터 하나의 크기를 256이라고 해봅시다. [3, 1, 2, 5, ...] 이런 숫자들의 나열이 256의 길이로 있다고 상상하면됩니다. 
- 다시 말해 훈련 데이터 하나 = 벡터의 차원은 256입니다. 만약 이런 훈련 데이터의 개수가 3000개라고 한다면, 현재 전체 훈련 데이터의 크기는 3,000 × 256입니다. 
- 3,000개를 1개씩 꺼내서 처리하는 것도 가능하지만 컴퓨터는 훈련 데이터를 하나씩 처리하는 것보다 보통 덩어리로 처리합니다. 
  - 3,000개에서 64개씩 꺼내서 처리한다고 한다면 이 때 batch size를 64라고 합니다. 그렇다면 컴퓨터가 한 번에 처리하는 2D 텐서의 크기는 (batch size × dim) = 64 × 256입니다.

> 3D Tensor in Image

- $\mid t \mid$ = (batch size , width, height)

![png](/assets/images/Python/34_2.png)

- 일반적으로 비젼분야에서 다루게 됩니다.
  - width, height 가 하나의 사진을 구성하게되고, batch size 만큼의 이미지가 쌓여있는 구조라고 생각하시면 됩니다.

> 3D Tensor in NLP 

- $\mid t \mid$ =  (batch size , width, height)
- 자연어 처리는 보통 batchsize, 문장 길이, 벡터의 차원 이라는 3차원 텐서를 사용합니다.

- 아래와 같이 4개의 문장으로 구성된 전체 훈련 데이터가 있습니다.

```
[[나는 사과를 좋아해], 
[나는 바나나를 좋아해], 
[나는 사과를 싫어해], 
[나는 바나나를 싫어해]]
```

- 컴퓨터는 아직 이 상태로는 '나는 사과를 좋아해'가 단어가 1개인지 3개인지 이해하지 못합니다. 우선 컴퓨터의 입력으로 사용하기 위해서는 단어별로 나눠주어야 합니다.

```
[['나는', '사과를', '좋아해'], 
['나는', '바나나를', '좋아해'], 
['나는', '사과를', '싫어해'], 
['나는', '바나나를', '싫어해']]
```

- 이제 훈련 데이터의 크기는 4 × 3의 크기를 가지는 2D 텐서입니다. 컴퓨터는 텍스트보다는 숫자를 더 잘 처리할 수 있습니데. 이제 각 단어를 벡터로 만들겁니다. 아래와 같이 단어를 3차원의 벡터로 변환했다고 하겠습니다.

```
'나는' = [0.1, 0.2, 0.9]
'사과를' = [0.3, 0.5, 0.1]
'바나나를' = [0.3, 0.5, 0.2]
'좋아해' = [0.7, 0.6, 0.5]
'싫어해' = [0.5, 0.6, 0.7]
```

- 위 기준을 따라서 훈련 데이터를 재구성하면 아래와 같습니다.

```
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]
```

이제 훈련 데이터는 4 × 3 × 3의 크기를 가지는 3D 텐서입니다. 이제 batch size를 2로 해보겠습니다.

```
첫번째 배치 #1
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]]]

두번째 배치 #2
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]
```

- 컴퓨터는 배치 단위로 가져가서 연산을 수행합니다. 그리고 현재 각 배치의 텐서의 크기는 (2 × 3 × 3)입니다. 이는 (batch size, 문장 길이, 단어 벡터의 차원)의 크기입니다.



# [Numpy ans Tensor](#link){: .btn .btn--primary}{: .align-center}

- `torch.FloatTensor` : 32bit float point
- `torch.LongTensor` : 64bit signed integer
- 텐서는 각 데이터 형태별로 위와 같이 2가지 타입을 만들어낼 수 있습니다.
  - 일반적으로 torch.FloatTensor 를 사용한다고 보시면됩니다.

> ## 1D with pytorch

- 파이토치로 1차원 텐서인 Vector 를 만들어봅시다.

```python
t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])
print(t)
# tensor([0., 1., 2., 3., 4., 5., 6.])
```

- 위와 같이 1차원 Tensor 를 출력합니다. 

```python
print(t.dim())  # 1 (차원)
print(t.shape)  # torch.Size([7])
print(t.size()) # torch.Size([7])
```

- 위와 같이 torch 의 사이즈 , dimension 을 알 수 있습니다. 

> ## 2D with pytorch

```python
t = torch.FloatTensor([[1., 2., 3.],
                       [4., 5., 6.],
                       [7., 8., 9.],
                       [10., 11., 12.]])
print(t)
#tensor([[ 1.,  2.,  3.],
#        [ 4.,  5.,  6.],
#        [ 7.,  8.,  9.],
#        [10., 11., 12.]])
```

- 위와 같이,  2차원 tensor 를 만들 수 있습니다. 

```python
print(t.dim())  # 2 
print(t.size()) # torch.Size([4, 3])
```

- 현재, 위 텐서의 차원은 2차원이고 (4,3) 의 크기를 가진다는것을 볼 수 있습니다.

> ## Manipulation

```python
print(t[:, 1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원의 첫번째 것 가져옴
# tensor([ 2.,  5.,  8., 11.])
print(t[:, :-1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원에서는 맨 마지막에서 첫번째를 제외하고 다 가져온다.
#tensor([[ 1.,  2.],
#        [ 4.,  5.],
#        [ 7.,  8.],
#        [10., 11.]])
```

- 위와 같이, Manipulation 은 기본적으로 Numpy 의 문법을 따른다는것을 알 수 있습니다.
- 기본적인 Numpy 문법은 다들 익숙하실것이니까.. 여기에서는 넘어가겠습니다.

---

**Reference**

- <https://wikidocs.net/52460>

파이토치의 기본이 되는 텐서에 대해서 알아보았습니다. 그 다음에는 Tensor 의 manipulation 에 대해서 더 깊이 알아보겠습니다.
{: .notice--success}

