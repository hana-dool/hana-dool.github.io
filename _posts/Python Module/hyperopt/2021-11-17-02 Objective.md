---
title:  "Objective Function"
excerpt: "목적이 되는 함수 정의"
categories:
  - Py_Hyperopt
last_modified_at: 2021-11-17

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

 어떻게 하면 최적화 할 수 있을지에 대해서 정말정말 쉬운 예제를 통해서 알아봅시다.
{: .notice--warning}

# [Objective Function](#link){: .btn .btn--primary}{: .align-center}

> ## Objective Function definition

```python
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials
fspace = {
    'x': hp.uniform('x', -5, 5)
}
def f(params):
    x = params['x']
    val = x**2
    return {'loss': val, 'status': STATUS_OK}
trials = Trials()
best = fmin(fn=f, space=fspace, algo=tpe.suggest, max_evals=50, trials=trials)
print(f'best :{best}')
print('trials:')
for trial in trials.trials[:2]:
    print(trial)
```

- 위와 같이 def 가 되는 f 가 바로 objective function 입니다.
  - 위의 경우 우리가 minimize 하고자 하는 함수가 들어가게 됩니다. 
- 이때 두가지 인자가 기본적으로 존재하는것을 확인할 수 있습니다. 

> ## Definition Rule

- 일반적으로 loss 함수의 output 은 dictionary 형태여야 합니다. 
- 이때 두가지는 필수적으로 들어가야 합니다. 

> `status` 

- STATUS_OK 를 지정하게 되면, 성공적이였다 판단하고, loss 를 저장하며 최적화를 진행합니다. 
- STATUS_FAIL 이면 값을 저장하지 않습니다. 

> `loss`  

- 실수값으로, 우리가 minimize 하고 싶은 대상이 오게 됩니다. 
- 만약 status 가 ok 면 무조건 정의되어야 합니다. 

> ## Example 1 : STATUS_FAIL 활용

- 아래의 경우, config.time_left() < 50 조건에 따라서, loss 를 저장할지 , 아닐지로 나누고 있습니다.

```python
def hyperopt_lightgbm(X_train: pd.DataFrame, y_train: pd.Series, params: Dict, config: Config, max_evals=10):
    X_train, X_test, y_train, y_test = data_split_by_time(X_train, y_train, test_size=0.2)
    X_train, X_val, y_train, y_val = data_split_by_time(X_train, y_train, test_size=0.3)
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_val, label=y_val)

    space = {
        "learning_rate": hp.loguniform("learning_rate", np.log(0.01), np.log(0.5)),
        #"max_depth": hp.choice("max_depth", [-1, 2, 3, 4, 5, 6]),
        "max_depth": hp.choice("max_depth", [1, 2, 3, 4, 5, 6]),
        "num_leaves": hp.choice("num_leaves", np.linspace(10, 200, 50, dtype=int)),
        "feature_fraction": hp.quniform("feature_fraction", 0.5, 1.0, 0.1),
        "bagging_fraction": hp.quniform("bagging_fraction", 0.5, 1.0, 0.1),
        "bagging_freq": hp.choice("bagging_freq", np.linspace(0, 50, 10, dtype=int)),
        "reg_alpha": hp.uniform("reg_alpha", 0, 2),
        "reg_lambda": hp.uniform("reg_lambda", 0, 2),
        "min_child_weight": hp.uniform('min_child_weight', 0.5, 10),
    }

    def objective(hyperparams):
        if config.time_left() < 50:
            return {'status': STATUS_FAIL}
        else:
            model = lgb.train({**params, **hyperparams}, train_data, 100,
                          valid_data, early_stopping_rounds=10, verbose_eval=0)
            pred = model.predict(X_test)
            score = roc_auc_score(y_test, pred)
            #score = model.best_score["valid_0"][params["metric"]]
            # in classification, less is better
            return {'loss': -score, 'status': STATUS_OK}

    trials = Trials()
    best = hyperopt.fmin(fn=objective, space=space, trials=trials,
                         algo=tpe.suggest, max_evals=max_evals, verbose=1,
                         rstate=np.random.RandomState(1))

    hyperparams = space_eval(space, best)
    log(f"auc = {-trials.best_trial['result']['loss']:0.4f} {hyperparams}")
    return hyperparams 
```

> ## Example 2 : CV 

- 아래의 경우, params 를 받고, 이를 model 에 전달해준 뒤 , CV 를 통하여 모델을 훈련시키고 있습니다.

```python
from sklearn import datasets
iris = datasets.load_iris()
X = iris.data
y = iris.target
def hyperopt_train_test(params):
    clf = KNeighborsClassifier(**params)
    return cross_val_score(clf, X, y).mean()
space4knn = {
    'n_neighbors': hp.choice('n_neighbors', range(1,50))
}
def f(params):
    acc = hyperopt_train_test(params)
    return {'loss': -acc, 'status': STATUS_OK}
trials = Trials()
best = fmin(f, space4knn, algo=tpe.suggest, max_evals=100, trials=trials)
print 'best:'
print best
```



---

**Reference**

- <https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce>
- <http://hyperopt.github.io/hyperopt/getting-started/search_spaces/>







