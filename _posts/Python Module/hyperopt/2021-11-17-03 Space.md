---
title:  "Space"
excerpt: "fmin 의 space 인자에 대해서"
categories:
  - Py_Hyperopt
last_modified_at: 2021-11-17

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

 어떻게 하면 최적화 할 수 있을지에 대해서 정말정말 쉬운 예제를 통해서 알아봅시다.
{: .notice--warning}

# [fmin(space = )](#link){: .btn .btn--primary}{: .align-center}

> ## Search Spaces 

- Hyperopt 는 space 에 대해서 설정을 다르게 할 수 있습니다. 
- 그 인자는 다음과 같습니다 .

> Choice

```python
from hyperopt import hp
space = hp.choice('a',
    [
        ('case 1', 1 + hp.lognormal('c1', 0, 1)),
        ('case 2', hp.uniform('c2', -10, 10))
    ])
```

- 위처럼 우리가 탐색할 space 를 정의합니다. 
  - 위에서 각각의 hp.lognormal 등이 어떤것을 의미하는지는 나중에 더 알아볼게요!

```python
import hyperopt.pyll.stochastic
print(hyperopt.pyll.stochastic.sample(space))
```

- 위처럼 hyperopt.pyll.stochastic 메서드를 통해서 , space 에서 직접 그 값을 확인하실 수 있습니다. 

```
('case 1', 1.2946603853478988)
```

- 위에서 중요하게 봐야할 부분은 모든 첫 argument 는 label 을 가진다는 것입니다.
  - 이러한 레이블은 나중에 반환받는 결과값에서 참조되는등 다양한 부분에서 쓰이므로 직관적으로 잘 설정하는게 중요합니다
- 그리고 그래프의 각 인자들은 튜플로 둘러쌓여있습니다.
- 또한 1 + hp.lognormal('c1', 0, 1) 처럼, 단일 hp Distribution 을 사용하는것 외에도, 어느정도 함수를 더 추가하여 Search 를 정의할 수 있다는 점입니다.

> ## parameters

> `hp.choice(label, options)`

- Options 중에서 하나의 값을 뽑아냅니다. 
- 이때 options 는 튜플 또는 딕셔너리가 될 수 있습니다.

> `hp.randint(label, upper)`

- [0, upper) 범위의 임의의 정수를 반환합니다. 
- 이것은 예를 들어 임의의 시드를 설명하는 데 적절한 분포입니다. 
- parameter 가 특정한 양의 정수 값을 가지는 경우에 적합합니다.

> `hp.uniform(label, low, high)`

- [low , high] 에 해당하는 Uniform 분포에서 하나를 뽑아냅니다.

> `hp.quniform(label, low, high, q)`

- round(uniform(low, high) / q) * q 의 분포입니다.
- 정수값을 가지는 분포에 적합합니다.

> `hp.loguniform(label, low, high)`

- 반환 값의 로그가 균일하게 분포되도록 exp(uniform(low, high)) 의 분포입니다.
- 이 분포는 [exp(low), exp(high)] 구간으로 제한됩니다.

> `hp.qloguniform(label, low, high, q)`

- round(exp(uniform(low, high)) / q) * q와 같은 값을 반환합니다.
- Exp 와 비슷한 분포를 나타내는 이산형 분포입니다.

> `hp.normal(label, mu, sigma)`

- 평균 mu 및 표준 편차 시그마로 정규 분포를 따르는 실수 값을 반환합니다. 
- Optimization 할때 딱히 상,하안가 제한은 없습니다. (Normal 이므로)

> `hp.qnormal(label, mu, sigma, q)`

- round(normal(mu, sigma) / q) * q 의 분포입니다..
- mu 주변의 값을 취하고, 제한이 없는 이산 변수에 적합합니다.

> `hp.lognormal(label, mu, sigma)`

- 반환 값의 로그가 정규 분포를 따르도록 exp(normal(mu, sigma)) 의 분포입니다.
- 이 분포는 양수로 제한됩니다.

> `hp.qlognormal(label, mu, sigma, q)`

- round(exp(normal(mu, sigma)) / q) * q 의 값을 반환합니다.

> ## Exmple : Simple Examples

- 제일 간단한 예시를 살펴보며 위에서의 정의를 다시 생각해봅시다.

```python
import hyperopt.pyll.stochastic
space = {
    'x': hp.uniform('x', 0, 1),
    'y': hp.normal('y', 0, 1),
    'name': hp.choice('name', ['alice', 'bob']),
}
print hyperopt.pyll.stochastic.sample(space)
```

```
{'y': -1.4012610048810574, 'x': 0.7258615424906184, 'name': 'alice'}
```

- 위를 보면 , optimization 공간은 다음과 같을것입니다. 
  - $x \in unif([0,1])$
  - $y\in N(0,1)$
  - $name \in \{ alice,bob\}$

> ## Example with Classification

- Classification 을 실시할때에는 여러가지 모델을 한번에 사용합니다.
- 이 때에 아래처럼 '모델도' Optimization 으로 넣어서 hyperparameter 처럼 취급할 수 있습니다.

```python
from hyperopt import hp
space = hp.choice('classifier_type', [
    {
        'type': 'naive_bayes',
    },
    {
        'type': 'svm',
        'C': hp.lognormal('svm_C', 0, 1),
        'kernel': hp.choice('svm_kernel', [
            {'ktype': 'linear'},
            {'ktype': 'RBF', 'width': hp.lognormal('svm_rbf_width', 0, 1)},
            ]),
    },
    {
        'type': 'dtree',
        'criterion': hp.choice('dtree_criterion', ['gini', 'entropy']),
        'max_depth': hp.choice('dtree_max_depth',
            [None, hp.qlognormal('dtree_max_depth_int', 3, 1, 1)]),
        'min_samples_split': hp.qlognormal('dtree_min_samples_split', 2, 1, 1),
    },
    ])
```

- 위를 이용한다면 Optimization 을 통해서 최적의 Model 도 한번에 고를 수 있을것입니다.

---

**Reference**

- <https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce>
- <http://hyperopt.github.io/hyperopt/getting-started/search_spaces/>







