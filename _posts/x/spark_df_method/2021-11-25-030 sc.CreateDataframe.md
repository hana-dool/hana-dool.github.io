---
title:  "sc.createDataFrame"
excerpt: "데이터프레임을 만들기"
tags:
  - Spark_Function
last_modified_at: 2021-11-26

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

데이터프레임을 우리가 직접 만들어봅시다.
{: .notice--warning}

# [createDataFrame](#link){: .btn .btn--primary}{: .align-center}

> ## Dataframe

> SparkSession.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)

- Creates a DataFrame from an RDD, a list or a pandas.DataFrame.
- When schema is None, it will try to infer the schema (column names and types) from data, which should be an RDD of either Row, namedtuple, or dict.

> Note

- 우리는 스파크를 사용할때에 데이터프레임을 자주 사용합니다. 우리가 직접 데이터프레임을 만들어보려면 어떻게 해야할까요? 
- 이때 사용되는 메서드가 바로 `pyspark.sql.SparkSession.createDataFrame` 메서드입니다. 먼저 공식 문서의 설명부터 읽어보도록 하겠습니다. 
- https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html
  - `SparkSession.createDataFrame`(*data*, *schema=None*, *samplingRatio=None*, *verifySchema=True*)
  - Creates a [`DataFrame`](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame) from an `RDD`, a list or a `pandas.DataFrame`.
- 즉 DataFrame 을 만드는 함수라는것을 짐작할 수 있습니다! 이를 어떻게 쓸 수 있을지 아래에서 알아봅시다.

> ## Example 

```python
l = [('Alice', 1)]
df1 = spark.createDataFrame(l, ['name', 'age'])
df1
# DataFrame[name: string, age: bigint]
df1.show()
#+-----+---+
#| name|age|
#+-----+---+
#|Alice|  1|
#+-----+---+
```

- 위와 같이 스파크의 Data 에는 `[('Alice', 1)]` 를 넣고 , schema 에는 ['name', 'age'] 를 넣어서 데이터를 만든 모습입니다.
- 이 경우에는 스키마를 '이름만' 지정했기 때문에 col 이름만 name, age 로 설정되고, type 은 스파크가 알아서 유추하게 됩니다. 

> ## Example : 스키마 지정해서 DF 만들기

```python
from pyspark.sql.types import *
schema = StructType([
   StructField("name", StringType(), True),
   StructField("age", IntegerType(), True)])
df2 = spark.createDataFrame([('Alice', 1)], schema)
df2
# DataFrame[name: string, age: int]
df2.show()
#+-----+---+
#| name|age|
#+-----+---+
#|Alice|  1|
#+-----+---+
```

- 위와 같이, df2 가 정의된것을 잘 살펴보면, 이전과 다르게 자료형(스키마) 도 우리가 원한대로 잘 지정되어 있음을 볼 수 있습니다.
  - 이것은 우리가 `createDataFrame` 의 메서드를 사용할때에 스키마도 제대로 넣어주었기 때문입니다!

```python
df = spark.createDataFrame([
    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),
    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),
    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))
], schema='a int, b double, c string, d date, e timestamp')
df
# DataFrame [a: int, b: double, c: string, d: date, e: timestamp]
```

- 위와 같이 `schema` 를 string 으로 넣어주어서 스키마를 지정해줄 수도 있습니다.

> ## Example : Row list 에서 Dataframe 만들기

```python
from datetime import datetime, date
import pandas as pd
from pyspark.sql import Row

df = spark.createDataFrame([
    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),
    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),
    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))
])
df
```

- 위와 같이 pyspark 의 ROW 로 이루어진 list 를 통해서 Dataframe 을 생성할 수 있습니다. 

> ## Example : 스키마를 이용하여 만들기 

```python
df = spark.createDataFrame([
    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),
    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),
    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))
], schema='a int, b double, c string, d date, e timestamp')
df
# DataFrame [a: int, b: double, c: string, d: date, e: timestamp]
```

> ## Example : pandas.df 를 이용해서 만들기

```python
pandas_df = pd.DataFrame({
    'a': [1, 2, 3],
    'b': [2., 3., 4.],
    'c': ['string1', 'string2', 'string3'],
    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],
    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]
})
df = spark.createDataFrame(pandas_df)
df
```

- 위처럼 `pd.DataFrame` 을 이용해서 단지 createDataFrame 을 통해서, createDataFrame 을 만들 수 있습니다.

> ## Example : RDD 에서 dataframe 을 만들기

```python
rdd = spark.sparkContext.parallelize([
    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),
    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),
    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))
])
df = spark.createDataFrame(rdd, schema=['a', 'b', 'c', 'd', 'e'])
df
```

- 위에서처럼 `sparkContext.parallelize` 를 이용하여 RDD 를 만들어 낸 이후에, `createDataFrame` 메서드를 통해서 Dataframe 을 만들 수 있습니다

**Reference**

- <https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html>



