---
title:  "df.collect"
excerpt: "SQL 을 사용하기 위한 준비과정"
tags:
  - Spark_Function
last_modified_at: 2021-11-25

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

스파크를 처음 시작할때에
{: .notice--warning}

# [SparkSession](#link){: .btn .btn--primary}{: .align-center}

> ## RDD Operation

- Spark 의 RDD 는 2가지 Operation(Transformation, Action) 을 사용해 조작할 수 있습니다
  - Transformation
    - 기존의 RDD 를 변경하여 새로운 RDD 를 생성하는 것입니다.
      - **즉, 리턴값이 RDD 입니다.**
    - map, filter 등을 예로 들 수 있습니다.
  - Action
    - RDD 값을 기반으로 무엇인가를 계산해서, 결과를 생성하는 것입니다.
      - **즉, 리턴값이 데이터 또는 실행 결과입니다.**
    - collect, count 등을 예로 들 수 있습니다.

> ## RDD 동작

- 이러한, RDD 동작 원리의 핵심은 **Lazy Evaluation (느긋한 연산)** 입니다.
  - 즉 RDD 는 Action 연산자를 만나기 전까지는, Transformation 연산자가 아무리 쌓여도 처리하지 않습니다.
- 따라서, Executor 에 할당된 RDD 의 내용을 확인하기 위해서는, Action 연산(collect, count, etc)이 필요합니다.
  - 이러한 Action 연산중에 collect 가 있는데요, 이 연산자에 대해서 알아보도록 하겠습니다.

> ## collect()

> DataFrame.collect()

- Returns all the records as a list of Row.

> Note

- Collect() 함수는 처리 결과를 배열로 변환합니다.
- 이때 collect() 함수는, Executor 에 할당된 RDD 를 모두 Driver Node 로 취합하는 Action 이기 때문에, out of memory 가 발생할 수 있습니다.
  - 그러므로 필터링 등 작은데이터 집합을 반환하는데에 사용하는것이 유용합니다.

```python
spark = SparkSession \
    .builder \
    .appName("Python Spark SQL basic example") \
    .config("spark.some.config.option") \
    .getOrCreate()

sc = spark.sparkContext
# 숫자 리스트를 바탕으로 RDD 객체 생성
list_rdd = sc.parallelize([1,2,3,4,5])
list_squared_rdd = list_rdd.map(lambda x: x**2)

# collect 를 통하여, array 로 변환
list_squared_list = list_squared_rdd.collect()
list_squared_list
# [1, 4, 9, 16, 25]
```

- 위와 같이 collect 를 실행하면, array 로 변환됩니다!

> Detaframe 을 collect 하는 경우

- Dataframe 은 각각이 row 로 이루어져 있으므로, 아래와 같은 array 가 출력됩니다.

```python
df.select(df.name, (df.age + 10).alias('age')).collect()
[Row(name='Alice', age=12), Row(name='Bob', age=15)]
```

**Reference**

- 스파크 완벽 가이드
- <https://statkclee.github.io/bigdata/bigdata-pyspark-data-transformation.html>
- <https://jaeyung1001.tistory.com/61>



