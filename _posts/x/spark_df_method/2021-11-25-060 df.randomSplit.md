---
title:  "df.randomsample"
excerpt: "무작위 샘플 만들기"
tags:
  - Spark_Function
last_modified_at: 2021-11-26

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

데이터타입 바꾸기
{: .notice--warning}

# [df.sample](#link){: .btn .btn--primary}{: .align-center}

> ## 공식문서

> pyspark.sql.DataFrame.randomSplit(weights, seed=None)

- Randomly splits this [`DataFrame`](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame) with the provided weights.

> parameter

- Parameters: weights : list
  - list of doubles as weights with which to split the DataFrame. Weights will be
  - normalized if they don't sum up to $1.0$.
- seed : int, optional
  - The seed for sampling.

> ## 설명

- 원본 데이터프레임을 분할할꺠 우리는 randomSplit 을 사용할 수 있습니다.
  - 이는 주로 머신러닝 모델에서 Train / Split 을 사용할때에 사용합니다!


```python
df.randomSplit([0.25,0.75],2014)
[DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint],
 DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]]
```

- 위와 같이 df 에 대해서 randomSplit 을 사용하게되면 array 가 생성됩니다. 이때 
  - `[첫번째 split DF ,두번째 split DF]` 이런 형식으로 array 가 생성됩니다. 

- 즉 위의 정보를 이용하면 split 된 형태는 다음과 같을것이라 생각할 수 있습니다.

```python
split_array[0].count()
# 58
split_array[1].count()
# 198
```

- 위와 같이 0.25 : 0.75 의 비율로 분해된것을 볼 수 있습니다.


---

**Reference**

- 스파크 완벽 가이드
- 스파크 공식 가이드북



