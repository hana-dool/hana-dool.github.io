---
title:  "K Means"
excerpt: ""
tags:
  - ML_Model
last_modified_at: 2021-10-05

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
typora-root-url: ../../../../hana-dool.github.io
---

 여기 쓰인 글들은 모두, 비공개 게시글입니다. 비공개로 남긴 이유는 상당수가 제 개인 작성이 아니라 여러 Reference 를 긁어오고, 정리한것에 불과하여, 저작권 이슈가 생길수도 있기 떄문입니다. 그러므로 이를 보신다면 Reference 로 쓰지 마시고 단순히 참고용으로만 보시기 바랍니다.
{: .notice--warning}

# [K-means 개요](#link){: .btn .btn--primary}{: .align-center}

> ## History

- "k-평균"에 대한 개념은 1957년 후고 스테인하우스에 의해 소개되었으나 용어 자체는 1967년에 제임스 매퀸(영어: James MacQueen)에 의해 처음 사용되었다.
- 현재 사용되고 있는 표준 알고리즘은 1957년에 스튜어트 로이드(영어: Stuart Lloyd)가 펄스 부호 변조(PCM)를 목적으로 처음으로 고안 하였으나 1982년이 되어서야 컴퓨터 과학 매거진에 처음 공개되었다.
- 알고리즘이 공개되기 이전인 1965년에 E. W. Forgy 또한 같은 알고리즘을 제안하였다.
- 차후 1975년과 1979년에 Hartigan과 Wong에 의해 거리 계산이 필요하지 않은 좀 더 효율적인 방법이 소개 되었습니다.
  - J.A. Hartigan (1975). 《Clustering algorithms》. John Wiley & Sons, Inc. 
  - Hartigan, J. A.; Wong, M. A. (1979). “Algorithm AS 136: A K-Means Clustering Algorithm”. 《Journal of the Royal Statistical Society, Series C》 28 (1): 100–108. JSTOR 2346830.

> ## Introduction 

- 아래와 같이 각 군집간의 평균 위치값을 잡고, 그 위치값을 기준으로 계속해서 최적화를 해나가는 알고리즘

![jpg](/assets/images/Stat/160_1.jpg){: .align-center}

# [K-means 알고리즘](#link){: .btn .btn--primary}{: .align-center}

> ## 표준알고리즘 

- 입력값
  - k: 클러스터 수
  - D: n 개의 데이터 오브젝트를 포함하는 집합
- 출력값 
  - k 개의 클러스터
- 알고리즘 
  - 데이터 오브젝트 집합 D에서 k 개의 데이터 오브젝트를 임의로 추출하고, 이 데이터 오브젝트들을 각 클러스터의 중심 (centroid)으로 설정한다. (초기값 설정)
  - 집합 D의 각 데이터 오브젝트들에 대해 k 개의 클러스터 중심 오브젝트와의 거리를 각각 구하고, 각 데이터 오브젝트가 어느 중심점 (centroid) 와 가장 유사도가 높은지 알아낸다. 그리고 그렇게 찾아낸 중심점으로 각 데이터 오브젝트들을 할당한다.
  - 클러스터의 중심점을 다시 계산한다. 즉, **2**에서 재할당된 클러스터들을 기준으로 중심점을 다시 계산한다.
  - 각 데이터 오브젝트의 소속 클러스터가 바뀌지 않을 때까지 **2**, **3** 과정을 반복한다.

> Step 1 : Clustering 의 갯수를 정하고, 랜덤하게 초기 점들을 정합니다.

![jpg](/assets/images/X/3_1.jpg){: .align-center}

- $K$ 개의 초기 centroid(클러스터의 중심이 될)를 random하게 선택한다.

> Step 2 : 각 데이터(object) 를 , 가장 가까운 centroid $k_i$ 에 할당합니다.

![jpg](/assets/images/X/3_2.jpg){: .align-center}

- 평균값을 기준으로 분할된 영역은 위와 같이 [보로노이 다이어그램](https://ko.wikipedia.org/wiki/보로노이_다이어그램)으로 표시될 수 있음 

> Step 3 : k 개의 클러스터의 평균값을 기준으로 중심정 (원) 이 조절됩니다.

![jpg](/assets/images/X/3_3.jpg){: .align-center}

> Step 4 : 수렴할 때까지 2), 3) 과정을 반복한다.

![jpg](/assets/images/X/3_4.jpg){: .align-center}



> ## Convergence of K-means 

K-means clustering 알고리즘은 수렴하는가?
- 그렇다. 알고리즘의 (1), (2)번 스텝에서 cost function의 값이 줄어든다. 따라서 전체 알고리즘에서의 cost 값은 점 진적으로(monotonically) 감소한다.
- Step (1) : reassigning clusters based on distance
  - Old clusters : $C_{1}^{o}, C_{2}^{o}, \ldots, C_{K}^{o}$
  - New clusters : $C_{1}^{N}, C_{2}^{N}, \ldots, C_{K}^{N}$

$$\begin{gathered}
\operatorname{cost}\left(C_{1}^{O}, C_{2}^{o}, \ldots, C_{K}^{O}, \mathbb{C}_{1}, \mathbb{C}_{2}, \ldots, \mathbb{C}_{K}\right) \geq \min _{C_{1}, \ldots, C_{K}} \operatorname{cost}\left(C_{1}, C_{2}, \ldots, C_{K}, \mathbb{C}_{1}, \mathbb{C}_{2}, \ldots, \mathbb{C}_{K}\right) \\
=\operatorname{cost}\left(C_{1}^{N}, C_{2}^{N}, \ldots, C_{K}^{N}, \mathbb{C}_{1}, \mathbb{C}_{2}, \ldots, \mathbb{C}_{K}\right)
\end{gathered}$$

- Step (2): reassigning centroids based on clusters
  - Old centroid : $\mathbb{C}_{1}, \mathbb{C}_{2}, \ldots, \mathbb{C}_{K}$
  - New centroid : $\mathbb{C}_{1}^{N}, \mathbb{C}_{2}^{N}, \ldots, \mathbb{C}_{K}^{N}$

$$\begin{gathered}
\operatorname{cost}\left(C_{1}^{N}, C_{2}^{N}, \ldots, C_{K}^{N}, \mathbb{C}_{1}, \mathbb{C}_{2}, \ldots, \mathbb{C}_{K}\right) \geq \min _{\mathbb{C}_{,}, \ldots, \mathbb{C}_{K}} \operatorname{cost}\left(C_{1}^{N}, C_{2}^{N}, \ldots, C_{K}^{N}, \mathbb{C}_{1}, \mathbb{C}_{2}, \ldots, \mathbb{C}_{K}\right) \\
=\operatorname{cost}\left(C_{1}^{N}, C_{2}^{N}, \ldots, C_{K}^{N}, \mathbb{C}_{1}^{N}, \mathbb{C}_{2}^{N}, \ldots, \mathbb{C}_{K}^{N}\right)
\end{gathered}$$

# [Parameter](#link){: .btn .btn--primary}{: .align-center}

> ## 초기 Point 선택 기법

> 무작위 분할 (Random Partition)

- 알고리즘
  - 무작위 분할 알고리즘은 가장 많이 쓰이는 초기화 기법으로 각 데이터들을 임의의 클러스터에 배당한 후, 각 클러스터에 배당된 점들의 평균 값을 초기 Points $\mu_{i}$로 설정합니다.
- 아래는 같은 데이터에 대에서 10번 무작위분할로 초기 Point 지점을 선택한 것입니다. 

![jpg](/assets/images/X/3_5.jpg){: .align-center}

- 특징
  - 무작위 분할 기법의 경우 다른 기법들과는 달리 데이터 순서에 대해 독립적이다. 
  - 무작위 분할의 경우 초기 클러스터가 각 데이터들에 대해 고르게 분포되기 때문에 각 초기 클러스터의 무게중심들이 데이터 집합의 중심에 가깝게 위치하는 경향을 띱니다.
  - 하지만 이 방법은 별로 권장되는 방법이 아닙니다. 
    - 이 방법을 사용하여 초기화하면 k-Means가 Local Optima에서 멈출 가능성이 더 높습니다.
- 잘 작동하는 Clustering 방법에 대한요약
  - <https://dl.acm.org/doi/10.1145/584792.584890>

> Forgy

- 알고리즘 
  - Forgy 초기화 기법은 임의의 k개 데이터를 선택해서 초기 중심으로 설정합니다.
  - 즉 "데이터" 값을 중심으로 선택하기 떄문에, 데이터와 겹치게 됩니다.
- 특징 
  - 하지만 **각 클러스터의 중심이 데이터 중심으로 부터 멀리 떨어져 위치**한다는 특징이 있습니다.
- 코드
  - 이를 코드로 구현하면 아래와 같습니다.

```python
def forgy_initialize(X, k):
    '''Return Randomly sampled points from the data'''
    return X[np.random.choice(range(X.shape[0]), replace = False,     size = k), :]
```

![jpg](/assets/images/X/3_6.jpg){: .align-center}

- Iteration 1,7,8 은 좋은 Starting points 를 나타냅니다.
- Iteration 2,3,4,5,6,9,10 은 같은 클러스터에 2~3개씩의 Center points 를 할당했습니다. 
  - 이는 Local Optimization 으로 클러스터링이 진행될 수 있습니다.

![jpg](/assets/images/X/3_7.jpg){: .align-center}

- 위는 잘못된 Initial Points 로 인해서 잘못된 클러스터링이 진행됭 예시입니다.

> kmeans++

- 이것은 표준적인 방법이며 일반적으로 Forgy의 방법과 k-Means를 초기화하는 Random Partition 방법보다 더 잘 작동합니다.
- 알고리즘  : 이 아이디어는 가능한 한 서로 멀리 떨어져있는 점을 선택하는것입니다.
  - 데이터에서 하나 임의의 점을 선택합니다.
  - 그런 다음 첫 번째 점에서 멀리 떨어져 있을 가능성이 더 큰 다음 점을 선택합니다. 
    - 이는 첫 번째 중심에서 점의 제곱 거리에 비례하는 확률 분포에서 점을 샘플링하여 이를 수행합니다.
  - 나머지 점은 가장 가까운 중심에서 각 점의 거리 제곱에 비례하는 확률 분포에 의해 생성됩니다. 
    - 따라서 가장 가까운 중심에서 멀리 떨어져 있는 점이 샘플링될 가능성이 더 큽니다.
- 예시

![jpg](/assets/images/X/3_8.jpg){: .align-center}

- 위를 보면 적절하게 10개의 Case 모두 초기화 지점을 잘 선택한것을 볼 수 있습니다.

> ## Cluster 수 선택 기법

> Rules of Thumb

- 데이터 수가 n개라고 한다면 **k = (n/2)^(1/2)** 로 정해주면 됩니다.

> Elbow Method 

- **클러스터 수를 순차적으로 늘려가면서 결과를 모니터링**하는 방법입니다. **하나의 클러스터를 추가했을 때, 이전보다 좋은 결과를 나타내지 않으면 이전의 클러스터 수를 최종 클러스터의 수로 설정**하면 됩니다.
- 

# [주의점](#link){: .btn .btn--primary}{: .align-center}

> ## Outlier 의 존재

- 만약 특이한 데이터 (Outlier) 가 끼면 K-means 알고리즘은 아래와 같이 계산되게 됩니다.

![jpg](/assets/images/Stat/160_2.jpg){: .align-center}

- K-Means의 큰 약점중 하나는 이상치(Outliers)가 많을 경우 적절한 군집화가 이루어지지 않는다는 점입니다.
- 또한 아래와 같이 클러스터의 중심점이 클러스터의 실제 중심에 있지 않고 이상값 방향으로 치우치게 위치할 수 있습니다.

![jpg](/assets/images/X/3_13.jpg){: .align-center}

- 실제 데이터 클러스터는 오른쪽 중간에 위치해있는데 반해 클러스터의 중심은 클러스터로부터 왼쪽으로 상당히 떨어진 지점에 위치한다. 
- 이는 실제 클러스터 외부에 퍼져 있는 이상값들이 중심점 계산에 있어 그 값을 크게 왜곡시켰기 때문이다. 
- 이를 방지하기 위해 k-평균 알고리즘을 실시하기 전에 이상값을 제거하는 프로세스를 먼저 실행하거나, 분할법의 일종인 k-대푯값 알고리즘 (k-medoids algorithm) 을 이용하면 이상값의 영향을 줄일 수 있다.

> ## 클러스터 갯수에 대해 Sensitive

- 이 알고리즘은 k값에 따라 결과값이 완전히 달라진다. 
- 예를 들어, 실제 데이터 4개 의 클러스터를 가지고 있는데, k=3으로 입력했다고 가정하자. 그러면 아래와 같은 결과가 나올 수 있다.

![jpg](/assets/images/X/3_9.jpg){: .align-center}

- 이는 실제 클러스터의 수 보다 k값이 작을 때 발생하는 현상이다. 반대로, 실제 클러스터가 5개인데 k=8을 입력했다고 가정하자. 결과는 아래와 같다.

![jpg](/assets/images/X/3_10.jpg){: .align-center}

- 따라서, k값을 어떻게 주느냐에 따라 클러스터링의 결과가 극명하게 달라지며, 좋지 못한 결과를 보여줄 가능성이 있습니다.

> ## 수렴이 Global 이 아닌 Local 으로 수렴할 가능성

- 이 알고리즘은 초기값을 어떻게 주느냐에 따라 최적화의 결과가 전역 최적값 (global optimum) 이 아닌 지역 최적값 (local optimum)으로 빠질 가능성이 있다.
- 예를 들어, 실제 데이터는 3개의 클러스터를 가지고 있고, k도 3이라고 하자. 데이터와 초기 중심값의 분포는 아래와 같다.

![jpg](/assets/images/X/3_11.jpg){: .align-center}

- 그러나 알고리즘을 수행하면서 비용 함수 최소화를 진행하면 아래와 같이 지역 최솟값에 빠져 그대로 수렴하게 된다.

![jpg](/assets/images/X/3_12.jpg){: .align-center}

- 즉, 비용 함수의 함수 공간에서 최적화를 시행할 때, 에러가 줄어드는 방향으로 최적해를 찾아가게 되는데, 전역이 아닌 지역 최솟값에 도달해도 알고리즘의 수렴 조건을 만족하게 되므로 더 이상 최적화를 진행하지 않게 된다. 따라서, 사용자가 기대한 결과를 얻지 못하게 되는 것입니다.
- Note 
  - 이를 방지하기 위해 [담금질 기법](https://ko.wikipedia.org/wiki/담금질_기법) (Simulated Annealing) 을 수행하여 의도적으로 에러가 줄어드는 방향을 피하는 방식을 이용하거나, 서로 다른 초기값으로 여러 번 시도해본 뒤 가장 에러값이 낮은 결과를 사용하는 기법 등을 이용함으로써 이 문제를 완화할 수 있습니다.

> ## 구형 (spherical) 이 아닌 클러스터를 찾는 데에는 적절하지 않다.

- k-평균 알고리즘은 비용 함수를 계산할 때 중심점과 각 데이터 오브젝트간의 거리를 계산하게 된다.
- 이 때 주로 사용하는 거리는 [유클리드 거리](https://ko.wikipedia.org/wiki/유클리드_거리)이다. 따라서, 알고리즘 수행 시 중심점으로부터 구형으로 군집화가 이루어지게 된다. 
- 만약 주어진 데이터 집합의 분포가 구형이 아닐 경우에는 클러스터링 결과가 예상과 다를 수 있다. 아래의 경우를 살펴보자.

![jpg](/assets/images/X/3_14.jpg){: .align-center}

- 위의 경우에는 총 4개의 클러스터가 존재하는데, 구형인 클러스터는 2개만 존재한다. 위와 같은 경우 클러스터링을 시행하면 중심점으로부터 가까이 있는 데이터 오브젝트들을 한 그룹으로 묶게 되는데, 그러면 얼굴 형상의 데이터 분포를 고르게 4 조각으로 자르는 형상이 나오게 된다. 
- 이는 알고리즘 사용자가 원하는 결과와는 크게 다르다. 위와 같은 경우에는 비슷한 밀도를 가진 데이터 군집을 하나의 클러스터링으로 묶는 방법인 DBSCAN (Density-Based Spatial Clustering of Applications with Noise) 알고리즘이나 mean-shift 클러스터링 알고리즘을 사용하면 원하는 결과를 얻을 수 있다.

# [Application](#link): .btn .btn--primary}{: .align-center}

> ## Outlier Detection 

- 정상 데이터 집합에 대해 k-means clustering을 적용하여 정상 데이터 영역을 설정하고 이 영역을 벗어나는 데이터를 이상치 데이터로 판정한다. 
  - 그러나 초기 중심점들과 k값의 선택에 따라 k-means clustering 의 결과는 다양하게 나타날 수 있어 학습되는 정상 데이터 영역 모델에 대한 신뢰성이 낮아질 수 있다.
- https://www.koreascience.or.kr/article/JAKO201809355934119.pdf

# [Tips ](#link): .btn .btn--primary}{: .align-center}

> ## Scaling 을 해야한다. 

|  ID  | Age  | Income(rupees) |
| :--: | :--: | :------------: |
|  1   |  25  |     80,000     |
|  2   |  30  |    100,000     |
|  3   |  40  |     90,000     |
|  4   |  30  |     50,000     |
|  5   |  40  |    110,000     |

- 위와 같은 데이터테이블에 대해서, Clustering 을 진행한다고 합시다. 
- 이떄 관측치 1과 관측치 2의 유클리드 거리 = [(100000–80000)^2 + (30–25)^2]^(1/2) 가 됩니다.
- 거리 기반의 Clustering 을 진행하는 K-Measn 의 경우 소득 (income) 의 단위가 너무 크므로 , 거리의 대부분을 결정하게 됩니다. 
- 즉 scaling 을 하지 않은 상태에서 Clustering 을 진행한다면 단위에 따라서 Feature 의 중요도가 왜곡될 수 있습니다. 
- 그러므로 이러한 경우, Scaling 을 진행하는게 중요합니다.

|  ID  |  Age   | Income(rupees) |
| :--: | :----: | :------------: |
|  1   | -1.192 |     -0.260     |
|  2   | -0.447 |     0.608      |
|  3   | 1.043  |     0.173      |
|  4   | -0.447 |     -1.563     |
|  5   | 1.043  |     1.042      |

- 위와 같이 Scaling 을 진행하고 난 뒤에, 유클리드 거리는  *[(0.608+0.260)^2 + (-0.447+1.192)^2]^(1/2)* 가 됩니다. 

# [Python](#link){: .btn .btn--primary}{: .align-center}

---

**reference**

- https://towardsdatascience.com/k-means-clustering-from-a-to-z-f6242a314e9a
- Initial 초기화기법
  - <https://medium.com/analytics-vidhya/comparison-of-initialization-strategies-for-k-means-d5ddd8b0350e>
- k-menas outlier
  - https://www.koreascience.or.kr/article/JAKO201809355934119.pdf