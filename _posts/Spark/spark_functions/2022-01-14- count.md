---
title:  "count"
excerpt: "갯수 세기"
categories:
  - Spark_Function
last_modified_at: 2021-12-25

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

알아봅시다
{: .notice--warning}

# [count](#link){: .btn .btn--primary}{: .align-center}

> ## 공식 문서

> pyspark.sql.functions.count(col)

- Aggregate function: returns the number of items in a group.
- New in version 1.3.

> ## 설명

- 다음 예제에서 count 함수는 액션이 아닌 트련스포메 이션으로 동작합니다.
-  count 함수는 두 가지 방식으로 사용할 수 있습니다. 
- 하나는 count 함수 에 특정 컬럼을 지정하는 방식이고, 다른 하나는 count (*)나 count (1)을 사용하는 방식입니다. 
- count 함수를 사용해 다음 예제와 같이 전체 로우 수를 카운트할 수 있습니다.

```python
from pyspark.sql.functions import count
df.select(count("StockCode")).show() # 541909
```

- 위와 같이 '특정한 column' 을 지정함으로서 , 이 컬럼에 'not null' 한 데이터가 얼마나 있는지를 셀 수 있습니다.
- 하지만 이때 null 도 포함하여, 순수하게 row 의 수를 세고 싶다면 아래와 같이 수행할 수 있습니다.

```python
from pyspark.sql.functions import count
df.select(count('*')).show() # 541909
```

> Note

- null 값이 포함된 데이터의 레코드 수를 구할 때에는 몇 가지 유의사항이 있습니다. 
  - 예를 들어 $\operatorname{count}(*)$ 구문을 사용하면 null 값을 가진 로우를 포함해 가운트합니다. 
  - 하지만 count 함수에 특정 컬럼을 지정하면 null 값을 카운트 하지 않습니다.


---

**Reference**

- 스파크 완벽 가이드
- 스파크 공식 Docs

