---
title:  "Introduction to Spark"
excerpt: "아파치 스파크에 대해서 알아봅시다."
categories:
  - Spark_Definite_Guide
last_modified_at: 2021-11-20

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

스파크의 기본만 한번 알아봅시다. 
{: .notice--warning}

# [스파크](#link){: .btn .btn--primary}{: .align-center}

> ## 스파크의 기본 아키텍쳐

- 컴퓨터 하나로는 대규모의 데이터를 처리하기가 힘듭니다. 
  - 그래서 여러 컴퓨터를 사용하는 필요성이 생깁니다.
- 컴퓨터 클러스터는 여러 컴퓨터의 자원을 모아서, 하나의 컴퓨터처럼 사용할 수 있게 한 것입니다.
  - 컴퓨터를 모았다고 해서 끝이 아니라 이를 조율하는 하나의 프레임워크가 필요합니다
- 바로 스파크가 이러한 역할을 하는 프레임워크가 됩니다. 스파크는 클러스터와의 데이터 처리 작업을 관리, 조율합니다. 
  - 사용자는 클러스터 매니저에 스파크 어플리케션을 제출 (submit) 합니다. 
  - 이를 받은 클러스터 매니저는 어플리케이션 실행에 필요한 자원을 할당하게 됩니다.

> ## 스파크 어플리케이션

- 스파크 어플리케이션은 드라이버 프로세스와 다수의 익스큐터 프로세스로 구성됩니다.

![png](/assets/images/Program/10_1.png)

> 드라이버 프로세스

- 클러스터 노드중 하나에서 실행되고, main() 함수를 실행하게 됩니다. 
  - 스파크 어플리케이션 정보의 유지관리, 사용자 입력에 대한 응답, 스케쥴링 등의 역할을 하게 됩니다.
- 즉 드라이버 프로세스는 어플리케이션의 심장과 같은 역할을 합니다.

> 익스큐터 

- 드라이버 프로세스가 할당한 작업을 수행하고 , 진행 상황을 다시 드라이버 노드에 보고하게 됩니다.
- 위 그림은 클러스터 매니저가 물리적인 머신을 관리하고, 스파크 어플리케이션에 자원을 할당하는것을 나타낸 것입니다.
  - 클러스터 매니저는 여러개중에 하나 ( 하둡 YARN 등) 를 선택할 수 있으며, 하나의 클러스터에서 여러개의 스파크 어플리케이션을 실행할 수 있게 해줍니다.

> Note

- 스파크는 클러스터 모드뿐만 아니라 로컬 모드도 지원합니다. 
  - 로컬모드로 실행하게되면 드라이버와 익스큐터를 단일 스레스형태로 지원합니다.

> Summary

- 스파크는 사용 가능한 자원을 파악하기 위해 클러스터 매니저를 사용
- 드라이버 프로세스는 주어진 작업을 완료하기 위해 드라이버 프로그램의 명령을 익스큐터에서 실행할 책임이 있음 

> ## 스파크의 다양한 언어 API

- 스파크의 언어 API 를 이용하여 다양한 언어로 스파크 코드를 실행할 수 있습니다.
  - 스칼라, 자바, 파이썬, SQL , R

![png](/assets/images/Program/10_2.png)

- 위는 SparkSession 과 스파크의 언어 API 관의 관계를 나타냅니다.
  - 사용자는 스파크 코드를 실행하기 위해 SparkSession 객체를 진입점으로 사용합니다. 
  - 스파크는 사용자가 파이썬이나 R 로 코드를 작성하게 될 때에, 이 코드를 스파크가 이용할 수 있게 바꾸어줍니다.

> Note : 스파크 API

- 다양한 언어로 스파크를 이용할 수 있는 이유는 스파크는 기본적으로 두가지 API 를 제공하기 때문입니다.
  - 하나는 저수준의 비 구조적 API , 하나는 고수준의 구조적 API 입니다.

> ## SparkSession

- 스파크 어플리케이션은 SparkSession 이라는 드라이버 프로세스로 제어하게 됩니다.
- SparkSession은 **스파크 응용 프로그램의 통합 진입점**으로 스파크의 기능들과 구조들이 상호방식하는 방식을 제공합니다.
- 스칼라와 파이썬콘솔을 시작하면 Spark 변수로 SparkSession 을 사용할 수 잇습니다

```python
from pyspark.sql import SparkSession
spark = SparkSession \
    .builder \
    .appName("Python Spark SQL basic example") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()
```

- 위와 같이 pyspark.sql 에서 스파크 세션을 만들 수 있습니다.

```
spark.range(1000).toDF('number')
```

- 위는 일정 범위의 숫자를 만드는 간단한 코드입니다.
  - 위에서 생성한 데이터프레임은 한개의 Columns 과 1000개의 row 로 구성되어있습니다.
  - 각 row 는 0~99까지의 값이 할당되어 있습니다.
  - 이 숫자들은 분산 컬렉션을 나타냅니다. 즉 클러스터 모드에서 위 예제를 실행하면 숫자 범위의 각 부분이 서로 다른 익스큐터에 할당됩니다. 

> ## DaraFrame

- 데이터프레임은 가장 대표적인 구조적 API 입니다.
  - 이때 컬럼과 컬럽의 타입을 정의한 목록을 스키마 라고 부릅니다.
- 데이터프레임은 칼럼에 이름을 붙인 스프레드 시트와 비슷하다고 생각하면 됩니다. 

![png](/assets/images/Program/10_3.png)

- 위의 그림은 데이터 프레임과 스프레드 시트의 차이점을 보여줍니다.
  - 스프레드시트 : 한 컴퓨터에 존재 
  - 스파크 Dataframe : 수천대의 컴퓨터에 분산
- 물론 R 이나 파이썬에도 데이터프레임이라는 개념이 존재하지만 모두 단일 컴퓨터에 존재합니다.
- 스파크는 파이썬과 R 을 모두 지원하기 떄문에 데이터프레임을 스파크 Dataframe 으로 쉽게 변환할 수 있습니다. 

> ## 파티션

- 스파크는 모든 익스큐터가 병렬로 작업을 수행할 수 있도록, 파티션이라고 부르는 청크 단위로 데이터를 분할합니다.
  - 파티션은 클러스터의 물리적 머신에 존재하는 row 의 집합을 의미합니다.
- DataFrame 의 파티션은 실행중 데이터가 컴퓨터의 클러스터에서 물리적으로 분산되는 방식을 나타냅니다. 
  - 파티션이 하나면 스파크에 수천개의 익스큐터가 있어도 병렬성은 1입니다.
  - 수천개의 파티션이 있어도 익스큐터가 하나면 병렬성은 1입니다.
- 데이터 프레임을 사용하면 파티션을 수동으로 처리할 필요가 없습니다.
  - 물리적 파티션에 변환용 함수를 지정하면 스파크가 처리 방식을 결정합니다. (이는 나중에 좀 더 알아보겠습니다.)

> ## 트렌스포메이션

- 스파크의 핵심 데이터 구조는 불변성을 지닙니다. 즉 한번 생성하면 변경할 수 없다는 의미입니다.
- 이는 처음 보면 참 이상한 개념처럼 보입니다. 데이터 구조를 변경할 수 없으면 어떻게 사용하란 말일까요? 
- Dataframe 을 변경하려면 스파크에게 알려줘야 합니다. 
  - 이때 사용하는 명령을 트렌스포메이션 이라고 합니다. 

```python
divisBy2 = myRange.where("number % 2 = 0")
```

- 위 코드를 실행해도 결과는 출력이 안됩니다.
  - 추상적인 트랜스포메이션만 지정했기 때문에, 액션을 호출하지 않으면 스파크에서는 실제 트랜스포메이션을 수행하지 않는 것입니다. 
- 트랜스포메이션은 스파크에서 좁은 의존성과 넓은 의존성 두가지가 있습니다.

> 좁은 의존성 (narrow dependenc)

![png](/assets/images/Program/10_4.png)

- 좁은 의존성을 가지는 Transformation 은 각 입력 파티션이 하나의 출력 파티션에만 영향을 끼칩니다.
- 이전 코드예제에서는 where 구문은 좁은 의존성을 가집니다.
  - 즉 위에서 볼 수 있듯이 하나의 파티션이 하나의 출력 파티션에만 영향을 미칩니다. 

```
// 좁은 종속성중의 하나. rdd에 map 연산으로 (x, 1) 의 튜플로 만든다.
val rdd2 = rdd1.map(x => (x, 1))
```

> 넓은 의존성 (wide dependency)

- 넓은 의존성을 가지는 트랜스포메이션은 하나의 입력 파티션이 여러 출력 파티션에 영향을 끼칩니다.
- 넓은 의존성은 여러 파티션의 정보가 연산에 필요한 경우여서 각 파티션들의 데이터를 셔플하여 시간이 오래 걸림. (SUM 등)

![png](/assets/images/Program/10_5.png)

- 이에 대해서는 나중에 더 깊게 다루어보도록 하겠습니다.

```
// 넓은 종속성, groupKey
val rdd3 = rdd2.groupByKey()
# key 로 구분되게 파티션을 한 뒤에 계산이 이루어져야함
```

> ## 지연 연산(Lazy evaluation)

> Definition

- 지연 연산은 스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식을 의미합니다.
- 스파크는 특정 연산 명령이 내려진 즉시 데이터를 수정하지 않고, 데이터에 적용할 트랜스포메이션의 실행 계획을 생성합니다.
  - 스파크는 마지막 순간까지 대기하다가, 원형 데이터프레임을 간결한 물리적 실행 계획으로 컴파일합니다.

> 지연연산의 장점

- 이렇게 연산을 마지막까지 미루는게 대체 어떤 이득이 있을까요?
  - Lazy evaluation 을 사용함으로써 action이 시작되는 시점에 트랜스포메이션(transformation)끼리의 연계를 파악해 실행 계획의 최적화가 가능해 집니다.
  - 즉 사용자가 입력한 변환 연산들을 즉시 수행하지 않고 모아뒀다가 가장 최적의 수행 방법을 찾아 처리하는 장점을 가진다.
- 예를 들어 물건을 사오는 심부름을 시킬 때 A상점에서 파는 물건과 B상점에서 파는 물건을 따로따로 여러 번사오게 하는 것보다 필요한 물건을 한꺼번에 주문해서 한 번 방문했을 때 필요한 물건을 한 번에 사는 것이 효율적인것과 마찬가지인 거입니다.

> ## 액션 (Action)

- 사용자는 Transformation 을 사용하여 논리적 실행 계획을 세울 수 있습니다.
  - 하지만 실제 연산을 수행하려면 결국에는 액션 명령을 내려야 합니다.
- 액션은 트렌스포메이션으로부터 결과를 계산하도록 지시하는 명령어입니다. 

```
divisBy2.count()
# 500
```

- 위처럼 가장 단순한 액션인 count 는 데이터프레임의 전체 record 의 수를 반환합니다. 

> 액션의 종류

- 콘솔에서 데이터를 보는 액션
- 각 언어로 된 네이티브 객체에 데이터를 모으는 액션
- 출력 데이터소스에 저장하는 액션

> 스파크 잡

- 액션을 지정하면 스파크 잡이 시작됩니다.
- 스파크 잡은 필터 ( 좁은 트랜스포메이션 )를 수행한 뒤, 파티션별로 레코드 수를 카운트 (넓은 트랜스포메이션) 하게 됩니다.
- 그리고 각 언어의 네이티브 객체에 데이터를 모읍니다. 

> ## 스파크 UI

- 스파크 ui 는 스파크 잡의 진행 상황을 모니터링할때 사용됩니다.
  - 드라이버 노드의 4040 포트에서 접속이 가능합니다.
  - 로컬모드에서 수행한다면 그 주소는 http://localhost:4040 입니다. 

![png](/assets/images/Program/10_6.png)

- 위처럼 spark 들의 실행 상황을 살펴볼 수 있습니다. 

# [Examples](#link){: .btn .btn--primary}{: .align-center}

> ## Dataframe

- 이번 예제에서는 미국 교통통계국 항공 운항 데이터중 일부를 스파크로 분석해 보겠습니다.

```python
flightData2015 = spark\
    .read\
    .option("inferSchema", "true")\
    .option("header", "true")\
    .csv("./data/flight-data/csv/2015-summary.csv")
```

- 스칼라와 스파크에서 사용하는 Dataframe 은 다수의 로우와 콜럼을 가집니다. 
  - Row 의 수를 알 수 없는 경우, 데이터를 읽는 과정이 지연 연산 Transformation 입니다. 
- 스파크는 각 columns 의 데이터 타입을 추론하기 위해서, 적은 양의 데이터를 읽습니다.

> take 를 통해서 데이터 읽기

![png](/assets/images/Program/10_7.png)

- 위 그림처럼 csv 를 읽고 Dataframe 에서 로컬 배열 또는 리스트 형태로 변환할 수 있습니다.

```
flightData2015.take(3)
[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),
 Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),
 Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]
```

- 위처럼 take 를 호출하여, head 와 같은 결과를 확인할 수 있었습니다.

> sort 

- sort 메서드는 데이터프레임을 변경하지 않습니다. 

![png](/assets/images/Program/10_8.png)

- sort 를 사용하면 위와 같이 새로운 Dataframe 을 생성해 반환합니다.
- take 는 array 를 만들어냈던것을 기억하세요!

```
flightData2015.sort('count').explain()
```

```
== Physical Plan ==
*(1) Sort [count#28 ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(count#28 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#57]
   +- FileScan csv [DEST_COUNTRY_NAME#26,ORIGIN_COUNTRY_NAME#27,count#28] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/gorany/Desktop/Python/pyspark/Spark-The-Definitive-Guide/data/fligh..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>
```

- 위처럼 explain 을 호출하면 Datagrame 의 게보나, 스파크의 쿼리 실행계획을 확인할 수 있습니다.
- 이러한 실행 계뢱은 다소 어려워 보이지만 조금 연습하면 읽기 쉽습니다. (나중에 연습하도록 해요...)

> Action 

- 이제 트랜스포메이션 실행 계획을 시작하기 위하여 액션을 호출해봅시다. 
- 액션을 실행하려면 몇가지 설정이 필요한데요, 스파크는 셔플 수행시 기본적으로 200개의 셔플 파티션을 생성합니다.
  - 이 값을 5로 설정하여 셔플의 출력 파티션 수를 줄여봅시다. 

```
spark.conf.set('spark.sql.shuffle.partitions','5')
```

```
flightData2015.sort('count').take(2)
```

```
[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),
 Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]
```

![png](/assets/images/Program/10_9.png)

- 위는 코드에서의 실행 계획을 그림으로 표현한 것입니다.
- 스파크는 셔플 파티션 수를 5로 설정하여, 5개의 출력 파티션이 생성됩니다.
  - 이 값을 바꾸면 스파크 잡의 특성을 제어할 수 있습니다. 

> ## Dataframe and sql

- 이제는 Dataframe 과 sql 을 사용하는 작업을 수행해 봅시다.
- 스파크는 언어에 상관 없이 같은 같은 방식으로 트랜스포메이션을 수행할 수 있습니다.
- 스파크 SQL 을 사용하면, 모든 Dataframe 을 테이블이나 뷰(임시 테이블) 로 등록한 후 SQL 쿼리를 사용할 수 있습니다.
  - 스파크는 SQL 쿼리를 Dataframe 코드와 같은 실행계획으로 컴파일 하므로 성능 저하는 없습니다. 

```python
flightData2015.createOrReplaceTempView('flight_data_2015')
```

- 위와 같이 createOrReplaceTempview 메서드를 호출하면 모든 Dataframe 을 테이블이나 뷰로 만들 수 있습니다. 
  - 이제 우리는 SQL 로 데이터를 조회할 수 있게 됩니다. 
- 새로운 데이터 프레임을 반환하는 sparksql 메서드로 sql 쿼리를 실행합니다.
- spark 는 SparkSession 의 변수입니다.
  - DataFrame 에 쿼리를 수행하면 새로운 Dataframe 을 반환합니다. 
- 이는 매우 효율적입니다. 

```python
sqlWay = spark.sql("""
SELECT DEST_COUNTRY_NAME, count(1)
FROM flight_data_2015
GROUP BY DEST_COUNTRY_NAME
""")

dataFrameWay = flightData2015\
  .groupBy("DEST_COUNTRY_NAME")\
  .count()
```

- 위 두가지의 실행 계획에 대해서 살펴봅시다. 

```
sqlWay.explain()

== Physical Plan ==
*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#26], functions=[count(1)])
+- Exchange hashpartitioning(DEST_COUNTRY_NAME#26, 5), ENSURE_REQUIREMENTS, [id=#86]
   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#26], functions=[partial_count(1)])
      +- FileScan csv [DEST_COUNTRY_NAME#26] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/gorany/Desktop/Python/pyspark/Spark-The-Definitive-Guide/data/fligh..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>
```

```
dataFrameWay.explain()

== Physical Plan ==
*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#26], functions=[count(1)])
+- Exchange hashpartitioning(DEST_COUNTRY_NAME#26, 5), ENSURE_REQUIREMENTS, [id=#105]
   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#26], functions=[partial_count(1)])
      +- FileScan csv [DEST_COUNTRY_NAME#26] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/gorany/Desktop/Python/pyspark/Spark-The-Definitive-Guide/data/fligh..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>
```

- 두가지 플랜은 동일한 기본 실행계획을 가지고 있음을 알 수 있습니다

---

**Reference**

- <https://brocess.tistory.com/187>
- 스파크 완벽 가이드
- <https://brocess.tistory.com/104>

