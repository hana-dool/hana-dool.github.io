---
title:  "df.repartition"
excerpt: "데이터 분할하기"
categories:
  - Spark_df_method
last_modified_at: 2021-11-27

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

 데이터프레임
{: .notice--warning}

# [df.repartition](#link){: .btn .btn--primary}{: .align-center}

> ## 공식문서

> pyspark.sql.DataFrame.repartition(numPartitions, *cols)

- *New in version 1.3.0.*
  - Returns a new [`DataFrame`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame) partitioned by the given partitioning expressions. The resulting [`DataFrame`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame) is hash partitioned.

- Parameters
  - numPartitions : int
    - can be an int to specify the target number of partitions or a Column. If it is a Column, it will be used as the first partitioning column. If not specified, the default number of partitions is used.

  - cols : str or Column
    - partitioning columns


> Example 

```python
df.repartition(10).rdd.getNumPartitions()
# 10
data = df.union(df).repartition("age")
data.show()
#+---+-----+
#|age| name|
#+---+-----+
#|  2|Alice|
#|  5|  Bob|
#|  2|Alice|
#|  5|  Bob|
#+---+-----+
data = data.repartition(7, "age")
data.show()
+---+-----+
|age| name|
+---+-----+
|  2|Alice|
|  5|  Bob|
|  2|Alice|
|  5|  Bob|
+---+-----+
data.rdd.getNumPartitions()
7
data = data.repartition(3, "name", "age")
data.show()
+---+-----+
|age| name|
+---+-----+
|  5|  Bob|
|  5|  Bob|
|  2|Alice|
|  2|Alice|
+---+-----+

```

> ## 설명

- 최적화 기법중 하나는 , 자주 필터링하는 컬럼을 기준으로 데이터를 분할하는 것입니다.
- 이를 통해, partitioning 스키마와 파티션 수를 포함해 클러스터 전반의 물리적 데이터 구성을 제어할 수 있습니다.
- repartition 메서드를 호출하게 되면, 무조건 전체 데이터를 셔플하게 됩니다. 향후에 사용될 파티션 수가 현재 파티션수보다 많거나, 컬럼을 기준으로 파티션을 만드는 경우에만 사용하는것이 좋습니다. 

**Reference**

- 스파크 완벽 가이드
- 스파크 공식 Docs

