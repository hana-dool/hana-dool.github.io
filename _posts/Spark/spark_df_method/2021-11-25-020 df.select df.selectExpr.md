---
title:  "df.select df.selectExpr"
excerpt: "select 를 통해서 살펴보기"
categories:
  - Spark_df_method
last_modified_at: 2021-11-25

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

select 와 expr 을 통하여 SQL 을 사용해보자
{: .notice--warning}

# [select and seletExpr](#link){: .btn .btn--primary}{: .align-center}

> ## select Method

- 우선 pyspark 의 공식 사이트에서는 어떻게 설명하고 있는지 알아보겠습니다. 

Projects a set of expressions and returns a new [`DataFrame`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame). <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.select.html>
{: .notice}

- 위와 같이 `pyspark.sql.DataFrame.select` 메서드는 Expression 에 대한 새로운 Dataframe 을 만들어냅니다.

> 모든 column 을 선택하기

```python
# iris_df 의 모든 column 을 가져오고 출력합니다.
iris_df.select('*').show()

#+------------+-----------+------------+-----------+-------+
#|sepal.length|sepal.width|petal.length|petal.width|variety|
#+------------+-----------+------------+-----------+-------+
#|         5.1|        3.5|         1.4|        0.2| Setosa|
#|         4.9|        3.0|         1.4|        0.2| Setosa|
#|         4.7|        3.2|         1.3|        0.2| Setosa|
#......
```

> 특정한 column 을 선택하기

- 이밖에 select 를 이용하여 여러 컬럼을 선택할수도 있습니다.

```python
iris_df.select('sepal_width','sepal_length').show()
#+-----------+------------+
#|sepal_width|sepal_length|
#+-----------+------------+
#|        3.5|         5.1|
#|        3.0|         4.9|
#|        3.2|         4.7|
#|        3.1|         4.6|
#..............
```

- 위처럼 여러개의 column 을 선택할 수 있습니다.

```python
df.select('*').collect()
# [Row(age=2, name='Alice'), Row(age=5, name='Bob')]
df.select('name', 'age').collect()
# [Row(name='Alice', age=2), Row(name='Bob', age=5)]
df.select(df.name, (df.age + 10).alias('age')).collect()
# [Row(name='Alice', age=12), Row(name='Bob', age=15)]
```

- 위와 같이 여러가지 방법으로, column 을 선택하고 출력할 수 있습니다.
  - 이때 collect() 는 , 출력 결과를 배열로 직접 얻어내는 방법입니다. 자세한것은 collect 문서보기

> ## Expr

> Expr

- expr 함수는 다양한 표현을 쉽게 할 수 있습니다. 다음과 같은 이점이 있습니다. 
  - 내부에서 sql 문법을 사용할 수 있다.
  - 산술 연산도 가능하다.
  - 타입 변환도 가능
  - 또한 조건문 자리에(예를 들면 filter 함수에 들어가는 조건문) expr 을 사용하면 sql 비교연산자를 사용하여 조건문을 완성 가능 등...
- 위와 같이 유연한 표현이 가능하므로 이점이 있는데요, 그 예시는 다음과 같습니다.

```python
# SQL 함수 사용 예제 (sql 문법 사용)
df.select(df.date,df.increment, expr("add_months(date,increment)") .alias("inc_date")).show()

# 산술 연산 예제
df.select(df.date, df.increment, expr("increment + 5 as new_increment") )

# 타입 변환 예제 : long type 을 string type 으로 바꿈
df.select("increment", expr("cast(increment as string) as str_increment"))

# 조건문에 넣는 예제
df = spark.createDataFrame(data).toDF("col1","col2")
df = df.filter(expr("col1 == col2")) 
```

- 각각에 대해서는 다른 문서 (expr) 에서 알아보도록 하고 여기에서는 이러한 expr 은 매우 '유연하고 강력하다!' 정도만 알면 될것같습니다.

> Example

```python
iris_df.select(expr('sepal_length AS new')).show()
#+---+
#|new|
#+---+
#|5.1|
#|4.9|
#|4.7|
```

- 위처럼 코드를 짜게 된다면, 다음 구문이 실행됩니다.
  - expr 을 통해서 sepal_length $\to$ new 로 이름이 바뀝니다.
  - select 를 통해서 바뀐 column 을 출력합니다.

> ## selectExpr

- 스파크 홈페이지에서 selectExpr 이 어떻게 설명되는지 찾아봅시다.

> Spark Homepages

- `DataFrame.``selectExpr`(**expr*)]
  - Projects a set of SQL expressions and returns a new [`DataFrame`](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame).
  - This is a variant of [`select()`](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.select.html#pyspark.sql.DataFrame.select) that accepts SQL expressions.

> Note

- 즉 selectExpr 은 expr 메서드와 select 메서드를 합친것이라고 생각하시면 편합니다. 
- 이런 seletExpr 은 스파크에서 자주 사용하는 인터페이스중에 하나입니다! (매우 편리!)

```python
iris_df.select(expr('sepal_length AS new'),'petal_length').show(3)
#+---+------------+
#|new|petal_length|
#+---+------------+
#|5.1|         1.4|
#|4.9|         1.4|
#|4.7|         1.3|
```

- 위처럼 expr 을 사용한다면 column 의 이름을 바꾸는것과, 선택하는것까지 동시에 수행할 수 있는데요, 이는 selectExpr 메서드로 한번에 가능합니다.

```python
iris_df.selectExpr('sepal_length AS new','petal_length').show(3)
#+---+------------+
#|new|petal_length|
#+---+------------+
#|5.1|         1.4|
#|4.9|         1.4|
#|4.7|         1.3|
```

- 위처럼 selectExpr 을 통해서 selet + Expr 을 동시에 사용 가능합니다.!

# [selectExpr Example](#link){: .btn .btn--primary}{: .align-center}

> ## col 이름 변경 및 출력

```python
iris_df.selectExpr('sepal_length AS new','petal_length').show(3)
#+---+------------+
#|new|petal_length|
#+---+------------+
#|5.1|         1.4|
#|4.9|         1.4|
#|4.7|         1.3|
```

> ## 새로운 col 추가하기

- 아래처럼 `*` 옆에 expr 을 통하여 새로 열을 추가할 수 있습니다.

```python
iris_df.selectExpr('*','petal_length=1.4 as check'  ).show(3)
#+------------+-----------+------------+-----------+-------+-----+
#|sepal_length|sepal_width|petal_length|petal_width|variety|check|
#+------------+-----------+------------+-----------+-------+-----+
#|         5.1|        3.5|         1.4|        0.2| Setosa| true|
#|         4.9|        3.0|         1.4|        0.2| Setosa| true|
#|         4.7|        3.2|         1.3|        0.2| Setosa|false|
#+------------+-----------+------------+-----------+-------+-----+
```

- 위처럼 `petal_length=1.4 ` 를 검사하고, 이게 맞다면 check col 에 true 를 출력하고, 그렇지 않으면 false 를 출력합니다.

> ## 집계함수 지정하기

- select 를 이용하여 집계함수를 출력할수도 있습니다.

```python
iris_df.selectExpr('avg(sepal_length)','count(distinct(petal_width))').show()
#+-----------------+---------------------------+
#|avg(sepal_length)|count(DISTINCT petal_width)|
#+-----------------+---------------------------+
#|5.843333333333334|                         22|
#+-----------------+---------------------------+
```

- 위와 같이 selectExpr 을 통하여 집계함수를 출력하고 있습니다!

**Reference**

- 스파크 완벽 가이드
- https://spark.apache.org/docs/latest/sql-getting-started.html

