---
title:  "df.GroupBy"
excerpt: "데이터 컬럼을 그룹바이 하기"
categories:
  - Spark_df_method
last_modified_at: 2022-01-18

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

 Group By
{: .notice--warning}

# [df.GroupBy](#link){: .btn .btn--primary}{: .align-center}

> ## 공식문서

```python
from pyspark.sql.functions import sum
df.select(sum("Quantity")).show() # 5176450
```

- 위와 같은 수준의 집계는 DataFrame 수준의 집계 입니다.
- 하지만 대부분은 데이터 그륩 기반의 집계를 수행하는 경우가 더 많습니다. 
- 데이터 그릅 기반의 집계는 단일 컬럼의 데이터를 그룹화하고 해당 그릅 의 다른 여러 컬럼을 사용해서 계산하기 위해 카테고리형 데이터 catcogonical data를 사용합니다.
-  데이터 그롭 기반의 집계를 설명하는 데 가장 좋은 방법은 그큡화를 해보는 겁니다. 이전에 했던 것처럼 카운트를 가장 먼저 수행합니다. 
- 고유한 송장번호(InvoiceNo)를 기준으로 그룹을 만들고 그룹별 물품 수를 카운트합니다. 이 연산은 또 다른 DataFrame을 반환하며 지연 처리 방식으로 수행됩나다.
- 그룝화 작업은 하나 이상의 컬럼을 그룹화하고 집계 연산을 수행하는 두 단계로 이루어집니다.
  - 첫번째 단계에서는 RelationalGroupedDataset이 반환됩니다.
  - 두 번째 단계에서는 DataFrame 이 반환됩니다. 
- 그륩의 기준이 되는 컬럽을 여러 개 지정할 수 있습니다.

```python
df.groupBy('InvoiceNo')
```

```
<pyspark.sql.group.GroupedData at 0x7fdc41f44d30>
```

- Group By 를 수행하면 위처럼 groupedData 가 생성이 됩니다.

```python
df.groupBy('InvoiceNo').count().show(4)
```

```py
+---------+-----+
|InvoiceNo|count|
+---------+-----+
|   536596|    6|
|   536938|   14|
|   537252|    1|
|   537691|   20|
+---------+-----+
```

- 위처럼 GroupBy 를 이용한 집계를 수행할 수 잇습니다.

> ## Agg 메서드의 활용

- count 함수를 select 구문에 표현식으로 지정하는 것보다 agg 메서드를 사용하는 것이 좋습니다.
-  Agg 매서드는 여러 집계 치리를 한 번에 지정할 수 있으며, 집계에 표현식을 사용할 수 있습니다. 또한 트련스포메이션이 완료된 컬럼에 alias 메서드를 사 용할 수 있습니다

```python
from pyspark.sql.functions import count
df.groupBy("InvoiceNo").agg(
    count("Quantity").alias("quan"),
    expr("count(Quantity)")).show()
```

```
+---------+----+---------------+
|InvoiceNo|quan|count(Quantity)|
+---------+----+---------------+
|   536596|   6|              6|
|   536938|  14|             14|
|   537252|   1|              1|
.......
```

- 우처럼 dataframe 에 대해서 다양한 수준의 집계를 나타내는것을 볼 수 있습니다.
- 또한 alias 를 활용해서 별칭을 지정할수도 있죠.

---

**Reference**

- 스파크 완벽 가이드
- 스파크 공식 Docs

