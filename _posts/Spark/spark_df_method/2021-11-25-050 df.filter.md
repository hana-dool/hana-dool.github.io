---
title:  "df.filter"
excerpt: "데이터 필터링하기"
categories:
  - Spark_df_method
last_modified_at: 2021-11-26

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

데이터타입 바꾸기
{: .notice--warning}

# [df.filter](#link){: .btn .btn--primary}{: .align-center}

> ## 공식문서

> pyspark.sql.DataFrame.filter(condition)

- Filters rows using the given condition.
  - [`where()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.where.html#pyspark.sql.DataFrame.where) is an alias for [`filter()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.filter.html#pyspark.sql.DataFrame.filter).


> parameter

- condition : Column or str
  - a Column of types.BooleanType or a string of SQL expression.

> ## 설명

- SQL 의 where 처럼 로우를 필터링할 수 있는 기능입니다. 
- 로우를 필터링하려면 참 / 거짓을 판별하는 표현식을 만들어야 합니다. 그러면 표현식에서 True 라고 판별되는 애들만 잡아낼 수 있습니다! 
- 이때 필터링을 위해서는 Dataframe 의 where 또는 filter 메서드로 필터링할 수 있습니다.
  - 두개 모두 동일하다는것을 알아주세요! 



```python
df.where('count<2').show(2)
#+-----------------+-------------------+-----+
#|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
#+-----------------+-------------------+-----+
#|    United States|            Croatia|    1|
#|    United States|          Singapore|    1|
#+-----------------+-------------------+-----+
```

- 위의 코드는 dataframe 에서 count 를 기준으로 2보다 작은값으로 필터링을 할 수 있습니다.

> ## 여러개의 필터

- 같은 표현식에 여러개의 필터를 적용해야할 수도 있을것입니다.
- 하지만 스파크는 자동으로 , 필터의 순서와 상관없이 '동시에 모든 필터링 작업을 수행' 하기 때문에, 다수의 필터링을 차례로 적용하는건 그다지 권장하지는 않습니다.

```python
df.where(col("count") < 2).where(col("ORIGIN_COUNTRY_NAME") != "Croatia")\
  .show(2)
#+-----------------+-------------------+-----+
#|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
#+-----------------+-------------------+-----+
#|    United States|          Singapore|    1|
#|          Moldova|      United States|    1|
#+-----------------+-------------------+-----+

```

- 위와 같이 where 을 여러개 사용하여 차례로 연결하고, 뒤의 판단은 스파크에 맡겨야 합니다.

**Reference**

- 스파크 완벽 가이드
- <https://statkclee.github.io/bigdata/bigdata-pyspark-data-transformation.html>
- <https://jaeyung1001.tistory.com/61>



