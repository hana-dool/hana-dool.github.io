---
title:  "df.withColumn"
excerpt: "column 추가하기"
categories:
  - Spark_df_method
last_modified_at: 2021-11-25

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

select 와 expr 을 통하여 SQL 을 사용해보자
{: .notice--warning}

# [select and seletExpr](#link){: .btn .btn--primary}{: .align-center}

> ## withColumn

> pyspark.sql.DataFrame.withColumn

- `DataFrame.withColumn`(colName, col)
- Returns a new [`DataFrame`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame) by adding a column or replacing the existing column that has the same name.
- The column expression must be an expression over this [`DataFrame`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame); attempting to add a column from some other [`DataFrame`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame) will raise an error.

> Parameter

- **Parameters**
  - colName : str
  - col : Column
    - a Column expression for the new column.

> Note

- 일반적으로 Dataframe 에 신규 컬럼을 추가하는 방식은 Dataframe 의 withColumn 메서드를 사용하는것을 권장합니다. 
- `df.withColumn(새로운 column 이름, 삽입할 값)` 의 명령어를 실시하면, 새로운 column 을 추가합니다.
  - 숫자 1 을 가지는 칼럼을 추가하는것은 아래의 예시와 같습니다.

```python
from pyspark.sql.functions import lit
df.withColumn('numberone',lit(1)).show(2)

#+-----------------+-------------------+-----+---------+
#|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberone|
#+-----------------+-------------------+-----+---------+
#|    United States|            Romania|   15|        1|
#|    United States|            Croatia|    1|        1|
#+-----------------+-------------------+-----+---------+
```

- 이때 위에서 'numberone' 은 새로운 column 의 이름이 됩니다.
- lit(1) 은 새로운 값이 됩니다.

```python
from pyspark.sql.functions import lit
df.withColumn('numberone',1).show(2)

# AssertionError: col should be Column
```

- 이때 위처럼 수행하게 되면 에러가 나는데요, 왜냐하면 '추가하는 값은 Column' 이여야 하기 때문이죠
  - 그래서 그냥 int 를 추가하는것은 불가능합니다!

> ## Example 

```python
df.withColumn("withinCountry", expr("ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME"))\
  .show(2)
#+-----------------+-------------------+-----+-------------+
#|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|
#+-----------------+-------------------+-----+-------------+
#|    United States|            Romania|   15|        false|
#|    United States|            Croatia|    1|        false|
#+-----------------+-------------------+-----+-------------+
```

- 위와 같이 expr 과 같이 사용하여서, 출발지 ( origin_country_name) 와 도착지 (dest_country_name) 가 같은지 검사할 수 있습니다.

**Reference**

- 스파크 완벽 가이드
- https://spark.apache.org/docs/latest/sql-getting-started.html

