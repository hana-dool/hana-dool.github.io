---
title:  "sc.read.csv"
excerpt: "스파크 csv 읽기"
categories:
  - Spark_other_method
last_modified_at: 2021-11-26

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

스파크의 기본만 한번 알아봅시다.
{: .notice--warning}

# [spark.read.csv](#link){: .btn .btn--primary}{: .align-center}

> ## Definition

-  스파크 2.0 부터는 csv 를 읽는 기능이 Spark 에 내장되었습니다. 그러므로 다음과 같이 쉽게 데이터를 읽을 수 있습니다.

```python
spark.read.csv("iris.csv")
```

```
+------------+-----------+------------+-----------+-------+
|         _c0|        _c1|         _c2|        _c3|    _c4|
+------------+-----------+------------+-----------+-------+
|sepal.length|sepal.width|petal.length|petal.width|variety|
|         5.1|        3.5|         1.4|         .2| Setosa|
|         4.9|          3|         1.4|         .2| Setosa|
|         4.7|        3.2|         1.3|         .2| Setosa|
|         4.6|        3.1|         1.5|         .2| Setosa|
|           5|        3.6|         1.4|         .2| Setosa|
|         5.4|        3.9|         1.7|         .4| Setosa|
.....
```

- 위와 같이, iris.csv 데이터를 읽을수 있기는 합니다. 
- 하지만 header 가 제대로 지정되어있지 않는 등 문제가 있습니다. 그러므로 이런 경우에 사용할 수 있는 옵션들을 제대로 지정해주어야 합니다. 
  - 이러한 옵션들에 대해서 짧게 알아봅시다. 

> ## Spark SCV Options

- `sep` :구분자
  - default: `,`
  - Spark 1.6 방식에서는 `delimiter`를 사용해야 한다
- `encoding`
  - default: `UTF-8`
- `quote` : value가 큰 따옴표로 묶인 경우 `"` 를 지정
  - defualt: `"`
- `escape`: 구분자가 value안에 사용된 경우 escape를 처리할 문자
  - default: `\`
- `comment` : 코멘트의 시작을 알리는 문자
  - default: `#`
- `header`: 첫 줄이 data가 아닌 헤더인 경우 `"true"` 를 지정합니다.
  - default: `false`
- `inferSchema`: schema를 Spark이 자동으로 알아내는 경우 사용
  - default: `schema`
- `samplingRatio` : schema inferring 시에 data의 얼마를 샘플링할지
  - default: `1.0`
- `ignoreLeadingWhiteSpace`: value의 앞에 있는 공백을 제거할지 여부
  - default: `false`
- `ignoreTrailingWhiteSpace` : value의 뒤에 있는 공백을 제거할지 여부
  - default: `false`
- `nullValue` : `null` 을 표현하는 문자열
  - default: empty string
- `emptyValue`: 공백을 표현하는 문자열
  - default: empty string
- `nanValue` : `non-number` 를 표현하는 문자열
  - default: `NaN`
- `positiveInf`: “양의 무한대”를 표현하는 문자열
  - default: `Inf`
- `negativeInf` : “음의 무한대”를 표현하는 문자열
  - default: `-Inf`
- `dateFormat` : 날자 포맷을 지정하는 문자열.
  - default: `yyyy-MM-dd`
- `maxColumns` : 최대 필드 개수
  - default: `20480`
- `maxCharsPerColumn`: 1개 필드에서 최대 문자열의 길이
  - default: `-1` (즉, 제한없음)
- `mode`: 에러 처리에 관련된 옵션. 개인적으로 csv를 다를 때 가장 중요한 옵션이라 생각한다
  - `PERMISSIVE` (default): 잘못된 포맷의 line을 만나면 해당 문자열을 `columnNameOfCorruptRecord`에서 지정한 필드에 저장하고, 나머지 필드는 모두 `null`로 설정한다
  - `DROPMALFORMED`: 잘못된 문자열의 레코드는 무시한다
  - `FAILFAST`: 잘못된 문자열을 레코드를 만나면 에러를 출력한다

> ## Read options

```python
iris_df = spark.read.csv("iris.csv", inferSchema = True, header = True)
```

- 위와 같이 지정하여 iris.csv 를 읽는다면 다음과 같이 수행을 하게 됩니다.
  - inferschema = True : 스파크가 알아서 데이터의 유형 파악
  - header = True : csv 의 헤더 부분을, 데이터가 아니라 colname 으로 파악
- 위와 같이 지정하고, 우리가 읽은 파일을 살펴볼까요?

```python
iris_df.show(4)
#+------------+-----------+------------+-----------+-------+
#|sepal.length|sepal.width|petal.length|petal.width|variety|
#+------------+-----------+------------+-----------+-------+
#|         5.1|        3.5|         1.4|        0.2| Setosa|
#|         4.9|        3.0|         1.4|        0.2| Setosa|
#|         4.7|        3.2|         1.3|        0.2| Setosa|
#|         4.6|        3.1|         1.5|        0.2| Setosa|
#+------------+-----------+------------+-----------+-------+
```

- 위와 같이 Header 들을 제대로 읽은것을 볼 수 있구요, 

```python
iris_df.printSchema()
#root
# |-- sepal.length: double (nullable = true)
# |-- sepal.width: double (nullable = true)
# |-- petal.length: double (nullable = true)
# |-- petal.width: double (nullable = true)
# |-- variety: string (nullable = true)
```

- 데이터도 제대로 읽은것을 볼 수 있습니다!

Hide

**Reference**

- <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.createOrReplaceTempView.html>



