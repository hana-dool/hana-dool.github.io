---
title:  "Boolean"
excerpt: "조건절과 같이 사용해보기"
categories:
  - Spark_Datatype
last_modified_at: 2021-11-27

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

 데이터프레임
{: .notice--warning}

# [Boolean DataType](#link){: .btn .btn--primary}{: .align-center}

> ## 불리언 데이터 타입 만들기

- 불리언은 모든 필터링 작업의 기반이 됩니다. (And , or , true 등으로 구성되기 때문)
- 불리언 구문을 사용해서 `ture` , `false` 로 평가되는 논리 문법을 만들게 됩니다.
  - 논리 문법은 데이터 로우를 필터링할때 필요조건의 일치 (`ture` , `false` 를 판별할때에 사용되게 됩니다. )

```python
from pyspark.sql.functions import col
df.where(col("InvoiceNo") != 536365)\
  .select("InvoiceNo", "Description")\
  .show(5)
```

- 위는 `col("InvoiceNo") != 536365` 라는 조건식 ( 즉 invoiceNo 가 536365 가 아닐때) 으로 필터링을 진행하는 예시입니다.

> ## 여러개의 필터에 적용하기

- 차례대로 여러개의 필터를 적용하고 싶은 경우, `and` 또는 `or` 메서드를 사용해서 불리언 표현식을 여러개 묶을 수 있습니다.
  - 하지만 이때 주의해야 할것은 `항상 모든 표현식을 and 로 묶어 차례대로 필터를 적용` 해야 한다는 것입니다.
- 차례대로 필터를 적용해야 하는 이유는 , 스파크는 boolean 문을 차례로 표현한다 하더라도, 스파크는 내부적으로 and 구문을 필터 사이에 추가해서 모든 필터를 하나의 문장으로 변환한 뒤에 필터들을 처리하기 때문입니다.
  - 즉 and 구문으로 조건문을 만들어도 되지만 어짜피 나중에 합치는것은 filter 를 여러번 사용하는것이나, and 로 연결하는것이나 같으므로 filter 를 여러번 사용하는게 가독성 면에서 좋습니다.
  - 반면에 or 구문을 사용하게 된다면 동일한 구문에 조건을 정의해야 합니다.
- 구체적으로 어떻게 사용하는지는 아래의 예시를 통해서 알아봅시다.

```python
from pyspark.sql.functions import instr
priceFilter = col("UnitPrice") > 600
descripFilter = instr(df.Description, "POSTAGE") >= 1
df.where(df.StockCode.isin("DOT")).where(priceFilter | descripFilter).show()
#+---------+---------+--------------+--------+-------------------+---------+----------+--------------+
#|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|
#+---------+---------+--------------+--------+-------------------+---------+----------+--------------+
#|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|
#|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|
#+---------+---------+--------------+--------+-------------------+---------+----------+--------------+
```

- 위처럼 df 에 적용할 필터들을 먼저 여러개 `priceFilter , descripFilter` 만듭니다. (이때 각 filter 들은 False / True 를 반환하게 되는 boolean 구문들입니다! )
- 그 이후에 df.where 에 우리가 사전에 정의한 filter 들을 여러개 묶어서 처리합니다.

> ## 데이터프레임을 필터링하기

```python
from pyspark.sql.functions import instr
# instr : 일치하는 문자의 위치를 반환함
DOTCodeFilter = col("StockCode") == "DOT"
priceFilter = col("UnitPrice") > 600
descripFilter = instr(col("Description"), "POSTAGE") >= 1
df.withColumn("isExpensive", DOTCodeFilter & (priceFilter | descripFilter))\
  .where("isExpensive")\
  .select("unitPrice", "isExpensive").show(5)

#+---------+-----------+
#|unitPrice|isExpensive|
#+---------+-----------+
#|   569.77|       true|
#|   607.49|       true|
#+---------+-----------+
```

- 위와 같이 필터링조건을 이용해서 살펴보는게 아니라, Boolean 컬럼을 활용하여, 데이터프레임을 필터링 할 수 있습니다.
  - where 구문에서 단지 `isExpensive` 컬럼만 조건으로 되어있는데요, 이 컬럼이 true / false 를 나타내므로 `isExpensive` 컬럼이 true 인 부분만 남기게 된다고 생각하시면 됩니다.
- 즉 withColumn 으로 새로운 컬럼을 만들고, 이 컬럼의 True / False 를 이용해서 Dataframe 을 필터링 하고 있습니다.
  - 위 구문이 어떻게 작동되는지 약간 헷갈린다면 아래의 `SQL` 같이 작동된다고 생각하시면 됩니다.

```sql
# SQL
SELECT UnitPrice, (StockCode = 'DOT' AND
    (UnitPrice > 600 OR instr(Description, "POSTAGE") >= 1)) as isExpensive
FROM dfTable
WHERE (StockCode = 'DOT' AND
	(UnitPrice > 600 OR instr(Description, "POSTAGE") >= 1))
```

- 위의 코드와 똑같이 작동하게 됩니다.

**Reference**

- 스파크 완벽 가이드
- https://spark.apache.org/docs/latest/sql-getting-started.html
- https://stackoverflow.com/questions/64054282/what-do-the-sparksession-appname-and-getorcreate-functions-mean
- https://wikidocs.net/16565

