---
title: "Linear regression and T-Test"
excerpt: "회귀분석과 Mean Test 의 관계"
tags :
  - AB_Stat
last_modified_at: 2022-01-01

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
typora-root-url: ../../../../hana-dool.github.io
---

T Test 도 사실은 mean test 의 일종이다?
{: .notice--warning}

# [Introduction](#link){: .btn .btn--primary}{: .align-center}

> ## Linear regression 은 무적이다.

- 기본적으로 Regression , T-Test , Anova 세개 모두 Regression 으로 표현할 수 있습니다.

> Regeression 

$$\begin{gathered}
Y=\beta_{0}+\beta_{1} X_{\text {(continuous) }}+\varepsilon \\
\text { where } \varepsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)
\end{gathered}$$

> T-test 

$$\begin{aligned}
Y=& \beta_{0}+\beta_{1} X_{(\text {dummy code })}+\varepsilon \\
& \text { where } \varepsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)
\end{aligned}$$

> ANOVA : 1 group 

$$\begin{gathered}
Y=\beta_{0}+\beta_{1} X_{(\text {dummy code })}+\varepsilon \\
\text { where } \varepsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)
\end{gathered}$$

> ANOVA : 2 group

$$\begin{gathered}
Y=\beta_{0}+\beta_{1} X_{(\text {dummy code 1) }}+\beta_{2} X_{(\text {dummy code 2) }}+\varepsilon \\
\text { where } \varepsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)
\end{gathered}$$

> ## Note 

- Regression 의 경우 $X$ 를 Continuous Variable 로 간주합니다. 
- T Test 의 경우 $X$ 는 Dummy code 로 간주합니다. 
  - Treatment group 이면 1 / Control Group 이면 0
- ANOVA 의 경우에도 $X$ 는 Dummy code 로 간주합니다. 
  - Treatment 1 이면 : $X_1 =1$ 
  - Treatment 2 이면 : $X_2 = 1$  ..... 

> ## T-Test and regression

> Two sample T 

- Two Sample T test 에 대해서, 두 그룹의 데이터를 $y_{1} \in \mathbb{R}^{n_{1}}$ and $y_{2} \in \mathbb{R}^{n_{2}}$ 라고 합시다.
- 이때 Two - Sample T Test 를 위 두 데이터에 대해서 적용해봅시다. 
- 먼저, Two Sample T Test 의 가정인 "등분산 Normal" 을 따른다고 합시다.
  - 그러면 아래와 Data 구조는 아래와 같이 정의될 수 있습니다.

$$y_{1 i} \sim N\left(\mu_{1}, \sigma^{2}\right) \Leftrightarrow y_{1 i}=\mu_{1}+\varepsilon_{i}, \varepsilon_{i} \sim N\left(0, \sigma^{2}\right) \text { for } i=1, \ldots, n_{1}$$

$$y_{2 i} \sim N\left(\mu_{2}, \sigma^{2}\right) \Leftrightarrow y_{2 i}=\mu_{2}+\varepsilon_{i}, \varepsilon_{i} \sim N\left(0, \sigma^{2}\right) \text { for } i=1, \ldots, n_{2}$$

- 위를 이제 Matrix Formulation 으로 고쳐볼까요? $n:=n_{1}+n_{2}$ 으로 정의한다면 

$$y \sim N\left(X \beta, \sigma^{2} I_{n}\right)$$

- 각각의 parameter 는 아래와 같습니다. 

$$y:=\left(\begin{array}{c}
y_{11} \\
\vdots \\
y_{1 n_{1}} \\
y_{21} \\
\vdots \\
y_{2 n_{1}}
\end{array}\right) \in \mathbb{R}^{n}, X:=\left(\begin{array}{cc}
1 & 0 \\
\vdots & \vdots \\
1 & 0 \\
0 & 1 \\
\vdots & \vdots \\
0 & 1
\end{array}\right) \in \mathbb{R}^{n \times 2}, \beta:=\left(\begin{array}{c}
\mu_{1} \\
\mu_{2}
\end{array}\right) \in \mathbb{R}^{2}, \text { and } \sigma^{2}>0$$

> Estimation of  two-sample t-test designs

- 이떄 Linear Regression 의 디자인 ($$y \sim N\left(X \beta, \sigma^{2} I_{n}\right)$$) 에 따라서 아래처럼 parameter 가 계산되게 됩니다.

$$\hat{\beta}=\left(\begin{array}{l}
\frac{1}{n_{1}} \sum_{i=1}^{n_{1}} y_{1 i} \\
\frac{1}{n_{2}} \sum_{i=1}^{n_{2}} y_{2 i}
\end{array}\right) \text { and } \hat{\sigma}^{2}=\frac{\sum_{i=1}^{n_{1}}\left(y_{1 i}-\bar{y}_{1}\right)^{2}+\sum_{i=1}^{n_{2}}\left(y_{2 i}-\bar{y}_{2}\right)^{2}}{\left(n_{1}-1\right)+\left(n_{2}-1\right)}$$

- 그에 따라서, 위처럼 계산되게 됩니다.

> Proof : $\beta$

$$\begin{aligned}
&\hat{\beta}=\left(X^{T} X\right)^{-1} X^{T} y\\
&=\left(\left(\begin{array}{cccccc}
1 & \cdots & 1 & 0 & \cdots & 0 \\
0 & \cdots & 0 & 1 & \cdots & 1
\end{array}\right)\left(\begin{array}{cc}
1 & 0 \\
\vdots & \vdots \\
1 & 0 \\
0 & 1 \\
\vdots & \vdots \\
0 & 1
\end{array}\right)\right)^{-1}\left(\begin{array}{cccccc}
1 & \cdots & 1 & 0 & \cdots & 0 \\
0 & \cdots & 0 & 1 & \cdots & 1
\end{array}\right)\left(\begin{array}{c}
y_{11} \\
\vdots \\
y_{1 n_{1}} \\
y_{21} \\
\vdots \\
y_{2 n_{2}}
\end{array}\right)\\
&=\left(\begin{array}{cc}
n_{1} & 0 \\
0 & n_{2}
\end{array}\right)^{-1}\left(\begin{array}{l}
\sum_{i=1}^{n_{1}} y_{1 i} \\
\sum_{i=1}^{n_{2}} y_{2 i}
\end{array}\right)\\
&=\left(\begin{array}{cc}
n_{1}^{-1} & 0 \\
0 & n_{2}^{-1}
\end{array}\right)\left(\begin{array}{c}
\sum_{i=1}^{n_{1}} y_{1 i} \\
\sum_{i=1}^{n_{2}} y_{2 i}
\end{array}\right)\\
&=\left(\begin{array}{l}
\frac{1}{n_{1}} \sum_{i=1}^{n_{1}} y_{1 i} \\
\frac{1}{n_{2}} \sum_{i=1}^{n_{2}} y_{2 i}
\end{array}\right) \text {. }
\end{aligned}$$

> Proof $\hat{\sigma}$

$$\begin{aligned}
&\hat{\sigma}^{2}=\frac{(y-X \hat{\beta})^{T}(y-X \hat{\beta})}{n-p}\\
&=\frac{1}{n_{1}+n_{2}-2}\left(\left(\begin{array}{c}
y_{11} \\
\vdots \\
y_{1 n_{1}} \\
y_{21} \\
\vdots \\
y_{2 n_{2}}
\end{array}\right)-\left(\begin{array}{cc}
1 & 0 \\
\vdots & \vdots \\
1 & 0 \\
0 & 1 \\
\vdots & \vdots \\
0 & 1
\end{array}\right)\left(\begin{array}{l}
\bar{y}_{1} \\
\bar{y}_{2}
\end{array}\right)\right)^{T}\left(\left(\begin{array}{c}
y_{11} \\
\vdots \\
y_{1 n_{2}} \\
y_{21} \\
\vdots \\
y_{2 n_{2}}
\end{array}\right)-\left(\begin{array}{cc}
1 & 0 \\
\vdots & \vdots \\
1 & 0 \\
0 & 1 \\
\vdots & \vdots \\
0 & 1
\end{array}\right)\left(\begin{array}{l}
\bar{y}_{1} \\
\bar{y}_{2}
\end{array}\right)\right)\\
&=\frac{1}{n_{1}+n_{2}-2}\left(\begin{array}{c}
y_{11}-\bar{y}_{1} \\
\vdots \\
y_{1 n_{1}}-\bar{y}_{1} \\
y_{21}-\bar{y}_{2} \\
\vdots \\
y_{2 n_{2}}-\bar{y}_{2}
\end{array}\right)^{T}\left(\begin{array}{c}
y_{11}-\bar{y}_{1} \\
\vdots \\
y_{1 n_{1}}-\bar{y}_{1} \\
y_{21}-\bar{y}_{2} \\
\vdots \\
y_{2 n_{2}}-\bar{y}_{2}
\end{array}\right)\\
&=\frac{\sum_{i=1}^{n_{1}}\left(y_{1 i}-\bar{y}_{1}\right)^{2}+\sum_{i=1}^{n_{2}}\left(y_{2 i}-\bar{y}_{2}\right)^{2}}{n_{1}+n_{2}-2} \text {, }
\end{aligned}$$

- 위 계산된 값은 Pooled Variance 가 됩니다.

> T-Test 

- 이떄 contrast vector $c:=(1,-1)^{T}$ and setting $\beta:=(0,0)^{T}$ 로 둔다면 

$$T_{\beta, c}=\frac{\bar{y}_{1}-\bar{y}_{2}}{\sqrt{\left(n_{1}^{-1}+n_{2}^{-1}\right)} s_{12}}$$

- 위처럼 통계량을 구할 수 있고,  이는 $n-p=n_{1}+n_{2}-2$ 의 자유도를 따르는 분포를 가지게 됩니다.

> Proof 

$$\begin{aligned}
&T_{\beta, c}=\frac{c^{T} \hat{\beta}-c^{T} \beta}{\sqrt{\hat{\sigma}^{2} c^{T}\left(X^{T} X\right)^{-1} c}}\\
&=\frac{\left(\begin{array}{cc}
1 & -1
\end{array}\right)\left(\begin{array}{c}
\bar{y}_{1} \\
\bar{y}_{2}
\end{array}\right)}{\sqrt{s_{12}^{2}\left(\begin{array}{ll}
1 & -1
\end{array}\right)\left(\begin{array}{cc}
n_{1} & 0 \\
0 & n_{2}
\end{array}\right)^{-1}\left(\begin{array}{c}
1 \\
-1
\end{array}\right)}}\\
&=\frac{\bar{y}_{1}-\bar{y}_{2}}{\sqrt{\left(n_{1}^{-1}-n_{2}^{-1}\right)\left(\begin{array}{c}
1 \\
-1
\end{array}\right)} s_{12}}\\
&=\frac{\bar{y}_{1}-\bar{y}_{2}}{\sqrt{\left(n_{1}^{-1}+n_{2}^{-1}\right)} s_{12}} .
\end{aligned}$$

> ## Conclusion

- 여태까지 구했던 Procedure 를 보면 

$$T_{\beta, c}=\frac{\bar{y}_{1}-\bar{y}_{2}}{\sqrt{\left(n_{1}^{-1}+n_{2}^{-1}\right)} s_{12}} \sim T(n_1+n_2-2)$$ 

-  위와 같습니다. 이는 결국 Two Sample T Test (Equal Variance) 와 같은 모양인 것이죠! 

> ## Note 

- 먼저 regression model 부터 확인해봅시다.

$$\begin{aligned}
Y=& \beta_{0}+\beta_{1} X_{(\text {dummy code })}+\varepsilon \\
& \text { where } \varepsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)
\end{aligned}$$

- 위 모델에서 Treatment 에 속하게 되면 X = 1 로 코딩을 한 뒤  

$$\begin{aligned}
&\mu_{control} = E(Y \mid X=0)=\beta_{0} \\
&\mu_{treatment}=E(Y \mid X=1)=\beta_{0}+\beta_{1}
\end{aligned}$$

- 위로부터 우리는 아래와 같은 공식을 이끌어낼 수 있습니다.

$$\mu_{treatment} - \mu_{control} = \beta_1$$

- 즉 $\beta_1$ 은 두 그룹의 Mean Difference 가 됩니다. 
- 이러한 $\beta_1$ 에 대한 inference 를 구하는 식으로 하는 방식도 있습니다. 
- 이런 방식도 결국에 모두 계산하고 나면 이전과 똑같은 결론을 내게 됩니다. 

> ## Inuition

- 이제 왜 이런 결과가 나왔는지에 대해서 대략적으로 회고해봅시다.
- Linear regression 과 T Test (등분산) 의 두 방법론은 Assumption 이 동일합니다. 
  - Assumption : 각 데이터의 error 가 등분산 normal 이라는 가정. 
- 결국 Assumption 이 같고, 테스트하고자 하는것 (평균차) 도 같으니 그 결과가 같게 되는 것입니다.

---

reference 

- http://www.fsb.miamioh.edu/lij14/311_2014_0416.pdf
- https://www.ewi-psy.fu-berlin.de/einrichtungen/arbeitsbereiche/computational_cogni_neurosc/teaching/The_General_Linear_Model_20_211/11_T-tests_and_simple_linear_regression.pdf
- https://be-favorite.tistory.com/24
- https://stats.stackexchange.com/questions/34616/difference-between-regression-analysis-and-analysis-of-variance/34618#34618
- https://docplayer.net/20650123-Statistics-courses-often-teach-the-two-sample-t-test-linear-regression-and-analysis-of-variance.html
