---
title: "Online Experimentation Diagnosis and Troubleshooting Beyond AA Validation"
excerpt: "실험의 Truthworthy 를 위한 다양한방법"
categories:
  - AB_Paper
last_modified_at: 2022-01-13

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

 Metric 을 분석할때 Randomization Unit 과 Analysis Unit 이 다를때에는 어떻게 해야 될까
{: .notice--warning}

# [Introduction](#link){: .btn .btn--primary}{: .align-center}

> ## Abstract 

> Online Experiemnt 의 문제점

- Online Experiment 는 현재 다양한 온라인 회사에서 이용되는 다양한 방법입니다.
  - 게다가 Online Experiment 에 대한 설계는 이론적으로는 매우 간단합니다.

- 그러면 이떄 Oneline Experiment 은 무적일까? 
  - 하지만 실제로 적용해볼떄에는, 결과의 해석을 복잡하게 하고 ,User Experience 에 대한 Conclusion 을 무의미하게 만드는 다양한 문제가 존재합니다. 
  - 이러한 문제들 중 대다수는 발견하기 어렵습니다. 


> 해결책?

- 우리가 먼저 이러한 문제를 인식하고 사전에 진단할 수 있다면, 실험자가 근본적으로 결함이 있는 데이터를 기반으로 Decision 을 내리는 것을 방지할 수 있을것입니다! 
- 온라인 실험을 수행할 때에 제일 먼저 보장되어야 할 것은 데이터 품질 보장입니다.
  - Treatment 를 도입하기 전에 AA a검사를 통해 일부 문제를 발견할 수 있지만, AA 기간에는 많은 문제가 발생하지 않고 AB 기간에만 나타나는 문제가 있습니다.
  - 다른 연구에서는 AB 기간 동안의 문제 해결에 대해 다루지 않았었습니다.
- 이 논문에서 우리는 야후의 다양한 인터넷 소비자 제품에 대한 실험에서 얻은 교훈과 Diagnotic ,절차 , 그리고 문제가 발견되었을떄에 치료 절차에 대해서 알아보겠습니다. 
- 여기에 제시된 대부분의 트러블슈팅 절차는 다른 회사의 문제와도 일치합니다. 예를 들면 트래픽 분할문제, 이상치 문제와 같은 경우는 다른 연구에서도 활발히 논의되었스빈다
  - 하지만 다른것들은 이전의 문헌들에서 기술되지 않았습니다.

> ## Introduction

- AB Test 라고 알려져있는 Online Controlled Experiments 는 Control(A)과 그 변형 버전인 Treatment(B) 에 대한 Randomization Experiment 를 일컫습니다.
- 이때 A 가 더 좋은지 B 가 더 좋은지를 결정하기 위해 실험완료 (또는 실험 도중) 에 다양한 행동 및 성능 메트릭을 분석하게 됩니다.
  - AB 테스트는 구글, 페이스북, 링크드인, 마이크로소프트, 야후, 아마존, 이베이, 넷플릭스, 징가, 우버, 에어비앤비, 핀터레스트 및 기타 많은 인터넷 기업에서 제품 향상과 개발에 널리 사용되고 있습니다.
- 하지만 이떄 변경사항 ( Treatment ) 가 사용자의 경험에 예기치 않게 안좋은 영향을 미칠 수 있으므로, 안전하게 실험을 하기 전에는 변경사항이 User 에게 영향을 끼치는지 아닌지와 관계 없이 Production 을 한번 테스트 해야합니다. 
  - ex : 네이버에서 웹툰의 UI 를 개편했다고 합시다. 이론적으로는 웹툰 개편은 네이버 메인에 노출되지 않으므로, 네이버 메인에 진입되는 유저에 대해서는 아무 영향이 없어야 할 것이지만, 이러한 개편이 코드상의 어떤 영향으로 메인의 Latency 를 변화시킬수도 있으므로 꼭 확인해 봐야 할 것입니다.
- 일반적으로 AB Test 의 주제로는 다음과 같은 예시가 있습니다.
  - Front - end 의 유저 인터페이스 개선 
  - Recommendation System 에 대한 백앤드 개선
  - 온라인 광고....
- 이러힌 Controlled Experiments 의 핵심 목표는 Treatment 와 측정가능한 유저의 행동 사이의 인과관계를 밝혀내는 것입니다.
- 이 인과 관계는 교란 요인이 무작위화에 의해 잘 제어되고 테스트와 대조군 그룹 사이에서 균형을 이룬다는 전제하에 성립하게 됩니다.
- 이러한 사용자 행동의 변화 외에도 온라인 환경에서 시험과 통제 사이의 관찰된 차이점에 기여할 수 있는 많은 교란 요인이 있다. 관찰된 효과가 교란 요인 중 하나가 아닌 사용자의 행동 차이로 인한 것임을 보장하기 위해 데이터 품질 보증 절차를 따라야 한다.

> A/A Testing

- Null 테스트라고도 알려진 AA 테스트는 Online Controlled Experiment 에서 동일한. identical variants 인 A 와 A 끼리 비교하여서  AB 시험이 시작되기 전에 Control과 Variants군 사이에 존재하는 차이를 없애기 위해서 실행하게 됩니다.
  - ex : AA 실험을 해 보았는데, 예상 외로 차이가 크게 나게 되면, 버킷에 대해서 Random 할당이 잘못되었다는것을 나타낼 것입니다.
- 이떄 많은 기업들이 AA 테스트를 자주 수행하거나, 또는 적극적으로 추천하고 있습니다. 
- AA 테스트 절차를 실험하게 된다면 아래와 같은 절차를 따르게 됩니다. 
  - AA 테스트에서 테스트 그룹과 대조군 그룹 사이에 유의미한 차이가 나타나지 않는 경우 Treatment 를 적용할 수 있습니다.  
- 또는 A/A 테스트의 대안으로서, retroactive AA Test 를 수행하여서 
- 대안으로, 소급 AA 시험을 수행하여 테스트와 제어 사이의 차이를 확립하거나 트래픽이 재랜덤화(재순열)된 후에 모든 실험을 시작할 수 있다. 

> A/A Test 가 만능일까? 

- AA 검사를 통하여 우리는 실험시작 전에 Control Population 과 Treatment Population 이 적절하게 balanced 되어 있는지를 알아볼 수 있습니다.
  - ex : AA 테스트를 해 보았더니 mac 유저와 window 유저가 균일하게 배분되어있지 않았다고 합시다. 그러면 현재 우리가 진행한 Randomization 이 올바르지 않았다는것을 의미합니다. 이럴떄는 Randomization 을 다시 수행할 수 있습니다.
  - ex : AA 테스트를 해 보았더니, 인당 구매금액 차이가 크게 난다고 합시다. 그러면 Randomization 이 잘못되어 한쪽 버킷에 큰손이 몰려있음을 의미합니다. 이럴떄에도 Randomization 을 다시 수행하는게 좋습니다. 
- 하지만 위처럼 A/A 테스트는 도움을 주긴 하지만 일부 불균형이 AB 테스트 기간동안에 발생하는 경우, 즉 불균형을 초래할수도 있는 Confounding Factor 를 피하는데에는 별 도움을 주지 못합니다.
- 따라서 AA 테스트를 수행하는 기간을 넘어서, 실험에 사용하는 Experiment data 에 대해서 데이터의 품질을 점검하고, Control 과 Variant 간 균형에 대한 상세한 검정이 필요합니다. 
- 이 논문의 매우 특징중 하나는 "AB 기간 동안 검증 테스트를 추가하기 위해 구현한 절차"에 대한 설명입니다. 이런 특별한 주제는 이전 논문에서 자세히 논의되지 않았습니다.

> root cause of experiment imbalance 

- 이러한 imbalanced 를 일으키는 근본 원인을 찾기 위해서는 experimental platform 의 근본 작동 방식을 살펴보고, engineering code base 레벨 수준에서 살펴보는 작업이 필요합니다.
  - 증상을 세심히 살펴보고, 어떤 부분이 imbalane 의 원인이 되는지 살펴보고 고치는 작업은 매우 중요합니다. 

- 이전 논문은 온라인 실험의 설계 및 분석에서 얻은 중요한 교훈을 문서화하였다 [27], [29], [31]. 관련 분야에서는 고객 정서 조사를 위한 실질적인 어려움과 해결책도 논의되었다[40]. 이 논문의 한 가지 특징은 실험 품질 검증 테스트, 증상, 근본 원인 및 치료법을 연결하는 실제 데이터의 예를 제공하는 것이다. 이러한 교훈을 통해 실무자는 검증 테스트를 활용하여 실험 품질을 평가할 뿐만 아니라 동인이 무엇이고 실제로 문제를 해결하는 방법을 이해할 수 있다.
- 우리는 야후의 온라인 실험에 대해서 몇가지 challenge 에 직면했습니다. 
  - Yahoo’s infrastructure 는 원래 Experiment 를 염두하고 설계된것이 아닙니다. 그래서 실험은 대부분 기존의 ecosystem 을 어느정도 개조하여서 실험한 것입니다.
  - 야후는 다양한 사이트들을 보유하고 있으며, 다양한 device type 에 대해서 다른 version 의 view 를 보여주고 있습니다.
  - user experience 를 개선하기 위해서 급진적인 구조 개선이 자주 이루어집니다.

- 이러한 분산된 구조때문에 setting up experimental infrastructure 의 책임은 다양한 부서로 분산됩니다. 
- 물론 중앙 집중식 실험플랫폼을 이용하여서 트래픽 분할(샘플링) 알고리즘과 실험 관리 및 보고 시스템을 공유합니다.
  - 하지만 product team 별로 experimentation code 나, meta information 등은 다르게 가져갈 수 밖에 없기 때문에, 중앙 관리가 다소 힘든점이 있습니다.

- 즉 일일히 product team 마다 찾아가서 문제점을 진단해줄 수 없기떄문에 일반적인 diagnotic solution 의 필요성이 대두되었습니다.

> ## Paper 에서 다루는것 개요

> Paper 에서 다루는것

- 본 논문은 AB 기간 동안의 불균형 검출 및 이에 대한 remedy 에 대한 우리의 접근방식을 제시합니다.
- 섹션 II
  - AB 기간 동안 실험이 균형을 이루는지 여부를 검증하기 위해 통계 테스트를 도입합니다. 
  - AB 기간 동안 편향되지 않은 선택과 샘플 균형의 검증은 어렵지만 이전 문헌에서는 거의 논의되지 않았었습니다. 
  - 도입된 테스트는 이론적으로는 간단하지만 매우 실용적입니다. 
  - 이 테스트는  이후 섹션에서 다양한 문제로 인한 불균형을 감지하고 진단하기 위해서 사용됩니다.
- 섹션 III 
  - 불균형 원인과 해결책은 대게 아래와 같은 세가지 범주로 논의됩니다
    - 트래픽 분할 및 테스트 ID 스탬프
    - instrumentation 및 logging 
    - 이상치 제거 
  - 이러한 주제 중 일부는 개념적 차원에서 여러 논문에서 논의되었다
  - 본 논문은 무엇이 일어날 수 있는지, 무엇을 관찰할 것인지, 어떻게 감지하고 진단할 수 있는지, 그리고 치료법이 무엇인지 보여주는 구체적인 예를 제시함으로써 논의를 진전시켰습니다. 
- 섹션 VI
  - 실험 진단 및 trouble shooting 에대한 추가 주제에 대해 논의한다.

> Traffic Split 에 대한 문제

- Traffic Split 을 진행할때에, 적절하지 않은 해싱 함수를 사용해서 발생하는 문제는 이전 [30][R. Kohavi, R. M. Henne, and D. Sommerfield. Practical guide to controlled experiments on the web: listen to your customers not to the hippo. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 959–967. ACM, 2007] (아래 reference)에 설명되었지만 트래픽 분할 구현에 관한 문제는 언급되지 않았었습니다.
- 이 주제에 대한 많은 Reference 들에도 불구하고,  실험이 수년 동안 존재해 온 야후에서도 트래픽 분할과 관련된 여러 가지 문제들이 몇 년 전까지만 해도 관찰되어 왔습니다. 
- 우리는 이러한 상황이 특별하지 않으며, 많은 기업들이 이러한 문제가 존재한다는 것을 인지하지 못한 채 이러한 문제를 계속 겪고 있다고 생각하고 있습니다.

> logging 과 instrumentation 

- 계측 및 로깅과 관련된 문제는 코하비[30]와 탕[44]을 통과하는 동안에만 언급되었으며, 우리가 마주친 특정 문제는 일반적이지만 이전에는 문헌에 기술되지 않았다.
- 마지막으로, 데이터 분석에서 특이치의 잠재적 위험은 잘 알려져 있지만, 이상치가 AB 테스트 결과에 미치는 영향은 이전에는 자세히 논의되지 않았다.

> ## 생략

- 생략생략생략

> ## outlier

- 온라인 실험 분석의 또 다른 과제는 데이터 특이치이다[17].
- 특이치는 실제 사용자 행동을 합리적으로 반영하지 않는 극단값을 갖는 데이터 점이며 실험 결과에 영향을 미칠 수 있다. 온라인 실험에서 이상값의 주요 원천은 로봇[29]으로, 짧은 시간 동안 엄청난 수의 기록된 이벤트를 생성할 수 있다. 
- 예를 들어, 한 실험에서 우리는 크롬 브라우저 하나가 78분 동안 10만 번의 클릭을 생성하는 것을 보았다. 
- 데이터가 분석 데이터 파이프라인으로 흐르기 전에 대부분의 로봇 이벤트를 제거하도록 세심하게 설계된 트래픽 보호 기능이 있지만, 로봇 생성자와 운영자는 탐지를 피하기 위한 새로운 방법을 끊임없이 찾고 있다. 
- 특이치는 신뢰할 수 없는 데이터 출처에서 발생할 수도 있습니다. 예를 들어, 소요된 시간 메트릭은 사용자 이벤트와 관련된 타임스탬프에서 파생됩니다. 
- 컴퓨터나 전화기의 내부 시계를 기준으로 클라이언트측 타임스탬프를 사용할 경우 클럭 동기화가 되지 않아 시계가 과거 또는 미래 시간으로 설정돼 데이터 측정 시점이 잘못될 수 있다. 이는 장치의 라디오가 꺼져 있거나 신호가 없는 모바일 앱의 경우 특히 어려우며, 이는 외부 클럭과 동기화할 방법이 없음을 의미한다.

![image-20220113032131544](img/2022-01-13-outlier/image-20220113032131544.png)

- 특이치는 실험 분석에 부정확한 측정과 메트릭 분산의 팽창이라는 두 가지 영향을 미치며, 그 중 후자는 치료 효과를 탐지할 수 있는 통계적 검정력을 감소시킨다. 다음의 예에서 한 가지 실험이 야후 디지털 콘텐츠 사이트 중 하나에서 수행되었다. 제품팀은 페이지 디자인에 약간의 변화를 주었고 사이트 기술 스택도 개선했습니다. 이 특정 실험의 핵심 메트릭 중 하나는 이 페이지의 링크 A를 클릭하는 것이었습니다. 그러나 데이터는 그림 5a와 같이 대조군에 비해 테스트 그룹의 클릭 수가 예상외로 크게 증가했음을 보여주었다. 사용자당 주당 클릭 수는 거의 두 배로 증가했습니다. A 링크에 더 많은 클릭을 유도할 수 있도록 설계된 기능 변경이 없었기 때문에 데이터 문제가 있는 것으로 의심됩니다. 테스트 그룹에서는 여러 사용자가 매일 링크 A를 수천 번 클릭하는 반면 대조 그룹에서는 이렇게 큰 값이 관찰되지 않았다. 링크 A를 클릭하는 사용자 비율이 상대적으로 적다는 점을 고려할 때, 수천 번의 클릭을 하는 사용자 수는 전체 그룹 평균을 상당히 높일 것이다.
- 특이치를 탐지하고 제거하기 위해 다양한 통계 및 기계 학습 방법이 개발되었다 [49], [7], [1], [34]. 예를 들어, 관심 있는 사용자 참여 메트릭이 주어지면 관찰된 메트릭 값의 발생을 모델링하기 위해 적절한 통계적 분포를 제안할 수 있다. 특이치는 분포 가정 하에서 확률이 매우 낮은 영역에 속하는 경우 탐지될 수 있습니다. 정규분포를 따르는 지표에 대해서는 3가지 규칙을 적용할 수 있습니다. 클릭과 같이 음수가 아닌 정수 값만 취하는 메트릭의 경우 정규 분포는 기본 참 분포를 모델링하지 못하므로 보다 적절한 분포를 제안해야 한다. 그러나 이상치 감지 방법에 대한 자세한 설명은 이 논문의 초점이 아니다. 이 경우, 우리는 내부 특이치 탐지 방법 4를 적용했다. 이 특이치 모델은 이 실험에서 총 샘플 사용자의 약 0.01%를 걸러냈다. 
- 추가 이상값 필터가 구현되면서 우리는 그림 5b와 같이 제어와 비교하여 테스트에서 클릭이 감소했다는 다른 사실을 알게 되었습니다. 링크 A와 경쟁해야 하는 테스트 경험에 추가 모듈이 추가되었기 때문에 이러한 감소는 합리적입니다. 이 이야기는 이상치에 더 영향을 받지 않는 것으로 간주되는 또 다른 메트릭, 즉 링크 A를 클릭한 사용자의 비율을 사용하여 확인되었다. 특이치를 걸러내기 전에 링크 A를 클릭한 사용자 비율은 감소(제어 대비 17.65%)했지만 사용자당 클릭 수는 반직관적으로 증가했다. 이상치를 제외한 후 클릭 사용자 비율의 감소는 -17.65%(이상치 비율은 무시할 수 있음)로 유지되었고, 감소는 사용자당 클릭 수 감소와 일치했다.

---

**reference**

- https://alexdeng.github.io/public/files/jsm2011-deng.pdf

