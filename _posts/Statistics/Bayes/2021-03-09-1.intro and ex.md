---
title:  "Inrto ans Example"
excerpt: "왜 베이즈?"
categories:
  - Stat
tags:
  - 1
last_modified_at: 2021-03-09

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

# <center><font size="20"> Intro </font></center>

## intro

우리는 종종 어떠한 양(quantity) 에 대한 우리의 믿음(belief)을 엉성하게 나마 확률로 표현해보려 애쓴다. 하지만 이런 information 을 확률로 표현하려는것을 체계화 할 수 있다. 베이즈 통계는 믿음(belief)을 새로운 정보(데이터)와 통합해 업데이트할 수 있게 해준다. 이러한 업데이트는 베이즈 룰을 통해서 가능하고, Baysian inference 라고 한다. 

베이즈 방법론은 다음과 같은 methods 를 제공한다.

- 좋은 통계학적 성질을 이용한 parameter 추정
- oberseved data 에 대한 간단한 설명
- Missing data 에 대한 예측(na imputation)
- model selection / validation 을 위한 computational framework



## baysian learning

Statistical induction 은 oberseved data(즉 population 의 subset) 에서 population 의 특성을 추론하는 것이다. population 의 특성은 일반적으로 parameter $\theta$ 로 나타나진다. 그리고 data 의 descriptions 는 data set $y$ 로 나타나진다. 

data set 이 얻어지기 전까지는 이러한 특성들을 알기 불가능하다. 데이터셋을 얻고난 이후 우리의 uncertainty about population 은 줄어들고, 얼마나 줄어드는지를 측정하는지가 Bayesian inference 의 목적이다. 

우선 논리를 이끌어내기 전에 몇개의 기호들을 정의하자. 

$\mathcal{Y}$  를 sample space 라 하자. 그러면 $\mathcal{Y}$ 는 set of all possible datasets 이 된다. 이  $\mathcal{Y}$ 로 부터 하나의 $\mathcal{y}$ 가 뽑히게 된다.

$\Theta$ 는 parameter space 라 하자. 그러면 $\Theta$ 는 set of all passible parameter value 가 된다. 우리는 $\Theta$  에서 true population을 제일 잘 나타내는 추정치를 얻어내고 싶다.

이제 베이지안 learning 이 어떻게 일어나는지 살펴보자. 베이지안 learning 은 ($\mathcal{Y}$, $\Theta$) 위에서 정의되는 joint belif $p(y\mid\theta)$ 에서부터 시작된다.

1. 각 $\theta \in \Theta$ 에 대해서 $p(\theta)$ 는 prior distribution 이라 하며 population 에 대한 우리의 믿음을 나타낸다. 

2. 각 $\theta \in \Theta$ 와 $y\in \mathcal{Y} $ 에 대해서 model $p(y\mid\theta)$ 는 $\theta$ 가 True 일때 y 에 대한 우리의 믿음을 나타낸다.
3. 만일 데이터 $y$ 를 얻는다면 우리의 마지막 step 은 $\theta$ 에 대한 우리의 믿음을 업데이트 하는것이다. 각각의 수적인 값 $\theta \in \Theta$에 대해, 우리의 사전 분포 $p(\theta)$는 $\theta$가 실제 모집단의 특성을 표현한다는 우리들의 믿음을 나타낸다.
   - 업데이트는 베이즈 룰에 따라 $p(\theta | y) = \frac{p(y|\theta)p(\theta)}{\int_{\theta}p(y|\tilde{\theta})p(\tilde{\theta})}$ 으로 주어진다.



<BR>

# <center><font size="20"> Why Bayes?</font></center>

Cox(1946,1961) 와 Savage(1954,1973) 는 사람의 믿음을 $p(\theta)$ 와 $p(y\mid\theta)$ 으로 나타나게 된다면 베이즈 rule 이 사람의 belief about $\theta$ 와 $y$ 를 나타내기에 최고의 방법이라는것을 수학적으로 증명하였다. 이 결과는 베이즈 룰을 통한 업데이트가 정당하다는 큰 기반이 될 수 있었다. 

하지만 $p(\theta)$ 은 우리의 '믿음' 인데 이를 정확하게 수학적으로 묘사하기에는 무리가 있다. 그래서 편의상 conjugate prior 를 써서 computational cost 를 낮추려고 하였다. 이렇게 되면 베이즈 통계에 대한 정당성은 어디에서 확보할 수 있을까?

사실 모든 모델은 틀린다. True 를 정확하게 캐치할 수 없기 떄문이다.  $p(\theta)$ 는 우리의 믿음과 매우 달라서 '틀릴' 수 있다. 하지만 그렇다고해서 $p(\theta\mid y)$ 가 쓸모없는게 아니다.   

<br>

# <center><font size="20"> what to do?</font></center>

내용