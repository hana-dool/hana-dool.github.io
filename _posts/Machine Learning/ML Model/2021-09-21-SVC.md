---
title:  "Linear regression"
excerpt: "선형성을 예측하는 제일 기본적인 모델"
categories:
  - ML_Model
last_modified_at: 2021-10-05

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

 SVM 은 서포트벡터 머신입니다. 이것에 대해서 자세히 알아봅시다.
{: .notice--warning}

# [Idea of SVM](#link){: .btn .btn--primary}{: .align-center}

![image-20211006004254972](C:\Users\goran\Desktop\Documents\Typora_image\image-20211006004254972.png)

# [kernel Trick](#link){: .btn .btn--primary}{: .align-center}

> ## 커널트릭 아이디어

![image-20211006010710398](C:\Users\goran\Desktop\Documents\Typora_image\image-20211006010710398.png)

> ## 계산량 절감

![image-20211006010748616](C:\Users\goran\Desktop\Documents\Typora_image\image-20211006010748616.png)

> ## 커널의 확장

![image-20211006010814769](C:\Users\goran\Desktop\Documents\Typora_image\image-20211006010814769.png)

> ## 많이 사용되는 커널

![image-20211006010831307](C:\Users\goran\Desktop\Documents\Typora_image\image-20211006010831307.png)

> ## 다항 커널

![image-20211006010844397](C:\Users\goran\Desktop\Documents\Typora_image\image-20211006010844397.png)



# [SVM Application](#link){: .btn .btn--primary}{: .align-center}

> ## Hyperparameter

![image-20211006005830125](C:\Users\goran\Desktop\Documents\Typora_image\image-20211006005830125.png)

> C 

- C 가 어떤것인지 기억하시나요? C 는 바로, 에러 마진에 대한 가중치입니다.

![image-20211006005147167](C:\Users\goran\Desktop\Documents\Typora_image\image-20211006005147167.png)

- 에러의 Minimize 는 아래임을 기억합시다/

![image-20211006005206266](C:\Users\goran\Desktop\Documents\Typora_image\image-20211006005206266.png)

- C 가 크면 '너 에러 조금만 있기만 해봐!' 라는 의미이고, 이는 오버피팅을 의미하게 됩니다. 
- C 가 작으면 '너 에러.. 있어도 되!' 라는 의미이거 , 이는 언더피팅을 의미하게 됩니다. 

> gamma 

- 감마는 뭘까요...?

![image-20211006005536338](C:\Users\goran\Desktop\Documents\Typora_image\image-20211006005536338.png)

- 일반적으로 위와 같이 SVM 의 파라미터를 설정할 수 있습니다.
- 제일 좋다고 알려진 가우시안 커널을 예시로 들어보겠습니다.
  - 감마가 크면 커널값이 작아지고, 각 샘플이 y 에 예측에 영향을 주는 범위가 줄어들음...

![image-20211006010230359](C:\Users\goran\Desktop\Documents\Typora_image\image-20211006010230359.png)

- gamma 가 크면 : '응 커널값 작아져~' 그러면 너희 샘플 영향 줄어~ Underfit
- gamma 가 작으면 : 커널값 커져~ 샘플에 지배당해 볼래? (Overfit)



# [SVM With Text ](#link){: .btn .btn--primary}{: .align-center}

- https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf

> ## Sparse Data 에 좋음

- Data Sparseness in SVM
  - https://www.ijcai.org/Proceedings/15/Papers/510.pdf

![image-20211006013915465](C:\Users\goran\Desktop\Documents\Typora_image\image-20211006013915465.png)

- 위와 같이 Support vector 가 경계 형성에 매우 중요한 역할을 하는데, 
  - 그 밖의 데이터 포인트들은 매우 미미한 영향을 끼칩니다. 
- 이는 차원에서도 마찬가지로, '개별 범주간의 지지벡터를 어느정도 식별하고 나면, 중요하지 않은 차원은 무시될 수 있다는 것입니다.
  - 그러므로 Sparse Data 에서도 잘 작동할 수 있습니다.
- https://datascience.stackexchange.com/questions/22137/svm-on-sparse-data/22151
- https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf

> ## High dimensional input space

- 위와 같이 소수의 Vector 에 의해 결정경계가 형상화 한다는 이점은 High Dimensionality 에도 같이 적용됩니다.
  - svm 은 다시말하지만 '두 범주를 구분하는 Support vector' 가 중요하지 Dimensionaltity 에는 Robust 합니다.

> ## HIgh dimension 에서 Linearly Separable

![image-20211006020615419](C:\Users\goran\Desktop\Documents\Typora_image\image-20211006020615419.png)

![image-20211006020625764](C:\Users\goran\Desktop\Documents\Typora_image\image-20211006020625764.png)

- 위처럼 차원을 높힐수록 Linear 하게 쪼개질 확률이 높아집니다. 그러므로 엄~청나게 High dimension 한 세상에서는 Linearly 하게 쪼개질 확률이 높아지겠죠. 
- Most text categorization problems are linearly separable 이라는 의미는, 단어벡터를 형성해 엄청나게 높은 차원으로 Text 들을 끌어 올리면, '2진 분류' 를 하는 SVC 특성상 Linear 한 CLassification 이 쉬울것이고, 이는 Linear 결정경계를 형성하는 SVM 이 적당하다는 의미가 됩니다.
- https://stats.stackexchange.com/questions/155596/how-to-prove-that-text-is-linearly-separable

> ## Good Model Properties

- 다른 복잡한 모델보다 파라미터가 많지를 않아요! (C,r,kernel)
  - R.F 만 하더라도 너~무 많습니다.
- 또한 Kernel 에 따라 매우 다양한 FItting 이 가능해서 '하나의 Model 이라고 보기는 힘들정도의' 보편적 모델링입니다. 
- 그러므로 SVM 만으로도 어느정도 합리적이고 좋은 모델링이 가능합니다.
- https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf







---

**Reference**

- <https://christophm.github.io/interpretable-ml-book/limo.html>
- <https://tootouch.github.io>

 Linear regression 에 대한 이야기를 하자면 사실 , regression 만을 위한 책이 (1), (2) 까지 구성될 정도로 광할하고 어려운 분야이다. 그래서 여기에서는 그냥 OLS 만 다루었고, 그에대한 해석도 되게 짧게만 소개했다. 다음에 기회가 된다면, 좀 더 다양한 Model 을 소개하는것으로 하고 여기까지만 하는것으로!
{: .notice--success}

