---
title: "Why Should I Trust You? &#58; Explaining the Predictions of Any Classifier"
excerpt: "Lime 방법론의 첫 제안이 되는 논문"
categories:
  - ML_Paper
last_modified_at: 2021-09-21

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

 Lime 방법론에 대해서 알아봅시다. 기본 Idea 는 매우 간단한 편이고, 사실 많은 분야에서 응용되기는 하지만 공격받을 부분이 많은 모델이라고 생각해요. 하지만 Lime 이 워낙 큰 파장을 일으킨 논문이고 다양한 변형을 낳았으며, 다른 모델에도 많은 영감을 준 모델이기 떄문에 , XAI 하시는분이라면 필수로 알아야한다고 생각합니다.
{: .notice--warning}

# [Introduction : Trusting](#link){: .btn .btn--primary}{: .align-center}

- 이 논문에서는 결과를 신뢰하는것을 두가지로 나누어 정의합니다.

> ## Trusting model

> Trusting Prediction

- 개별 예측을 믿고 이를 바탕으로 행동을 취하는 것을 말한다.
- 개별 예측에 대한 신뢰는 예측값에 기반한 의사결정 문제에서 매우 중요한 요소이다. 

ex) 예컨대 의료 진단을 하거나 테러범을 탐지하는 문제에서 예측 과정에 대한 이해 없이 그저 모델이 예측한 값을 토대로 판단하는 것은 매우 위험할 것.  진단을 내리는데 결정적인 역할을 한 증상이 무엇인지, 어떤 특징이 테러리스트라고 판단하는데 영향을 주었는지를 알 수있다면 사용자가 결과를 신뢰할지 신뢰하지 않을지 판단하는데 도움이 될 수 있다.
{: .notice}

> Trusting a model

- 모형 전반에 대한 신뢰를 말한다.
- 즉 이 모델이 전체적으로 믿을만 한지에 대한 것이다.
- 모델이 우리가 가진 데이터 외에도, 실제 데이터를 적용하였을 때애도 잘 작동하는지 알고싶다.
- 머신러닝 모델을 만들때 이를 위해서 일반적으로 validation 또는 test set에 대한 Accuracy Metric을 이용해서 accuracy를 측정한 뒤, 이 값을 토대로 모델의 성능을 추정한다.
- 이 방법은 유용하긴 하지만 실제 시스템에 적용하여 실데이터의 예측값을 산출했을 때 보다 정확도를 과대 측정하는 경우가 많다. (에러가 많아)

> 잘못된 모델 평가를 일으키는 대표적인 원인 

- Data leakage : 의도하지 않은 잘못된 시그널이 훈련/검증 데이터 셋에 포함되는 경우. 예를 들어 환자의 id와 발병여부는 매우 높은 상관관계를 가지지만 의미가 없는 잘못된 데이터임. 이것이 실수로 데이터셋에 포함된다면 모델은 의미 없는 패턴을 학습하게 됨.
- Data shift : 학습 데이터 셋과 테스트 데이터셋의 특성이 다른 경우

> ## How to Trusting model?

- 위의 2가지 “신뢰” 문제를 해결하기 위해서 논문은 2가지 방법론을 제안한다.
- 그것은 바로 Lime 과 SP lime 이다.

> Lime (locally interpretable model-agnostic explanations)

- 개요
  - 모델의 개별 예측값을 설명하기 위한 알고리즘
  - 국지적(local)인 지역에 대해 모델을 설명하는 기법 

- 접근
  - 복잡한 모형을 해석이 가능한 심플한 모형으로 locally approximation을 수행하여 설명을 시도.

- 특징
  - 전체 모델이 아닌 개별 prediction의 근방에서만 해석해준다.
  - 어떠한 모델이든 적용이 가능하다는 특징이 있다.
  - 일반적으로 해석가능한 모델을 해석할 때에는 모델의 구조 및 가중치들을 살펴보면서 신중하게 모델을 해석하지만, LIME 의 경우는 Black box model 에 input 데이터를 넣고 그 결과들을 가지고 해석
  - 좀더 자세히 말하자면 input 데이터를 lime 알고리즘에 맞게 변형한 데이터를 black box model 에 여러차례 넣어봄으로서 리턴되는 결과를 가지고 해석하는 방법

> SP-LIME [SUBMODULAR Pick]

- 개요
  - 모델 자체의 신뢰 문제를 풀기위해 대표적인 인스턴스를 선택하는 알고리즘

- 접근
  - 모델의 신뢰 문제를 풀기 위해서 논문은 개별 Prediction에 대한 해석을 살펴보는 접근법을 취한다.
  - 그런데, 전체 Prediction을 모두 살펴보는 것은 대규모 데이터셋에서 어려운 일이다.
  - 따라서 중복되는 정보들을 담고있는 인스턴스들은 제외하고 중요 정보를 담고 있는 소수의 인스턴스를 추려내는 과정이 필요한데, 이를 수행하는 알고리즘이 SP-LIME이다.

# [local Fidelity, interpretability](#link){: .btn .btn--primary}{: .align-center}

> ## Local Fidelity

- ※ 모델의 충실도(Fidelity) **:** **모델이 실재와 얼마나 닮았는가** 
- 아래의 전제는 논문의 핵심 전제이므로, 이는 꼭 알아두도록 하자!

![jpg](/assets/images/ML/14_1.png)

『데이터 전체공간에서 모형을 설명하는 것은 어렵지만, 설명하고자 하는 데이터와 가까이 있는 국소적인 공간에서는 의미있는 모형으로 설명할 수 있다.』
{. :notice}

- 위 예제에서 보면 굵은 빨간 십자가가 설명하고자 하는 데이터라고 하자.
- 검은 선은 그에 따른 local explanation이다(여기서는 선형회귀로 설명하고 있다.)
- 전체적으로 볼 때에는 빨간 영역과 파란 영역 구분은 잘 못하지만,
- 굵은 십자가 근방에서는 선형모델이 데이터의 추이를 매우 잘 설명하고있다는 것을 볼 수 있다.

> ## Interpretability

※ 모델의 해석력(Interpretability) : 모델이 결과를 예측하는 것을 사람이 얼마나 이해 가능한지

- 국소공간의 설명만은 우리가 원하는 전체공간의 해석을 할 수 없다.
- local 적으로 중요한 변수도 다른공간에서는 중요하지 않을 수 있기 때문이다.
- 전체 공간의 explanation 은 local 적인 공간을 어느정도 설명할 수 있겠지만, 모델이 복잡해지면 이를 해석 (interpretability) 하기가 어렵다.
- 모델이 복잡해질수록 해석력은 낮아진다.

> ## Our Task

- 이 둘을 모두 반영할 수 있는 Explanation g 를 찾는게 바로 궁극적인 목표가 됩니다.

# [Noations](#link){: .btn .btn--primary}{: .align-center}

> ## Data 

- interpretable explanation 은 사람이 이해하는 표현법을 말한다.
- 데이터의 원 형태를 고려하지 않고, 사람이 해석할 수 있도록 데이터의 모양을 바꾼다.
  - ex) text classification에서 binary vector(단어가 있는지 없는지를 나타내는 벡터) 
  - ex) categorical data는 원핫벡터를 생각할 수 있다.
  - ex) image 데이터를 3 chennel 로 나타낸 벡터
- original representation, 즉 원 데이터를  $X\in R^d$   라고 하자.
- 이때 이 데이터를 해석 가능하게 변환한 데이터 $x' \in \{0,1\}^{d'}$는 binary vector for interpretable representation 라고 생각하자.
  - 즉 해석가능한 표현을 {0,1}들의 집합으로 생각하자는 것이다

> ## Function

- g : explanation as a model 즉 우리의 local 데이터를 설명할 때에 사용할 모델 
  - g는 g acts over absence/presence of the interpretable components.
  - 모든 g 는 해석하기에 알맞게 쉬운 모델이여야 한다.

- G : 해석가능한 모델들의 모음 ( linear model , decision tree .... )
- Domain(g) : $\{0,1\}^{d'}$ 즉 원핫벡터들의 모음
- $\Omega(g)$ : measure of complexity ( interpretability 와는 반대된다. ) 
  - 모델이 복잡해질수록 해석력이 낮아지므로 이 값을 낮추고싶다.
  - tree model 에서는 트리의 깊이, linear model 에서는 nonzero weight 계수의 수 등이 될 것
- f : explained 가 될 모델. 이때 $f : R^d \to R$ 이라고 하자. 
  - 일반적으로 매우 복잡한 xgboost , deeplearning 모델등이 가능할 것이다. 
  - classification에서 f(x)는 probability (또는 binary indicator) 가 될 것이다.
- $\pi_x(z)$ : proximity measure between instance z to x 
  -  x,z 의 거리를 재는 측도이다. 어떻게 정할지는 나중에 더 알아보자.
- $\mathcal{L}(f,g,\pi_x)$ : : measure of how unfaithful g is in approximating f in the locality defined by $\pi_x$
  - local fidelity를 measure 한다. 이값이 작을수록 local 해석이 좋은 것.
  - 해석가능한 모델 g 가 얼마나 원래 모델 f 와 가깝게 되는지에 대한 측도. 이를 줄이고싶다.
- we must minimize $\mathcal{L}(f,g,\pi_x)$ while having $\Omega(g)$ be low enough to be interpretable by humans. 
  - 이를 수식으로 고치게 된다면 $\xi(x) = argmin_{g\in G} \mathcal{L}(f,g,\pi_x)+\Omega(g)$
  - 우리는 interpretability 와 local fidelity 둘의 균형을 잘 조정하면서 최적화 해야한다는 의미이다.

# [Sampling for local exploration](#link){: .btn .btn--primary}{: .align-center}

> ## How to sampling?

- 우리는 minimaze $\mathcal{L}(f,g,\pi_x)$ 를 할때에, model-agnostic 해석을 원하므로 , f 에 대한 어떠한 가정 없이 출발할 것입니다.
- f 에 대한 가정없이 local 에 대한 f 의 행동을 관찰하기 위해  $\mathcal{L}(f,g,\pi_x)$ 의 근사값을 by drawing sample weighed by $\pi_x$ 를 통해 (즉 가중치가 곱해진 샘플들을 통해) f 의 local 적인 행동을 분석석하려 합니다.
- 우선 $x'\in\{0,1\}^{d'}$ 의 주변에서 sample 을 뽑는게 local 해석의 첫 단계라고 할 수 있습니다.
- 이떄에 주변에서 뽑은 Sample 을 z라 합시다. 이러한 z sample 을 뽑는 방법은 여러가지가 있습니다.

> ## Categorical sampling 

*『The R LIME package (Pedersen (2019)) sample with probabilities of the frequency of each category appearing in the original dataset.』*<br>
[Limitations of Interpretable Machine Learning Methods 14.1.2.1]
{: .notice}

- 즉 우리가 가지고 있는 데이터셋의 분포대로 sampling 을 한다는 의미입니다.
- 원래 데이터셋에 korean 속성을 가진 데이터가 많다면 sampling 할떄에도 korean 속성의 데이터가 많이 나오게 됩니다. 

> ## Numerical features sampling

*『R LIME package (Pedersen (2019)) for sampling numerical features. The first* *–* *and default* *–* *one uses a fixed amount of bins. The limits of these bins are picked by the quantiles of the original dataset. In the sampling step, one of these bins will be randomly picked and after that, a value is uniformly sampled between the lower and upper limit of that bin』*<br>
[Limitations of Interpretable Machine Learning Methods 14.1.2.1] 
{: .notice}

- 즉 이 경우에는 데이터의 분포를 보고 quantile 에 맞게 bin 을 나눈뒤에 bin 을 랜덤하게 고르게 됩니다.
- 그 이후에 bin 의 upper / lower 값의 범위 내에서 uniform 하게 하나를 고르게 됩니다. 

*『Another option would be to approximate the original feature through a normal distribution and then sample out of that one.』*<br>[Limitations of Interpretable Machine Learning Methods 14.1.2.1] 
{: .notice}

- 위와 같이, N(0,1) 에서 샘플을 뽑은뒤 원래 데이터의 평균, 표준편차에 맞게 역변환 하는 방법도 존재합니다.
- 즉 원래 데이터가 normal 과 비슷할것이라는 약간의 가정이 들어간것입니다. 

> ## Perturbed samples

- 위에서 만들어진 sample 을 perturbed sample $z'\in\{0,1\}^{d'}$ 라고 합니다.
- 우리는 이 $z'$ 를 모형의 input 형태 $z\in R^d$ 로 바꾸어줍니다.
- 그리고 모형 f 에 z 를 input 으로 넣어서 f(z) 을 얻습니다.
- 이때 f(z) 는 label for the explanation model 로 이용된다. $\xi(x) = argmin_{g\in G} \mathcal{L}(f,g,\pi_x)+\Omega(g)$ 를 최적화 하기 위한 첫 발걸음을 뗀 것입니다.

# [Sparse linear explanation](#link){: .btn .btn--primary}{: .align-center}

> ## Find Optimization equation

- 여기에서부터 우리는 G 를 class of linear model 이라고 하고 식을 전개해 보겠습니다. 즉 $g(z') = w_g \cdot z'$
- locally aware loss $\mathcal{L}(f,g,\pi_x) = \sum_{z,z'\in Z} \pi_x(z)(f(z)-g(z'))^2$ 으로 정의하겠습니다.
- g(해석가능) 모델이 f (복잡하지만 실제와 비슷함) 와 얼마나 가까운지 잰다는 의미에서 $(f(z) -g(z'))^2$ 의 수식은 알맞은 정의이다.
- $\pi_x(z) = exp(-D(x,z)^2/\sigma^2)$
- D 는 distnace function 으로서 Text data 일떄에는 cosine distance, image 일떄에는 L2 distance 등이 될 수 있을것이다. 
- $\pi_x(z)$ 를 위 같이 정의하면 가깝다면 높은 값을 가지게 될 것이고, 이는 가까운 데이터를 더 중요하게 고려하게 됩니다. 
  - 이러한 정의는 local 적인 해석을 원한다는 점에서 매우 알맞은 정의입니다. 
- 즉 $\mathcal{L}(f,h,\pi_x) = \sum_{z,z'\in Z} exp(-D(x,z)^2/\sigma^2)(f(z) - g(z'))^2$ 가 됩니다. 
- 이 논문에서 최적화 문제의 모형 복잡도는 $\Omega(g) = \infty 1_{\mid\mid\beta\mid\mid_0>k}$ = $\infty I(\mid\mid\beta\mid\mid_0 > K)$ 로 정의하였다. 
  - 즉, K 보다 커지게되면 , $\Omega$ 값이 폭등(inf) 하게 되고, 이러한 모델은 고려하지 않게됩니다. 
  - 그러므로 , 논문에서 제시한 방법은, 복잡도를 K 이하로 Bounded 시킨것이라 할 수 있습니다. 
- 이는 G 를 linear model 로 가정했기 떄문에, 0 이 아닌 계수들의 수로 모델의 복잡도를 정의합니다. 
  - 0이 아닌 회귀계수가 K 개보다 많아지면 이 떄에는 Loss 함수가 무한대가 되어버리므로 우리는 K 개 이하으 복잡도를 가진 모델만 고려하게 됩니다. 
- 총 정리하면 최적화 문제는 $\xi(g) = argmin_\beta [\sum_{i=1}^N exp(-D(x,z_i)^2/\sigma^2)(f(z_i)- g(z_i'))^2 + \infty 1_{\mid\mid \beta\mid\mid>k}]$ 가 됩니다. 

> ## Prove optimization equation

- 그런데, 이제 한가지 문제가 남았습니다.
- interpretable representation K 개를 선택하는것은 사용자가 정의하도록 하였습니다.
- 최적의 K 개를 찾는 방법으로 저자는 모델로서 LASSO 를 이용하였습니다. 

![jpg](/assets/images/ML/14_2.png)

- 계속 Z 를 업데이트 하는데, 이때에 ($z_i'$(x 데이터) , $f(z_i)$(y값) , $\pi_x(z_i)$ 가중치) 세개가 Z 에 추가되게 됩니다. 
- 여기에 선택한 g 모델(Lasso) 는 $z_i'$ 데이터를 x 변수로, $f(z_i)$ 들을 각각의 y 변수로 고려한 상태에서 K 개의 설명변수를 가지기 위하여 $\lambda$ 를 조절하게 됩니다.
  - 이러면 return 으로 K 개의 변수를 가지고 있는 LASSO 모델을 내보내게 됩니다
  - 선택된 K 개의 변수 : 다른 변수보다 중요하다고 판단된 K 개의 변수
  - 각 변수들의 계수 : LAsso 모델의 계수이므로 이는 importance 를 의미합니다. 

# [Submodular pick for explaining models](#link){: .btn .btn--primary}{: .align-center}

- SP-LIME : 모델 자체의 신뢰 문제를 풀기 위해 대표적인 인스턴스를 선택하는 알고리즘 입니다. 

> ## Pick problem

- 데이터 $X \in R^{n*d}$ 를 고려합시다.
- 모든 데이터를 살펴볼 수 없기 때문에, 특징이 다르고 중요한 데이터만을 뽑아서 생각한다.
- 살펴볼 데이터 budget B를 정한다. 이는 데이터를 총 몇 개 뽑아서, 내 데이터를 대표할 샘플로 만들지의 값이다
  - 이 값을 100개를 정한다면, 총 100개의 Sample 을 뽑게 됩니다.
- original representataion 으로부터, 해석가능한 표현   $X' \in R^{n*d'}$    를 만든다. 
- 데이터의 local 한 importance를 나타낼 matrix W 를 아래와 같이 정의합시다.
  - linear model을 explanation 으로 사용했다면 데이터 $x_i$ 에 대하여 explanation $g_i = \xi(x_i)$  를 얻을 수 있고, $W_{ij} = \mid w_{g_{ij}} \mid$  로 정의할 수 있습니다.
  - 이떄에, $g_{i}$ 는 데이터 $x_i$ 에 대한 Explainable model 입니다. (linear)
  - 그리고 $g_{ij}$ 는  linear model 의 j번째 feature 에 대한 계수입니다. 
- 추가적으로  $I_j$ (j column 의 global importance를 정의) $I_j = \sqrt{(\sum_i W_{ij})}$ 을 정의합니다.
  - 모든 instance i 에 대하서 , 설명 가능한 모델 $g_i$를 Construct 한 뒤에 , j 번째 feature 의 계수의 절댓값 모두 더한것입니다.
  - 당연히 '중요한 feature' 라면, 많은 샘플에 대해서, 왜 이런 예측을 하였는니? 에 대한 Local 적인 해석을 하더라도 계속 포함될 것입니다.
  - 예시로 월급을 예측한다고 할떄에 Feature 에 나이가 있다면,각 instance (철수, 영희,.....) 에 대한 월급 설명을 할때에 대부분 나이 변수가 크게 영향을 끼칠것이기 떄문입니다.
- 최대한 특징이 다르며 많은 정보를 포함하고 있는 데이터를 추출하기 위해서 $c(V,W,I) = \sum_{j=1}^{d'}1_{\exist i \in V : W_{ij}>0}I_j$라고 converge function 을 정의합니다.

![jpg](/assets/images/ML/14_3.png)

- 우선 위와 같이 예시를 들어보겠습니다. 
- 각 instance x_i 에 대하여, 설명모델은 가로줄과 같습니다. 
  - x1 은 $1\cdot f_1 + 1 \cdot f_2  = x_1$ 로 설명되고 있다고 보시면 됩니다. 
- $V=\{1,3\}$ 인 경우에 $c(V,W,I)$ = (1+4) + (4+2) 가 됩니다.
  - 그 이유는 $x_1$ 의 경우 $f_1, f_2$ 에 의해 설명이 되고, 각각의$I_1, I_2$ 는 1,4 이기 때문입니다.
  - $x_3$ 의 경우 $f_2,f_3$ 에 의해 설명이 되고, 각각의 $I_2, I_3$ 은 4,2 이기 떄문입니다.
- 이렇게 Coverage function 을 최대화 하는 집합 V 를 찾으면 중요한 데이터들을 쏙쏙 뽑아낼 수 있을것입니다.
  - 모두, 다양한 local 모델들에 대해서 중요하다고 생각되었던 Feature 들이기떄문에 중요한 데이터일것입니다.

$$Pick(\mathcal{W},I)=\underset{\mathcal{V},|\mathcal{V}|\leq B}{\operatorname{argmax}}c(\mathcal{V},\mathcal{W},I)$$

- 이러한 집합 V 를 찾아내는것은 위와 같이 정의됩니다. 

> ## SP Lime

- 하지만 , 위와 같이 , '최대화 하는 집합' 을 찾는것은 Global 한 optimization 문제로 매우 어렵습니다. 
- 그러므로 차선책으로 Greedy 하게 골라내려고 합니다. 

![jpg](/assets/images/ML/14_4.png)

- 그 방법론은 위와 같습니다. 

# [Examples](#link){: .btn .btn--primary}{: .align-center}

> ##  Example 1

![jpg](/assets/images/ML/14_5.png)

- 위 값은 lime 의 과정이다
- 파란색은 0, 검정색은 1 의 class를 가지는 경우
- 설명할 예측 데이터셋은 노란색이다. 그 주변으로 생성된 데이터가 검은색이다.
- 설명을 위해 예측할 가까운 sample 에 높은 weight를 매긴다. 그에따라 sample 주위의 데이터가 큰 점이 된 것을 볼 수 있다.
- 그 local에서 해석된 경우가 마지막 경우이다. + 는 파란색(0) , - 는 검은색(1)을 예측한 것
- local 에서는 잘 맞아보이는듯하다

> ##  Example 2 

![jpg](/assets/images/ML/14_6.png)

- 구글의 pretrained – neural network를 가지고 lime 분석을 해본 것이다.
- tabular data 와는 다르게 이미지를 super fixel 로 나누어서 데이터를 구성하게된다. 
- 이때에 보면 이 데이터를 분류 모델에 넣게되면 전자기타, 일렉트릭기타, 리트리버 세 개의 값이 각각 0.32,0.24,0.21 로 예측하게 되는데, 그 때에 어떤 super fixel 이 큰 역할을 했었는지에 대한 explaining 이 나온다. 
- 즉 구글의 뉴런네트워크는 올바른 근거에 의한 예측을 했음을 알 수 있고, 이는 모델의 예측치를 Trust 하는데에 근거가 될 수 있을 것이다

> ##  Example 3

![jpg](/assets/images/ML/14_7.png)

- 이 경우 newsgroup data set에서 기독교인지, 무신론인지 판단하는 것을 모델을 이용해 예측한 결과이다. 
- 아래에 Document 가 보이고, 이를 어떤 근거로 무신론자인지 판단한 결과가 나오고 있다.
- 이 때에 둘다 예측은 맞으나, 한쪽은 God, mean 등 어느정도 근거가 있는 데이터를 이용해 추론하였으나, 오른쪽의 경우는 판단의 근거가 Posting, Host 등 근거없는 text 로 예측하고 있다.

> ##  Example 4

![jpg](/assets/images/ML/14_8.png)

- 다음과 같이 실험을 진행했다고 합시다. 
  - 로지스틱 classifier를 20개의 이미지로 10개는 평범한 배경으로 한 허스키 사진을, 10개는 눈을 배경으로 한 늑대 사진을 훈련했다고 하자. 
  - 이렇게 훈련시킨 경우 classifier 는 개의 모양새로 판단하는 것이 아니라 그저 ‘눈’을 기준으로 허스키인지 늑대인지 판별하게 된다. 
  - 그리고 10개의 이미지중
    - 1개는 평범한 배경의 늑대[예측 : 개 ] 
    - 1개는 눈     배경의 개  [예측 : 늑대 ]
    - 8개는 평범한 배경의 개 , 눈 배경의 늑대 [예측 : 맞게 함]
    - 을 선택했다고 하자. 
- 이 10개의 이미지 분류 결과를 보고 실험자들에게 이 모델이 매우 믿음직하고 잘 작동하는지, 그리고 늑대와 개를 어떻게 분류하고 있을지에 대해 물어보았다.
  - (이때 실험자는 27명으로 모두 대학에서 최소 1개의 머신러닝 수업을 들은사람들임.. )
- Before , after 은 우리의 lime explanation을 듣고 난 이후에 결과이다.
- 그냥 10개 샘플에 대한 분류결과만 본 사람들은 27명중에 10명이나 이 모델을 믿음직하다고 판단.
- 그리고 snow 가 아마 늑대 예측에 어느정도의 feature 로 작용하였을거라는 예측이 12명이였다.
  - 즉 약 2/3 가량은 틀린판단을 하고있는 것
- 이것을 우리의 explanation 과 함께 보여주자 거의 모든사람이 맞는 선택을 하고있음을 볼 수 있었다.
  - 사실 모델은 눈을 이용해 늑대 / 개를 예측하고 있었다는 사실을 꺠달은것이다.

- 즉 모델의 예측 결과만 가지고는 모델을 Trust 하기에는 부족하다.

> ## Example 5

- Text data를 분석할때는 모델 해석을 위한 dataset을 구성하는 방법이 tabular data와는 다릅니다.
- Text LIME은 dataset을 생성할때 Original TEXT에서 단어(word)를 제거해가며 분석하는 방식으로 dataset을 만듦니다.
- 예를 들면, 아래 표와 같이 각 단어 유무에 따라 1, 0 으로 표시한 data를 만듦니다.

![jpg](/assets/images/ML/14_9.png)

- 여기서, prob은 해당 데이터를 black box model에 넣었을 때의 return된 확률값을 의미합니다.
- weight의 경우, original data에 비해 소실된 데이터의 비율로 weight를 나타냄
- 예를 들어 전체 7단어에서 한단어가 빠진 6단어로 표현했을 때, weight = 1 - 1/7 로 나타낼 수 있습니다.
- 그리고 이렇게 만들어진 data를 LIME Algoritm에 집어 넣어 아래 표처럼 각 feature weight를 계산하게 됨

![jpg](/assets/images/ML/14_10.png)

- 이런 식으로 나오면 channel이라는 단어가 spam을 구분하는데 큰 weight가 매겨져 있음을 확인할 수 있습니다.

> ## Example 6 

- Image를 위한 LIME은 tabular, text 데이터와는 또 다르게 데이터셋을 생성시킴
- 일반적으로 각 pixel을 random하게 변형시킬 것이라고 생각하지만 그렇게 하지 않음
  - 왜냐하면 각 픽셀들은 너무나 정보량이 작아서 '우리가 해석 가능한 수준' 이라고 보지 않기 떄문입니다.
- superpixels을 껐다 켰다하는 식으로 데이터를 생성 시킴
  - superpixels : pixels 끼리 서로 비슷한 색으로 연결되어 있는 것을 지칭
  - 이러한 superpixel 은 하나의 덩어리기 떄문에, 사람이 좀 더 해석하기가 편리합니다.
  - superpixels을 끈다는 의미는 superpixel의 값을 사용자가 정의한 값으로 변경 시킴을 의미 (gray)
- Inception v3 model을 이용해 bread를 구분하는 문제가 있다고 하자

![jpg](/assets/images/ML/14_11.png)

- LIME은 위의 그림처럼 label을 판단하는데 영향을 주는 부분을 표시할 수 있습니다.
- 여기서 green은 해당 pixels로 인해 해당 label을 판단하는데 양의 가중을 주는 것을 의미하고, red는 음의 가중을 주는 것을 의미함 (해당 label 이 아님을 증거)
- 위 그림에서 베이글을 예측하는데 그림의 녹색부분이 큰 역할을 했다는 것을 바로 확인할 수 있을것입니다.

---

**reference**

- [sampling 방법, example1]
  - https://rstudio-pubs-static.s3.amazonaws.com/442427_4d8243a35bcd4df3982b992d851a0a6b.html
- [sampling 방법]
  - https://compstat-lmu.github.io/iml_methods_limitations/lime.html
- [모델 설명]
  - https://christophm.github.io/interpretable-ml-book/
- [논문]
  - https://arxiv.org/pdf/1602.04938.pdf 
- [sampling 방법 및 exmple]
  - https://dodonam.tistory.com/202

 사실 '국소적인 부분에서 Linear' 하다라는 가정이 핵심가정인데, 이를 뒷받침할만한 증거가 없어서 약간 아쉽습니다. 그리고 국소적인 해석에 대한 내용이 주여서 , 나중에 전체 데이터의 해석을 위한 Submodular pick 이라는 방법을 제시하긴 하는데 이도 결국 '이렇게 하면 어느정도 대표성이 보장된다고 볼 수 있지 않을까?' 정도의 느낌이고 Justification 이 약한것 같아요. (SP-lime 은 사실 GLobal해석으로는 별로 선호되지 않는다고 하네요) 또한 Lime 을 직접 적용해보면 데이터 크기가 클 떄에, Sample 의 크기를 작게하면 Random 성에 의해 크게 해석이 자꾸 바뀌는 경향이 있어, 큰 데이터의 경우에는 Feature Reduction 을 하고 난 이후에 해석을 해야될것 같아요
{: .notice--success}
