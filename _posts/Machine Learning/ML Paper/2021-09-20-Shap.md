---
title: "A Unified Approach to Interpreting Model Predictions(ing)"
excerpt: "Shapeley Value 의 원본 논문"
categories:
  - ML_Paper
last_modified_at: 2021-09-20

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

  모델이 특정한 예측하는것을 이해하는것은, 때론 예측의 정확성보다 중요할떄가 있습니다. 하지만 요즘의 모델들은 전문가조차 해석하기 어려운 모델들이 많아 해석하는데에 어려움이 있습니다. 그래서 해석을 위한 다양한 방법론이 제시되었지만 어떤 방법론도 뚜렷한 근거를 가지고 좋다고 보기는 힘들었습니다. <br> 이 논문에서는 이러한 해석을 위한 SHAP(Shapley Addition Explanations) 을 제시합니다. Shap 은 additive feature 를 가진 importance measure 를 제시하고, 중요도에 대한 unique 한 값을 가지는 importance 를 만들어냅니다. 이런 방법론은 다른 방법론보다 훨씬 안정적이며, 직관적인 해석을 제시해줍니다. 
{: .notice--warning}

![jpg](/assets/images/ML/12_1.png)

# [Additive Feature Attribution model](#link){: .btn .btn--primary}{: .align-center}

- 간단한 모델일 경우, 가장 좋은 설명은 사실 모델 그 자체입니다.
  - Linear regression 을 생각해봅시다. 이 모델은 '모델 자체가 해석' 입니다.
- 하지만 복잡한 모델의 경우에는 이해하기가 쉽지 않기 때문에 모델만 제시하는것 가지고는 해석력을 가진다고 보기가 힘듭니다.
  - Random Forest 모델만 하더라도 앙상블과, 배깅으로 인해 모델이 너무 어렵게 생겼습니다.

> ## Additive Feature Attribution Methods 

- 그러므로 우리는 모델 그 자체를 해석하는 방법보다는, 모델을 설명하는 쉬운 모델을 하나 제시하는것이 더 좋을것입니다.
- 우리는 이것을 interpretable model 이라고 정의하였습니다. 
- 먼저 이를 정의하기에 앞서, 용어들을 정리하고 넘어갑시다.

> 용어정리

- $f$ : 설명이 필요한 원래의 모델
- $g$ : $f$ 를 설명하기 위한 단순한 모델 
- $x$ : $f$ 에 들어가는 input
- $x'$ : $g$ 의 input 으로 들어가는, $x$ 의 단순화된 모습
- $h_x$ : $x'$ 를 $x$ 로 다시 만들어주는 함수 ($x = h_x(x')$)

> 모델이 가져야할 좋은 특성

- 모델 해석을 위해 사용하는 함수는 $g(z) \approx f(h_x(z'))$ 를 만족해야 합니다. 
  - 왜냐하면, $g$ 가 원래 모델과 비슷하게 근사해야, $g$ 는 올바르게 원래 모델을 해석한다고 볼 수 있기 때문입니다.

> ## Definition

- Additive Feature attribution method 는 이진 변수 $z'\in \{0,1\}^M$ 에 대해서 아래와 같은 모델을 가집니다.

$$g(z') = \phi_0 + \sum_{i=1}^{M} \phi_i z_{i}^{'}$$

- $z'\in \{0,1\}^M$
- M = 단순화된 input 에 대한 features 의 수 
- 위에서, $\phi_i$ 의 계수들을 이용하여, 우리는 어떤 변수가 중요한지에 대해서 알 수 있게됩니다. 
- 자세한것에 대해서는 뒤에서 좀 더 많이 알아보도록 하겠습니다.

# [Shapley Value](#link){: .btn .btn--primary}{: .align-center}

- Shapley Value 가 어떻게 정의되는지 살펴보겠습니다.

$$\phi_i = \sum_{S \subseteq F \setminus \left\{ i \right\}} \frac{|F|!}{|S|! (|F| - |S| - 1)!} \left[ f_{S \cup \left\{ i \right\}} (x_{S \cup \left\{ i \right\}}) - f_S(x_S) \right]$$

$F$ : 모든 변수의 집합<br>
$S$ : $F$ 의 부분집합 <br>
$f_S$ : $S$ 를 이용해 training  된 모델<br>
$x_s$ : set $S$ 변수만을 이용한 input <br>
$f_{S\cup\{i\}}$ : $S$ 와 $i$ 번째 feature 를 이용하여 적합한 model
{: .notice}

- 즉 Shapley value 는 i 번째 Feature 의 importance 를 측정하기 위해서 이 변수가 있었을때에 영향도의 평균을 측정하게 됩니다.

> ## Approximation

- 하지만 위와 같은 방법을 이용한다면, Feature i 에 대한 중요도를 계산하기 위해서 모든 부분집합을 고려해야합니다. 
- 이렇게 모든 부분집합을 고려하게 된다면 $2^{\mid F \mid}$ 만큼의 계산량이 생기게 됩니다.
- 즉 이러한 과도한 계산을 피하기 위하여 , Sample approximation 을 통해서 근사하는 방법을 쓰기도 한다고 합니다.

# [Shapley Value Properties](#link){: .btn .btn--primary}{: .align-center}

- 사실상 이 논문의 핵심중의 핵심이라고 생각합니다. 
- 이러한 산출은 게임이론과 매우 흡사합니다.
  - 게임 내의 모든 플레이어들은 게임의 결과에 영향을 미칠 수 있다.
  - Explanation model의 변수의 계수들은 예측 결과에 영향을 미칠 수 있다.
  - Explanation model의 변수의 계수 = 게임 내의 모든 플레이어, 모델의 예측 결과 = 게임의 결과
- 위에서 살펴보았던, Additive feature attribution model 이 하나의 unique 한 해를 가지기 위해서는 아래에서 살펴볼 3가지 property 를 만족한다면 된다는 것입니다.
- 이 논문에서는 Additive feature attribution methods가 오직 하나의 해를 갖기 위해 가져야 할 조건들을 다음과 같이 제시합니다.

> ## Property 1 : Local Accuracy

![jpg](/assets/images/ML/12_2.png)

- Local accuracy 란, 특정한 input x 에대해서 g 가 모델 f 를 잘 근사할떄,  Attribution Value 들의 합은 $f(x)$ 와 같다.
- 즉 모든 중요도의 점수를 합치면 전체 점수가 된다고 이해하면 됩니다.

> ## Property 2 : Missingness

![jpg](/assets/images/ML/12_3.png)

- 단순화된 input $x'$ 에서 Feature i 에 해당하는 값이 0 이라면 해당 변수의 영향력또한 0이다.
- 이는 Null 한 Feature 에 대해서 중요도는 계산되지 않는다는 좋은 특성입니다.

> ## Property 3 : Consistency

![jpg](/assets/images/ML/12_4.png)

- 모델이 $f$ 에서 $f'$ 으로 바뀌었을떄에, feature i 의 영향력이 커졌다면 importance 또한 큰 값을 가진다는 것입니다.

> ##  Theorem

![jpg](/assets/images/ML/12_6.png)

- 위와 같인 Property 3가지를 만족한다면, Explanation model g 는 오로지 하나 존재하고 위와 같은 정의를 가지게 됩니다.
- 즉 위의 유용한 성질을 만족하는 유일한 Importance measure 은 오로지 Shapley value 하나뿐임을 보이는 것입니다.

# [Shap (SHapley Additive exPlanation) Value](#link){: .btn .btn--primary}{: .align-center}

- 이떄에, 논문에서 shap 의 계산을 다음과 같이 정의합니다
- $f_x(z') = f(h_x(z')) = E[f(z)\mid z_S]$
- S 는 set of non zero indexs in z' 가 됩니다.

---

**Reference**

- <https://arxiv.org/pdf/1705.07874.pdf>




