---
title:  "Binomial Distribution"
excerpt: "이항분포에 대해서 알아보자"
categories:
  - Mathematical_Statistics
last_modified_at: 2021-11-23

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true
use_math: true
---

 Binomial Distribution
{: .notice--warning}

# [Binomial Distribution Distribution](#link){: .btn .btn--primary}{: .align-center}

> ## Definition

- 성공확률이 $p$ 인 베르누이 시행을 $N$ 번 반복하는 경우를 생각해 봅시다.
- 가장 운이 좋을 때에 는 $N$ 번 모두 성공할 것이고 가장 운이 나쁜 경우에는 한 번도 성공하지 못할 것이다. 

> ## Binomial RandomVariable

- 확률이 p 인 베르누이 시행을 N 번 반복하였을때 성공의 횟수를 X 라 하면, 이를 이항분포의 확률변수라 합니다. 
  - $X$ 의 값은 0 부터 $N$ 까지의 정수 중 하나가 될 것입니다.
- 이 경우에는 시행횟수 N 과 각각의 성능확률 p 를 모수로 가지고 , 그 분포는 아래와 같이 표기합니다.

$$X \sim \operatorname{Bin}(N,p)$$

> ## Binomial Distribution 

- 확률변수 X 가 따르는 분포는 다음과 같이 계산됩니다.

$$p(x)= \begin{cases}\left(\begin{array}{l}
n \\
x
\end{array}\right) p^{x}(1-p)^{n-x} & x=0,1,2, \ldots, n \\
0 & \text { elsewhere }\end{cases}$$

> Proof

- 각각의 N 번의 시행중에, x 번 성공이 나올 확률을 생각하면 쉽게 위의 분포를 가지는것을 알 수 있습니다. 

# [Properties](#link){: .btn .btn--primary}{: .align-center}

> ## Properties

$$\begin{array}{|l|l|}
\hline \text { Notation } & B(n, p) \\
\hline \text { Parameters } & n \in\{0,1,2, \ldots\} \text { - number of trials } \\
& \begin{array}{l}
p \in[0,1]-\text { success probability for each } \\
\text { trial }
\end{array} \\
& q=1-p \\
\hline \text { Support } & k \in\{0,1, \ldots, n\} \text { - number of } \\
& \text { successes } \\
\hline \text { PMF } & \left(\begin{array}{l}
n \\
k
\end{array}\right) p^{k} q^{n-k} \\
\hline \text { CDF } & I_{q}(n-k, 1+k) \\
\hline
\hline \text { Mean } & n p \\
\hline \text { Median } & \lfloor n p\rfloor \text { or }\lceil n p\rceil \\
\hline \text { Mode } & \lfloor(n+1) p\rfloor \text { or }\lceil(n+1) p\rceil-1 \\
\hline \text { Variance } & n p q \\
\hline \text { Skewness } & \frac{q-p}{\sqrt{n p q}} \\
\hline \text { Ex. kurtosis } & \frac{1-6 p q}{n p q} \\
\hline \text { Entropy } & \frac{1}{2} \log _{2}(2 \pi e n p q)+O\left(\frac{1}{n}\right) \\
& \begin{array}{l}
\text { in shannons. For nats, use the natural log } \\
\text { in the log. }
\end{array} \\
\hline \text { MGF } & \left(q+p e^{t}\right)^{n} \\
\hline \text { CF } & \left(q+p e^{i t}\right)^{n} \\
\hline \text { PGF } & G(z)=[q+p z]^{n} \\
\hline \text { Fisher } & g_{n}(p)=\frac{n}{p q} \\
\text { information } & \text { (for fixed } n) \\
\hline
\end{array}$$

> ## Proof : E[X] = np

> Proof

각각의 시행을 독립된 Ber(p) 로 고려하게 되면 아래와 같이 쉽게 증명할 수 있습니다. $X=X_{1}+\cdots+X_{n}$ 라 하면

$$\mathrm{E}[X]=\mathrm{E}\left[X_{1}+\cdots+X_{n}\right]=\mathrm{E}\left[X_{1}\right]+\cdots+\mathrm{E}\left[X_{n}\right]=p+\cdots+p=n p$$

> ## Proof : V(X) = np(1-p)

> Proof

각각의 시행을 독립된 Ber(p) 로 고려하게 되면 아래와 같이 쉽게 증명할 수 있습니다. $X=X_{1}+\cdots+X_{n}$ 라 하면

$$\mathrm{V}[X]=\mathrm{V}\left[X_{1}+\cdots+X_{n}\right]=\mathrm{V}\left[X_{1}\right]+\cdots+\mathrm{V}\left[X_{n}\right]=p(1-p)+\cdots+p(1-p)=n p (1-p)$$

> ## Proof : mgf

> Proof

$$\begin{aligned}
M(t) &=\sum_{x} e^{t x} p(x)=\sum_{x=0}^{n} e^{t x}\left(\begin{array}{l}
n \\
x
\end{array}\right) p^{x}(1-p)^{n-x} \\
&=\sum_{x=0}^{n}\left(\begin{array}{l}
n \\
x
\end{array}\right)\left(p e^{t}\right)^{x}(1-p)^{n-x} \\
&=\left[(1-p)+p e^{t}\right]^{n}
\end{aligned}$$

---

**reference**

- Hoggs Introduction to Mathematical Statistics 7ed
- <https://en.wikipedia.org/wiki/Binomial_distribution>





