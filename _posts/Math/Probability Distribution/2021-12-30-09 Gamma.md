---
title:  "Gamma"
excerpt: ""
categories:
  - Mathematical_Distribution
last_modified_at: 2021-11-24

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true
use_math: true

---

 Binomial Distribution
{: .notice--warning}

# [Gamma Function](#link){: .btn .btn--primary}{: .align-center}

> ## Definition

- 감마 분포에 대해 알아보기 전에 먼저 감마 함수를 알아야 합니다! 감마 함수는 아래 공식과 같이 정의되는 함수인데요. 이 감마 함수는 수학자들이 **팩토리얼**  $$n !=n(n-1)(n-2) \cdots$$을 자연수에만 한정해서 적용하지 않고, 더 큰 수 체계에도 적용할 수 있는 방법을 고민하다가 나온 함수라고 합니다. 
- 즉, `복소수 범위까지 일반화 된 팩토리얼`인 것이죠

$$\Gamma(\alpha)=\int_{0}^{\infty} t^{\alpha-1} e^{-t} d t, \quad(\alpha>0)$$

- 이 감마함수는 아래 세가지 식과 같이 계산됩니다. 이걸 증명하는 과정은 부분적분 이용해서 계산 노동 하다보면 쉽게 해결됩니다.

$$\begin{aligned}
&\text { - } \Gamma(\alpha)=(\alpha-1) \Gamma(\alpha-1),(\alpha>1) \\
&\text { - } \Gamma(\alpha)=(\alpha-1) ! \\
&\text { - } \Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}
\end{aligned}$$

> ## Property

> $\Gamma(1)$

- If $\alpha=1$, Then

$$\Gamma(1)=\int_{0}^{\infty} e^{-y} d y=1$$

> $\Gamma(k)$

- If $\alpha>1$, then an integration by parts shows that

$$\Gamma(\alpha)=\int_{0}^{\infty}(\alpha-1) y^{\alpha-2} e^{-y} d y=(\alpha-1) \Gamma(\alpha-1)$$

- so that for a positive integer $\alpha$ greater than 1 

$$\Gamma(\alpha)=(\alpha-1)(\alpha-2) \cdots(2)(1) \Gamma(1)=(\alpha-1) !$$

> ## Relationship with gamma distribution

- Letting $y=(x / \beta)$, where $\beta>0$, the gamma function $\Gamma(\alpha)$ can be written as

$$\begin{aligned}
\Gamma(\alpha) &=\int_{0}^{\infty}\left(\frac{x}{\beta}\right)^{\alpha-1} e^{-x / \beta}\left(\frac{1}{\beta}\right) d x \\
&=\int_{0}^{\infty} \frac{1}{\beta^{\alpha}} x^{\alpha-1} e^{-x / \beta} d x \\
1 &=\int_{0}^{\infty} \frac{1}{\Gamma(\alpha) \beta^{\alpha}} x^{\alpha-1} e^{-x / \beta} d x \\
&=\int_{0}^{\infty} f_{X}(x) d x
\end{aligned}$$

- where $f_{X}(x)$ is a pdf of a gamma random variable $X$ with parameters $\alpha$ and $\beta$. 
- We will write $X$ has a $\Gamma(\alpha, \beta)$ distribution.

# [Gamma Distribution](#link){: .btn .btn--primary}{: .align-center}

> ## Definition

- 본격적으로 감마 분포에 대해서 알아보겠습니다. 감마 분포의 확률밀도함수는 다음과 같습니다.

$$\begin{gathered}  X\sim Gamma(\alpha, \beta) \cr f_X(x) = \frac{1}{\beta^{\alpha} \Gamma(\alpha)}x^{\alpha-1}e^{-\frac{x}{\beta}}, \quad (x,\alpha ,\beta >0) \cr (\alpha: shape\space parameter, \beta: scale\space parameter) \end{gathered}$$

> ## Shape

![jpg](/assets/images/Stat/152_1.jpg)

> ## Property

$$\begin{array}{|l|l|}
\hline \text { Parameters } &  k>0 \text { shape } \\
&  \theta>0 \text { scale } \\
\hline \text { Support } & x \in(0, \infty) \\
\hline \text { PDF } & f(x)=\frac{1}{\Gamma(k) \theta^{k}} x^{k-1} e^{-\frac{x}{\theta}} \\
\hline \text { CDF } & F(x)=\frac{1}{\Gamma(k)} \gamma\left(k, \frac{x}{\theta}\right) \\
\hline \text { Mean } & k \theta \\
\hline \text { Median } & \text { No simple closed form } \\
\hline \text { Mode } & (k-1) \theta \text { for } k \geq 1 \\
\hline \text { Variance } & k \theta^{2} \\
\hline \text { Skewness } & \frac{2}{\sqrt{k}} \\
\hline \text { MGF } & (1-\theta t)^{-k} \text { for } t<\frac{1}{\theta} \\
\hline \begin{array}{l}
\text { Ex. } \\
\text { kurtosis }
\end{array} & \frac{6}{\sqrt{k}} \\
\hline \text { Entropy } & k+\ln \theta+\ln \Gamma(k) \\
& \quad+(1-k) \psi(k) \\
\hline
\end{array}$$

> ## MGF

$$\begin{aligned}
M(t) &=\int_{0}^{\infty} e^{t x} \frac{1}{\Gamma(\alpha) \beta^{\alpha}} x^{\alpha-1} e^{-x / \beta} d x \\
&=\frac{1}{\Gamma(\alpha) \beta^{\alpha}} \int_{0}^{\infty} x^{\alpha-1} e^{-x /\left(\frac{\beta}{1-\beta t}\right)} d x \\
&=\frac{1}{\Gamma(\alpha) \beta^{\alpha}} \cdot \Gamma(\alpha)\left(\frac{\beta}{1-\beta t}\right)^{\alpha} \\
&=\frac{1}{(1-\beta t)^{\alpha}} ... (\text{for t<1/$\beta$})
\end{aligned}$$

> ## Mean

- MGF 를 이용하여서 Mean 을 계산해봅시다.
- 미분하면 $$M^{\prime}(t)=(-\alpha)(1-\beta t)^{-\alpha-1}(-\beta)$$
- 즉 평균 $$\mu=M^{\prime}(0)=\alpha \beta$$

> ## Variance 

- Variance 도 똑같이 MGF 를 이용해서 계산해봅시다.
- 미분하면 $$M^{\prime \prime}(t)=(-\alpha)(-\alpha-1)(1-\beta t)^{-\alpha-2}(-\beta)^{2}$$
- 즉 분산 $$\sigma^{2}=M^{\prime \prime}(0)-M^{\prime}(0)^{2}=\alpha(\alpha+1) \beta^{2}-\alpha^{2} \beta^{2}=\alpha \beta^{2}$$

# [More Properties](#link){: .btn .btn--primary}{: .align-center}

> ## Scale Parameter Property

> Property

- Let $X$ be a gamma random variable such that $X \sim \Gamma(\alpha, \beta) .$ Let $Y=X / \beta \equiv$ $u(X)$. Then we have

$$Y \sim \Gamma(\alpha, 1)$$

> Proof

- The pdf of $Y$ is given by

$$\begin{aligned}
f_{Y}(y) &=f_{X}\left(u^{-1}(y)\right)|J| \\
&=\frac{1}{\Gamma(\alpha) \beta^{\alpha}}(\beta y)^{\alpha-1} e^{-(\beta y) / \beta} \cdot \beta \\
&=\frac{1}{\Gamma(\alpha)} y^{\alpha-1} e^{-y}
\end{aligned}$$

- which is a pdf of a gamma distribution with parameters $\alpha$ and 1 

> ## Summation Property

> Property

- Let $X_{1}, \ldots, X_{n}$ be independent gamma random variables such that

$$X_{i} \sim \Gamma\left(\alpha_{i}, \beta\right)$$

for $i=1, \ldots, n$. Let $Y=\sum_{i=1}^{n} X_{i}$. Then we have

$$Y \sim \Gamma\left(\sum_{i=1}^{n} \alpha_{i}, \beta\right)$$

> Proof

- Proof. Using independence and the mgf of gamma distributions, the mgf of $Y$ is

$$\begin{aligned}
M_{Y}(t) &=E\left(e^{\sum_{i=1}^{n} t X_{i}}\right) \\
&=E\left(\prod_{i=1}^{n} e^{t X_{i}}\right) \\
& \stackrel{\text { ind }}{=} \prod_{i=1}^{n} E\left(e^{t X_{i}}\right) \\
&=\prod_{i=1}^{n}(1-\beta t)^{-\alpha_{i}} \\
&=(1-\beta t)^{-\sum_{i=1}^{n} \alpha_{i}}
\end{aligned}$$

- which is the mgf of a gamma distribution with parameters $\sum_{i=1}^{n} \alpha_{i}$ and $\beta$.

> ## Relationship with Exponential

- Gamma Distribution 의 pdf 는 아래와 같이 생겼다는것을 기억합시다.

$$\begin{gathered}  X\sim Gamma(\alpha, \beta) \cr f_X(x) = \frac{1}{\beta^{\alpha} \Gamma(\alpha)}x^{\alpha-1}e^{-\frac{x}{\beta}}, \quad (x,\alpha ,\beta >0) \cr (\alpha: shape\space parameter, \beta: scale\space parameter) \end{gathered}$$

- 이때 $\alpha = 1$ 을 적용하게 되면 식은 

$$\begin{gathered}  X\sim Gamma(1, \beta) \cr f_X(x) = \frac{1}{\beta^{}}e^{-\frac{x}{\beta}}, \quad (x,\alpha ,\beta >0) \cr (\alpha: shape\space parameter, \beta: scale\space parameter) \end{gathered}$$

- 위처럼 , $Exp(\beta)$ 의 분포를 따르게 되죠.
- 즉 모수가 $\beta$ 인 지수 분포를 따르는 서로 독립이고 동일한(iid) 확률 변수 $\alpha$ 개를 합한 확률변수는, 모수가 $\alpha$ 와 $\beta$ 인 감마 분포 를 따릅니다!

> ## Interpret Graph

- 아래는 각 모수 α,β에 따른 감마 분포입니다. β가 고정되어 있는 상태에서 α가 증가할수록 평균과 분산이 커지면서 점점 오른쪽으로 퍼지는 형태라는 것을 알 수 있습니다. 
  - 이 때, β를 증가하면 증가할수록 이 경향은 심해집니다.
-  β가 증가할수록 분산이 훨씬 커지게 되어 더욱 퍼지는 형태가 된다는 것을 확인할 수 있습니다. 
  - 직관적으로도 기다릴 사건 개수(α)가 많아질수록, 사건 사이 평균 소요시간(β)이 커질수록 전체 대기 시간이 길어질 것입니다.

![jpg](/assets/images/Stat/152_1.jpg)

> ## Relationship with Poisson

- 이때 Exponential 가 Poisson 과 관계가 있었음을 기억하시나요? 
  - 즉 Gamma Distribution 가 Exponential 과 관계가 있었으므로 poisson 과도 관계가 있을것이라 생각할 수 있습니다.
  - 그러면 구체적으로 어떤 관계가 있을까요? 

> Property

- $X \sim \operatorname{Gamma}(\alpha, \beta)$ 라고 하자. 이 때, 어떠한 $x$ 에 대해 $Y \sim \operatorname{Possion}\left(\frac{x}{\beta}\right)$ 라면, 다음의 관계가 만족된다. $P(X \leq x)=P(Y \geq \alpha)$

> Proof

- 먼저 직관적인 의미부터 이해해봅시다. 
- 좌변은 사건 사이 평균 소요 시간이 $\beta$ 인 사건이 $\alpha$ 개 일어날 때까지의 소요 시간이 $x$ 보다 작거나 같을 확률입니다. 한편, 전체 소요 시간을 사건 사이 평균 소요 시간으로 나눈 것, 즉 $x / \beta$ 는 사건의 개수가 될 것입니다. 따 라서 우변은 평균 발생 건수를 $x / \beta$ 로 하는 포아송 분포에서 사건이 $\alpha$ 개 이상 일어날 확률을 의미합니다
- 즉, 사건이 $\alpha$ 개 일어나는 데 걸렸던 시간이 $x$ 이하라는 것은, $x$ 시간 동안 사건이 $\alpha$ 개 일어나거나 또는 그 이상 일어났다는 것과 같을 것입니다. 
- 다시 말해서, 좌변은 소요 시간 $x$ 가 확률변수이지만, 우변에서는 고정된 것입니다. 이제 이 관계를 수리적으로 증명해 보겠습니다.

$$\begin{aligned}
P(X \leq x) &=\int_{0}^{x} \frac{1}{\Gamma(\alpha) \beta^{\alpha}} t^{\alpha-1} e^{-\frac{t}{\beta}} d t \\
&=\frac{1}{\Gamma(\alpha) \beta^{\alpha}}\left[-\beta e^{-\frac{t}{\beta}} t^{\alpha-1}\right]_{0}^{x}+\int_{0}^{x} \beta e^{-\frac{t}{\beta}}(\alpha-1) t^{\alpha-2} d t \\
&=\frac{1}{\Gamma(\alpha) \beta^{\alpha}}\left(-\beta e^{-\frac{x}{\beta}} x^{\alpha-1}\right)+\frac{1}{\Gamma(\alpha-1) \beta^{\alpha-1}} \int_{0}^{x} t^{\alpha-2} e^{-\frac{t}{\beta}} d t \\
&=\cdots \\
&=-\frac{e^{-\frac{x}{\beta}} x^{\alpha-1}}{\Gamma(\alpha) \beta^{\alpha-1}}-\frac{e^{-\frac{x}{\beta}} x^{\alpha-2}}{\Gamma(\alpha-1) \beta^{\alpha-2}}-\cdots-\frac{e^{-\frac{x}{\beta}} x}{\beta}+\int_{0}^{x} \frac{e^{-\frac{t}{\beta}}}{\beta} d t \\
&=1-\frac{e^{-\frac{x}{\beta}}(x / \beta)^{\alpha-1}}{(\alpha-1) !}-\frac{e^{-\frac{x}{\beta}}(x / \beta)^{\alpha-2}}{(\alpha-2) !}-\cdots-e^{-x / \beta} \\
&=1-P(Y=\alpha-1)-\cdots-P(Y=0) \\
&=1-P(Y \leq \alpha-1) \\
&=P(Y \geq \alpha)
\end{aligned}$$



---

**reference**

- Hoggs Introduction to Mathematical Statistics 7ed
- <https://elementary-physics.tistory.com/139>
- https://soohee410.github.io/gamma_dist







