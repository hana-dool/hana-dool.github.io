---
title:  "Bernoulli"
excerpt: ""
categories:
  - Mathematical_Distribution
last_modified_at: 2021-11-23

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true
use_math: true
---

 여러 변수들에 대한 Linearlity 성질
{: .notice--warning}

# [Bernoulli Distribution](#link){: .btn .btn--primary}{: .align-center}

> ## Definition

- 결과가 두 가지 중 하나로만 나오는 실험이나 시행을 **베르누이 시행**(Bernoulli trial)이라고 합니다.
- 예를 들어 동전을 한 번 던져 앞면(H:Head)이 나오거나 뒷면(T:Tail)이 나오게 하는 것도 베르누이 시행입니다.

> ## Bernoulli Random Variable

- 베르누이 시행의 결과를 실수 0 또는 1로 바꾼 것을 **베르누이 확률변수(Bernoulli random variable)**라고 합니다. 
  - 베르누이 확률변수는 두 값 중 하나만 가질 수 있으므로 이산확률변수(discrete random variable)다. 
- 베르누이 확률변수의 표본값은 보통 정수 1과 0으로 표현하지만 때로는 정수 1과 -1로 표현하는 경우도 있습니다. 일반적으로는 아레와 같이 정의됩니다.

$$X(\text { success })=1 \text { and } X(\text { failure })=0$$

> ## Bernoulli Distribution

- 어떤 확률변수 $X$ 가 베르누이분포에 의해 발생 된다면 **’확률변수 $X$ 가 베르누이분포를 따른다'**라고 말하고 다음과 같이 수식을 이용하게 됩니다.

$$X \sim \operatorname{Bern}(p)$$

pmf 는 아래와 같이 정의합니다.

$$p(x)=p^{x}(1-p)^{1-x}, \quad x=0,1$$

# [Properties](#link){: .btn .btn--primary}{: .align-center}

> ## Figure

![png](/assets/images/Stat/109_1.png)

> ## Properties

$$\begin{array}{|l|l|}
\hline \text { Parameters } & 0 \leq p \leq 1 \\
& q=1-p \\
\hline \text { Support } & k \in\{0,1\} \\
\hline \text { PMF } & \begin{array}{ll}
q=1-p & \text { if } k=0 \\
p & \text { if } k=1 \\
p^{k}(1-p)^{1-k}
\end{array} \\
\hline \text { CDF } & \begin{cases}0 & \text { if } k<0 \\
1-p & \text { if } 0 \leq k<1 \\
1 & \text { if } k \geq 1\end{cases} \\
\hline \text { Mean } & p \\
\hline \text { Median } & \begin{cases}0 & \text { if } p<1 / 2 \\
{[0,1]} & \text { if } p=1 / 2 \\
1 & \text { if } p>1 / 2\end{cases} \\
\hline \text { Mode } & \begin{cases}0 & \text { if } p<1 / 2 \\
0,1 & \text { if } p=1 / 2 \\
1 & \text { if } p>1 / 2\end{cases} \\
\hline \text { Variance } & p(1-p)=p q \\
\hline \text { MAD } & \frac{1}{2} \\
\hline \text { Skewness } & \frac{q-p}{\sqrt{p q}} \\
\hline \text { Ex. kurtosis } & \frac{1-6 p q}{p q} \\
\hline \text { Entropy } & -q \ln q-p \ln p \\
\hline \text { MGF } & q+p e^{t} \\
\hline \text { CF } & q+p e^{i t} \\
\hline \text { PGF } & q+p z \\
\hline \text { Fisher information } & \frac{1}{p q} \\
\hline
\end{array}$$

> ## Proof : E[X] = p

$$\mathrm{E}[X]=\operatorname{Pr}(X=1) \cdot 1+\operatorname{Pr}(X=0) \cdot 0=p \cdot 1+q \cdot 0=p$$

> ## Proof : V(X) = pq

We first find

$$\mathrm{E}\left[X^{2}\right]=\operatorname{Pr}(X=1) \cdot 1^{2}+\operatorname{Pr}(X=0) \cdot 0^{2}=p \cdot 1^{2}+q \cdot 0^{2}=p=\mathrm{E}[X]$$

From this follows

$$\operatorname{Var}[X]=\mathrm{E}\left[X^{2}\right]-\mathrm{E}[X]^{2}=\mathrm{E}[X]-\mathrm{E}[X]^{2}=p-p^{2}=p(1-p)=p q$$





---

**reference**

- Hoggs Introduction to Mathematical Statistics 7ed
- <https://people.math.gatech.edu/~ecroot/3225/rho_notes.pdf>





